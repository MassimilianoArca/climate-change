{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"..\", \"..\", \"..\", \"data\", \"berlin\")\n",
    "clean_data_folder = os.path.join(data_folder, \"clean_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = pd.read_excel(os.path.join(clean_data_folder, \"surface.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df = pd.read_excel(os.path.join(clean_data_folder, \"ground.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns = [\"DateTime\", \"Station\"]\n",
    "bacteria_columns = [\n",
    "    \"E.Coli (MPN/100ml)\",\n",
    "    \"Enterococcus (MPN/100ml)\",\n",
    "    \"Coliform (MPN/100ml)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(df: pd.DataFrame, lags: int, rolling_window: int, poly_degree: int):\n",
    "    \n",
    "    initial_features = df.columns\n",
    "    # add polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_degree)\n",
    "    df_poly = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df_poly, columns=poly.get_feature_names_out(df.columns))\n",
    "    \n",
    "    # add lagged, rolling and expanding features for each variable in df\n",
    "    for col in initial_features.difference([\"Year\", \"Month\"]):\n",
    "        for lag in range(1, lags + 1):\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            \n",
    "        df[f\"{col}_rolling{rolling_window}\"] = df[col].rolling(rolling_window).mean()\n",
    "        \n",
    "    # fill NaN values with bfill\n",
    "    df.bfill(inplace=True)\n",
    "    \n",
    "    df.drop(columns=['1'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    \"Ammonium (mg/l)\",\n",
    "    \"Conductivity (ÂµS/cm)\",\n",
    "    \"Dissolved Oxygen (mg/l)\",\n",
    "    \"Nitrate (mg/l)\",\n",
    "    \"pH\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Year\"] = df[\"DateTime\"].dt.year\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Month\"] = df[\"DateTime\"].dt.month\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"DateTime\"] = datetime_column.values\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Year\"] = df[\"DateTime\"].dt.year\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Month\"] = df[\"DateTime\"].dt.month\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"DateTime\"] = datetime_column.values\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Year\"] = df[\"DateTime\"].dt.year\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Month\"] = df[\"DateTime\"].dt.month\n",
      "/var/folders/z5/plf0_b5s39nb0_gctbxmpndc0000gn/T/ipykernel_85690/3521460032.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"DateTime\"] = datetime_column.values\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "# Prepare the data for the models\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    df = surface_df[surface_df['Station'] == station_id]\n",
    "    \n",
    "    # add the year and month columns\n",
    "    df[\"Year\"] = df[\"DateTime\"].dt.year\n",
    "    df[\"Month\"] = df[\"DateTime\"].dt.month\n",
    "    \n",
    "    # Save the datetime column for later (drop diff returns error\n",
    "    # if I remove it before)\n",
    "    datetime_column = df.drop(columns=bacteria_columns).dropna()[\"DateTime\"]\n",
    "    \n",
    "    df = df.drop(columns=diff_columns + bacteria_columns).dropna()\n",
    "    \n",
    "    X = df.drop(columns=[\"DOC (mg/l)\"])\n",
    "    y = df[[\"DOC (mg/l)\"]]\n",
    "    \n",
    "    X = extend_features(X, lags=1, rolling_window=3, poly_degree=2)\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = X.columns\n",
    "    \n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X, columns=cols)\n",
    "    \n",
    "    # Add the datetime column back\n",
    "    X[\"DateTime\"] = datetime_column.values\n",
    "    y[\"DateTime\"] = datetime_column.values\n",
    "    \n",
    "    \n",
    "    X = X.set_index(\"DateTime\")\n",
    "    y = y.set_index(\"DateTime\")\n",
    "    \n",
    "    X_tr, X_ts = X[:int(train_size * len(X))], X[int(train_size * len(X)):]\n",
    "    y_tr, y_ts = y[:int(train_size * len(y))], y[int(train_size * len(y)):]\n",
    "    \n",
    "    datasets[station_id] = (X_tr, X_ts, y_tr, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    model = sm.OLS(y_tr, sm.add_constant(X_tr))\n",
    "    results = model.fit()\n",
    "    \n",
    "    predictions = results.get_prediction(sm.add_constant(X_ts)).summary_frame(alpha=0.05)\n",
    "    \n",
    "    lr_results[station_id] = {\n",
    "        \"y_pred\": predictions['mean'],\n",
    "        \"y_pred_lower\": predictions['mean_ci_lower'],\n",
    "        \"y_pred_upper\": predictions['mean_ci_upper'],\n",
    "        \"model\": results,\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_ts, predictions[\"mean\"])),\n",
    "        \"r2\": r2_score(y_ts, predictions[\"mean\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_rf(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    # define the hyperparameters to search over\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 500, step=10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 32),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X_cv)):\n",
    "        cv_rmse[i] = fit_and_validate_rf(\n",
    "            X_cv, y_cv, train_index, test_index, params\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "    \n",
    "    if os.path.exists(f\"RandomForest-Station{station_id}-Extended.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - RandomForest\"\n",
    "        + \" + \"\n",
    "        + f\"Station {station_id}\",\n",
    "        storage=f\"sqlite:///RandomForest-Station{station_id}-Extended.sqlite3\",\n",
    "    )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///RandomForest-Station{station_id}-Extended.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - RandomForest\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    rf_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "rf_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = rf_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = RandomForestRegressor(random_state=42, **params)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    rf_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "\n",
    "    if os.path.exists(f\"XGBoost-Station{station_id}-Extended.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - XGBoost\"\n",
    "        + \" + \"\n",
    "        + f\"Station{station_id}\",\n",
    "        storage=f\"sqlite:///XGBoost-Station{station_id}-Extended.sqlite3\",\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///XGBoost-Station{station_id}-Extended.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - XGBoost\"\n",
    "            + \" + \"\n",
    "            + f\"Station{station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "            \n",
    "    xgb_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = xgb_studies[station_id].best_params\n",
    "    \n",
    "    params[\"objective\"] = \"reg:squarederror\"\n",
    "    params[\"booster\"] = \"gblinear\"\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = xgb.XGBRegressor(**params, random_state=42)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    xgb_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lgbm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "\n",
    "    if params is not None:\n",
    "        model.set_params(**params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_estimators\": trial.suggest_int(\n",
    "            \"n_estimators\", 1, 20, step=1\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 16, step=1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 20, step=1),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\n",
    "            \"min_data_in_leaf\", 2, 50, step=1\n",
    "        ),\n",
    "        \"lambda_l1\": trial.suggest_float(\n",
    "            \"lambda_l1\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"lambda_l2\": trial.suggest_float(\n",
    "            \"lambda_l2\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"min_split_gain\": trial.suggest_float(\n",
    "            \"min_split_gain\", 0, 15, step=0.5\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"min_child_samples\": trial.suggest_int(\n",
    "            \"min_child_samples\", 20, 1000, log=True\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 500, step=10),\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lgbm_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "        \n",
    "        X_tr, _, y_tr, _ = datasets[station_id]\n",
    "    \n",
    "        if os.path.exists(f\"LGBM-Station{station_id}-Extended.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - LGBM\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            storage=f\"sqlite:///LGBM-Station{station_id}-Extended.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///LGBM-Station{station_id}-Extended.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - LGBM\"\n",
    "                + \" + \"\n",
    "                + f\"Station {station_id}\",\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lgbm_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.933745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.839389\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 607\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.730070\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.751490\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 607\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.765000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 608\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 6.050169\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.825404\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 643\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.798628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.796495\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 588\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.802475\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 605\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.907455\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 618\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.798888\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.859248\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.874561\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 694\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.685487\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.853151\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 622\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.846005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.724284\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 654\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.812536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.878144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 602\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.827984\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 583\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.755014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 655\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.818636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.722753\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 592\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.741059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 703\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.916622\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.948328\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.812170\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 606\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.823773\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 679\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.825540\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.790626\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 610\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.791217\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 667\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.898801\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.790740\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 609\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.710051\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 609\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.741816\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 659\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.845721\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 654\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.894070\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 608\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.941079\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 576\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.614320\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 663\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.752577\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 646\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.871007\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.813077\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 694\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.887840\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.917238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.751918\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 599\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.894365\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.762304\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 595\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.977832\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.644554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 618\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.792292\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 687\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.927559\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 687\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.806657\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 657\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.819332\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 696\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.867978\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 632\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.793213\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.904414\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 687\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.858962\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 588\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.837053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 695\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.695140\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.670277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 575\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.812169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 631\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.891623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.882885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 698\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.780357\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.900435\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 688\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.747572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 629\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.811017\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.952992\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.798984\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 639\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.951864\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 663\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.742151\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.813346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 625\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 6.018486\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 621\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.766786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 649\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.741882\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.686258\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 698\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.839141\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 622\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.850676\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 618\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.929494\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.945581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 665\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.784222\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 637\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.891021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.854066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 627\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.688420\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 692\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.783391\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.800668\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.652539\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 703\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.846475\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.853204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 666\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.803380\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 619\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.844434\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.714102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.789694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 702\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.780464\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 635\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.862377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.799008\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.915949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.849253\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 5.755137\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008681257378218206, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008681257378218206\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0035375709277243434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0035375709277243434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6067749848195905, subsample=0.9444290735841788 will be ignored. Current value: bagging_fraction=0.6067749848195905\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=300 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.011827151587395912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.011827151587395912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.737154\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.821487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.837149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.861417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.876392\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.812809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.822650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.749415\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.798755\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.725543\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.947083\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.764102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.806629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.742940\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.857007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.838477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.866239\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.776759\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.741995\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.835182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.750075\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.733665\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.863772\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.788743\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.789374\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.922666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.869632\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.771841\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.833356\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.782377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 667\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.814826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.867708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.842668\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.866971\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.760336\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.896934\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.798169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.830525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.856800\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.765624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.857845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.808907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.827933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.779378\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.870271\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.908926\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.729290\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.928305\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.805519\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.785201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.831983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.812018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.862750\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.777651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.727326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.806659\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.820790\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.858830\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.793924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.741753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.972727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.683902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.773257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.720388\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.759014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.846918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.915731\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.862436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.868014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.889698\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.858335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.800000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.887987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.808577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.933205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.945460\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.794616\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.781150\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.775782\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.780448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.915076\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.859634\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.912929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.727118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.851115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.850218\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.772956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.705057\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.808689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.834054\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.829681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.803029\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.736292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.850748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.888724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.934338\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 669\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.922727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.894497\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.737013\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 154, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 8.808929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.001026241895969053, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.001026241895969053\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0020976851763693017, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020976851763693017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11861115933227796, subsample=0.9963871252277148 will be ignored. Current value: bagging_fraction=0.11861115933227796\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=70 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8907662253900199, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8907662253900199\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.574072\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2648\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.458718\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2605\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.529689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.552057\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2555\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.528107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2607\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.470986\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2568\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.451942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2562\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.526414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2609\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.448182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2505\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.493938\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2563\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.513178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.519713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2630\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.446612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2593\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.516585\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.568058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2608\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.466078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2588\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.544977\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2583\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.620982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2476\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.411664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2587\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.536011\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2573\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.486307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2562\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.666938\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2614\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.447441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2585\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.593340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2587\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.554888\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2571\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.532579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2580\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.513686\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2592\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.511581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2558\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.643856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2572\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.552992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2571\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.482604\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2540\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.556393\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2627\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.543368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2542\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.389919\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2559\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.575025\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.612074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2516\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.570891\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2615\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.539599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2583\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.556469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.514130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2567\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.571703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2577\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.552496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2498\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.523181\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2547\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.626516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2601\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.515971\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2603\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.447373\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2547\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.516691\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2589\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.514364\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2543\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.564959\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2597\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.564220\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2594\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.524762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2605\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.425491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.383482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2614\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.498753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2584\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.386487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2518\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.486458\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2607\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.594153\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2565\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.571058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.533632\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2581\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.501530\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2554\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.530277\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2584\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.583246\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2570\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.535125\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2575\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.518586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2579\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.583464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2568\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.454433\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2551\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.580847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2584\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.596608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2614\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.408287\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2555\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.556361\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.591652\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2571\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.514600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2493\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.513762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2586\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.460040\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2534\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.553043\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2576\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.515098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2524\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.543379\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2541\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.554403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2506\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.528326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2560\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.526082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2543\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.526668\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2525\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.570732\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2586\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.524690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2583\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.497915\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2609\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.538846\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2530\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.495204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2541\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.422451\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2579\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.528446\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2542\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.575870\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2612\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.505709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2586\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.524063\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2553\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.462306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2542\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.413236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2581\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.584640\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2542\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.441977\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2620\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.415080\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2604\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.508109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2616\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.511915\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2587\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.588876\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2551\n",
      "[LightGBM] [Info] Number of data points in the train set: 313, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 7.515502\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.11286413565654808, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.11286413565654808\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.007973379963012775, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007973379963012775\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.14309993452828618, subsample=0.8489681098419225 will be ignored. Current value: bagging_fraction=0.14309993452828618\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=700 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.0022104738088989688, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.0022104738088989688\n"
     ]
    }
   ],
   "source": [
    "lgbm_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = lgbm_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        linear_tree=True,\n",
    "        )\n",
    "        \n",
    "        model.set_params(**params)\n",
    "        \n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    lgbm_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_nn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        random_state=42,\n",
    "        hidden_layer_sizes=tuple(params[\"layers\"]),\n",
    "        max_iter=1000,\n",
    "    )\n",
    "\n",
    "    param = params.copy()\n",
    "    param.pop(\"layers\")\n",
    "    model.set_params(**param)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"layers\": [\n",
    "            trial.suggest_int(f\"n_units_{i}\", 50, 100, step=5)\n",
    "            for i in range(trial.suggest_int(\"n_layers\", 2, 2))\n",
    "        ],\n",
    "        \"activation\": trial.suggest_categorical(\n",
    "            \"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "        ),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1),\n",
    "        \"learning_rate\": trial.suggest_categorical(\n",
    "            \"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "        ),\n",
    "        \"power_t\": trial.suggest_float(\"power_t\", 0.1, 1),\n",
    "        \"beta_1\": trial.suggest_float(\"beta_1\", 0.1, 1),\n",
    "        \"beta_2\": trial.suggest_float(\"beta_2\", 0.1, 1),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-8, 1),\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_nn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:12:41,692] A new study created in RDB with name: Hyperparameter Tuning - MLP + Station 305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6542eae7c8dc44d4a71f571a063a9f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:12:43,676] Trial 0 finished with value: 1.1514721960078518 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.7089248938819706, 'learning_rate': 'constant', 'power_t': 0.5364303567609794, 'beta_1': 0.4468611797258778, 'beta_2': 0.9755537413482973, 'epsilon': 0.8596652865112296}. Best is trial 0 with value: 1.1514721960078518.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:12:47,100] Trial 1 finished with value: 7.885458432198815 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 65, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.26949807919372093, 'learning_rate': 'invscaling', 'power_t': 0.7774205243424374, 'beta_1': 0.40879423787141245, 'beta_2': 0.754387881023421, 'epsilon': 0.5597253793365822}. Best is trial 0 with value: 1.1514721960078518.\n",
      "[I 2024-08-11 13:12:47,332] Trial 2 finished with value: 1.1620520424906338 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 85, 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.4205711090629336, 'learning_rate': 'constant', 'power_t': 0.9327643438140221, 'beta_1': 0.36148403156305486, 'beta_2': 0.4585082120361287, 'epsilon': 0.030893303747570156}. Best is trial 0 with value: 1.1514721960078518.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:12:50,196] Trial 3 finished with value: 1.1584997824964542 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 70, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.9558887180536854, 'learning_rate': 'adaptive', 'power_t': 0.9565691392162011, 'beta_1': 0.7547945963353813, 'beta_2': 0.9660214979301578, 'epsilon': 0.7819665494136894}. Best is trial 0 with value: 1.1514721960078518.\n",
      "[I 2024-08-11 13:12:51,961] Trial 4 finished with value: 1.6829500140081106 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.7713825967973327, 'learning_rate': 'constant', 'power_t': 0.7191561520444582, 'beta_1': 0.1418361982930467, 'beta_2': 0.8679778846580093, 'epsilon': 0.21636737608283813}. Best is trial 0 with value: 1.1514721960078518.\n",
      "[I 2024-08-11 13:12:53,440] Trial 5 finished with value: 1.0290029379945054 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.13430498029686475, 'learning_rate': 'adaptive', 'power_t': 0.6875158715435131, 'beta_1': 0.4264082709486088, 'beta_2': 0.8360442141458313, 'epsilon': 0.9812227884987961}. Best is trial 5 with value: 1.0290029379945054.\n",
      "[I 2024-08-11 13:12:54,665] Trial 6 finished with value: 1.4019013244363574 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 100, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.640927233351075, 'learning_rate': 'constant', 'power_t': 0.34038485273965335, 'beta_1': 0.7093729468348576, 'beta_2': 0.512097409476682, 'epsilon': 0.029908998516363677}. Best is trial 5 with value: 1.0290029379945054.\n",
      "[I 2024-08-11 13:12:55,255] Trial 7 finished with value: 1.0271689056764786 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 55, 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.13732074613613357, 'learning_rate': 'adaptive', 'power_t': 0.42237347947298187, 'beta_1': 0.6507311957626798, 'beta_2': 0.4446720651194094, 'epsilon': 0.49083103228204256}. Best is trial 7 with value: 1.0271689056764786.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:12:57,500] Trial 8 finished with value: 1.2676791583758353 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 65, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.916016650666433, 'learning_rate': 'invscaling', 'power_t': 0.9281806210907944, 'beta_1': 0.1293310250038501, 'beta_2': 0.2998347999973005, 'epsilon': 0.44315749308610203}. Best is trial 7 with value: 1.0271689056764786.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:00,504] Trial 9 finished with value: 0.8768910401454475 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 85, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6817757995226055, 'learning_rate': 'invscaling', 'power_t': 0.600206677460487, 'beta_1': 0.4846067603502102, 'beta_2': 0.13086822032671955, 'epsilon': 0.8081528290989863}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:00,676] Trial 10 finished with value: 1.2622988034081652 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.5003051556197592, 'learning_rate': 'invscaling', 'power_t': 0.11135686411870716, 'beta_1': 0.9979441648454104, 'beta_2': 0.10029873487847407, 'epsilon': 0.6590774103733259}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:01,086] Trial 11 finished with value: 1.058454658026488 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0663868368410479, 'learning_rate': 'adaptive', 'power_t': 0.4211097474609455, 'beta_1': 0.6497804951889066, 'beta_2': 0.15316365791400516, 'epsilon': 0.3391476070807805}. Best is trial 9 with value: 0.8768910401454475.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:02,568] Trial 12 finished with value: 0.9602616310923272 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.35539349982065593, 'learning_rate': 'adaptive', 'power_t': 0.36375220298513444, 'beta_1': 0.8709710951959756, 'beta_2': 0.3540683569109977, 'epsilon': 0.6256661650781431}. Best is trial 9 with value: 0.8768910401454475.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:03,765] Trial 13 finished with value: 1.2771747738707186 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.4421648074838355, 'learning_rate': 'invscaling', 'power_t': 0.2381389067251658, 'beta_1': 0.892779092536381, 'beta_2': 0.289794573632148, 'epsilon': 0.7022930198765895}. Best is trial 9 with value: 0.8768910401454475.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:06,249] Trial 14 finished with value: 0.9225070354014259 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5771613886070138, 'learning_rate': 'adaptive', 'power_t': 0.5707255814383188, 'beta_1': 0.5448659100652162, 'beta_2': 0.2843560902523168, 'epsilon': 0.9782798741941803}. Best is trial 9 with value: 0.8768910401454475.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:08,924] Trial 15 finished with value: 0.9353166587490342 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 85, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5733600435579475, 'learning_rate': 'invscaling', 'power_t': 0.5645604985119164, 'beta_1': 0.5448309964046585, 'beta_2': 0.2047896334168232, 'epsilon': 0.9967172904886685}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:10,189] Trial 16 finished with value: 0.9175601506934294 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8135085483029916, 'learning_rate': 'adaptive', 'power_t': 0.5691498490838778, 'beta_1': 0.2775451190030981, 'beta_2': 0.6807882107202311, 'epsilon': 0.858891695125023}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:11,125] Trial 17 finished with value: 0.9735862556719125 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 100, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8163306782523443, 'learning_rate': 'invscaling', 'power_t': 0.8069318019854372, 'beta_1': 0.26111701226912687, 'beta_2': 0.6341200308882747, 'epsilon': 0.8189089489465323}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:12,415] Trial 18 finished with value: 0.9174921091181882 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8137519638666928, 'learning_rate': 'adaptive', 'power_t': 0.5917820779649416, 'beta_1': 0.26788493585499634, 'beta_2': 0.7164693220153675, 'epsilon': 0.8770166734568459}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:13,659] Trial 19 finished with value: 0.9432661986274349 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 95, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.9887178868939793, 'learning_rate': 'invscaling', 'power_t': 0.6768450934019772, 'beta_1': 0.23811953373487954, 'beta_2': 0.6010667418352825, 'epsilon': 0.7492735446656522}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:14,973] Trial 20 finished with value: 0.9186680481705152 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 85, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.6984572193874821, 'learning_rate': 'adaptive', 'power_t': 0.50501987998739, 'beta_1': 0.32518389753354626, 'beta_2': 0.7930407327179366, 'epsilon': 0.9073426474142091}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:16,264] Trial 21 finished with value: 0.917507508347097 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8469641149480323, 'learning_rate': 'adaptive', 'power_t': 0.6217646105324489, 'beta_1': 0.24535489085452747, 'beta_2': 0.6905174135696137, 'epsilon': 0.8780079074174483}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:17,583] Trial 22 finished with value: 0.9176141504307942 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8775569302003138, 'learning_rate': 'adaptive', 'power_t': 0.642171712685697, 'beta_1': 0.48984553874956993, 'beta_2': 0.715667041055389, 'epsilon': 0.8986572313517511}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:18,687] Trial 23 finished with value: 0.9428163722050276 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 95, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.7355419451295682, 'learning_rate': 'adaptive', 'power_t': 0.8156533068532565, 'beta_1': 0.21139252602120115, 'beta_2': 0.6189422720252322, 'epsilon': 0.75997641877617}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:19,785] Trial 24 finished with value: 0.9244671494894648 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 80, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8762291129383903, 'learning_rate': 'adaptive', 'power_t': 0.46474850220407204, 'beta_1': 0.18401570380848664, 'beta_2': 0.5293757728129953, 'epsilon': 0.6155011628669109}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:21,033] Trial 25 finished with value: 0.9964631230713531 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.6257997993038418, 'learning_rate': 'invscaling', 'power_t': 0.6158107658501949, 'beta_1': 0.34192845804596617, 'beta_2': 0.8714401596235478, 'epsilon': 0.9091840104860248}. Best is trial 9 with value: 0.8768910401454475.\n",
      "[I 2024-08-11 13:13:22,763] Trial 26 finished with value: 0.8743252614506302 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 85, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8304438798329598, 'learning_rate': 'adaptive', 'power_t': 0.7573355028925961, 'beta_1': 0.28635177104269194, 'beta_2': 0.6926396123735873, 'epsilon': 0.6882096823248522}. Best is trial 26 with value: 0.8743252614506302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:25,901] Trial 27 finished with value: 0.8338503906692363 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6572339382285273, 'learning_rate': 'adaptive', 'power_t': 0.8758049754699853, 'beta_1': 0.4837408905356175, 'beta_2': 0.41099650687685385, 'epsilon': 0.7258234344989474}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:28,259] Trial 28 finished with value: 0.9596164824754675 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.654425524116489, 'learning_rate': 'constant', 'power_t': 0.8678725432222906, 'beta_1': 0.4885153550099752, 'beta_2': 0.39463449457765354, 'epsilon': 0.7003528275350447}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:30,608] Trial 29 finished with value: 0.957043244693006 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5172796921378704, 'learning_rate': 'invscaling', 'power_t': 0.7393761512672089, 'beta_1': 0.6107811238147844, 'beta_2': 0.1963529008839267, 'epsilon': 0.5499146325258226}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:33,217] Trial 30 finished with value: 1.0077438046048006 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7140848268220419, 'learning_rate': 'constant', 'power_t': 0.8439099247822466, 'beta_1': 0.40172645953199465, 'beta_2': 0.5696488613880322, 'epsilon': 0.3538095899401847}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:35,562] Trial 31 finished with value: 1.0582527712555285 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 85, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7544963119135901, 'learning_rate': 'adaptive', 'power_t': 0.8813449045655365, 'beta_1': 0.4962080721200113, 'beta_2': 0.7529751058707408, 'epsilon': 0.8162055158946891}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:38,135] Trial 32 finished with value: 1.0943565355179568 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 85, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.7915667692332998, 'learning_rate': 'adaptive', 'power_t': 0.738993577755879, 'beta_1': 0.3102247893534926, 'beta_2': 0.9796374924355813, 'epsilon': 0.7252767432203042}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:39,986] Trial 33 finished with value: 1.2468194091331497 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6806470423937919, 'learning_rate': 'adaptive', 'power_t': 0.7637305918323843, 'beta_1': 0.3899853887467444, 'beta_2': 0.6643915960569862, 'epsilon': 0.6587278501850569}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:40,968] Trial 34 finished with value: 0.9297216123647607 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 75, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.5859698968274021, 'learning_rate': 'adaptive', 'power_t': 0.5088975927917743, 'beta_1': 0.448821102811645, 'beta_2': 0.4857383514503607, 'epsilon': 0.5679887158151261}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:43,164] Trial 35 finished with value: 1.4294449759544876 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 95, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9228649667713995, 'learning_rate': 'adaptive', 'power_t': 0.9875087530016394, 'beta_1': 0.5896969116167593, 'beta_2': 0.9163898685196313, 'epsilon': 0.8114736999761105}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:45,546] Trial 36 finished with value: 1.3263199055753048 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 85, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.7418043933897736, 'learning_rate': 'constant', 'power_t': 0.6865817539332939, 'beta_1': 0.3563961810930459, 'beta_2': 0.7753374430024165, 'epsilon': 0.7785113711086101}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:46,185] Trial 37 finished with value: 0.9659269019612834 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 80, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.41222629486620366, 'learning_rate': 'adaptive', 'power_t': 0.9105249542752891, 'beta_1': 0.1622135869949199, 'beta_2': 0.40801952773331907, 'epsilon': 0.1302120193447458}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:49,283] Trial 38 finished with value: 8.479838121742628 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 65, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.9941663522061343, 'learning_rate': 'invscaling', 'power_t': 0.800716414604074, 'beta_1': 0.44683458585515506, 'beta_2': 0.572510647005837, 'epsilon': 0.8419428507701923}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:51,091] Trial 39 finished with value: 1.4379971581398467 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 75, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.6188852348409984, 'learning_rate': 'adaptive', 'power_t': 0.6700558533385381, 'beta_1': 0.7419781597942372, 'beta_2': 0.35042241181827616, 'epsilon': 0.9277043059952853}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:13:53,334] Trial 40 finished with value: 0.9902149769824646 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 50, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.26081610296266367, 'learning_rate': 'constant', 'power_t': 0.9983291013671556, 'beta_1': 0.3003332911611821, 'beta_2': 0.48177743610887813, 'epsilon': 0.49982036215537756}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:55,133] Trial 41 finished with value: 0.9176205036758551 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8450117379798391, 'learning_rate': 'adaptive', 'power_t': 0.6124790138566328, 'beta_1': 0.11186694005876974, 'beta_2': 0.7046933095678901, 'epsilon': 0.8740105699996848}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:56,478] Trial 42 finished with value: 1.025914136160184 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8682495650204863, 'learning_rate': 'adaptive', 'power_t': 0.617247509502222, 'beta_1': 0.2271384109105484, 'beta_2': 0.8241099841054904, 'epsilon': 0.9442186661666849}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:57,626] Trial 43 finished with value: 0.9714611917787727 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 85, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.9235411893025941, 'learning_rate': 'adaptive', 'power_t': 0.5166240094415245, 'beta_1': 0.18356799270740984, 'beta_2': 0.7413545546855931, 'epsilon': 0.7746366105033416}. Best is trial 27 with value: 0.8338503906692363.\n",
      "[I 2024-08-11 13:13:57,973] Trial 44 finished with value: 1.0484353906371768 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.7975687213268758, 'learning_rate': 'adaptive', 'power_t': 0.7020643163687594, 'beta_1': 0.37228274024944724, 'beta_2': 0.6574110435073846, 'epsilon': 0.6724517370751097}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:00,224] Trial 45 finished with value: 1.445270382713957 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 100, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.7635840811139037, 'learning_rate': 'adaptive', 'power_t': 0.7698356353515123, 'beta_1': 0.27172587900893036, 'beta_2': 0.10784116603213986, 'epsilon': 0.5914980292793637}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:03,676] Trial 46 finished with value: 0.8655750416557456 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6683173007063927, 'learning_rate': 'invscaling', 'power_t': 0.4519116356296241, 'beta_1': 0.43329284813525915, 'beta_2': 0.2178923309010659, 'epsilon': 0.8584473566104759}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:07,258] Trial 47 finished with value: 1.061303005417126 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.5372858640284649, 'learning_rate': 'invscaling', 'power_t': 0.39768997640848613, 'beta_1': 0.44683854594201466, 'beta_2': 0.2314040951802601, 'epsilon': 0.4326562711783855}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:10,476] Trial 48 finished with value: 0.9370977806347444 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6624481109707728, 'learning_rate': 'invscaling', 'power_t': 0.3148089798788163, 'beta_1': 0.5679379895211512, 'beta_2': 0.14232544633591127, 'epsilon': 0.7257980918550172}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:13,869] Trial 49 finished with value: 0.8644634605291452 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7015623148384724, 'learning_rate': 'invscaling', 'power_t': 0.4699818994221253, 'beta_1': 0.5137168472843107, 'beta_2': 0.2578586491029195, 'epsilon': 0.8209546025915035}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:14,788] Trial 50 finished with value: 1.2620010796319647 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.4700995313261367, 'learning_rate': 'invscaling', 'power_t': 0.4611576547730102, 'beta_1': 0.6615515044637232, 'beta_2': 0.2592353672126927, 'epsilon': 0.9615888355856315}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:18,414] Trial 51 finished with value: 0.8648362888031222 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6979066715051443, 'learning_rate': 'invscaling', 'power_t': 0.4608353852444141, 'beta_1': 0.5150116789178764, 'beta_2': 0.17811779035761494, 'epsilon': 0.8373342442868289}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:21,447] Trial 52 finished with value: 0.936501729601947 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7053137636333456, 'learning_rate': 'invscaling', 'power_t': 0.2846019600398254, 'beta_1': 0.5347968983845576, 'beta_2': 0.16146052011002404, 'epsilon': 0.8186085257158656}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:25,740] Trial 53 finished with value: 0.8651044167307178 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5489162429594627, 'learning_rate': 'invscaling', 'power_t': 0.45144734008048737, 'beta_1': 0.5123132370480883, 'beta_2': 0.3214134359985838, 'epsilon': 0.8394535312679482}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:29,088] Trial 54 finished with value: 0.8627060019763148 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6031378676018669, 'learning_rate': 'invscaling', 'power_t': 0.381653892897183, 'beta_1': 0.6286237366454017, 'beta_2': 0.3571611487464641, 'epsilon': 0.7351092812837802}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:32,569] Trial 55 finished with value: 0.8650145908214183 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.557138327026769, 'learning_rate': 'invscaling', 'power_t': 0.3858764589264927, 'beta_1': 0.5191741327507366, 'beta_2': 0.32610565345135867, 'epsilon': 0.8385092110523759}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:34,443] Trial 56 finished with value: 1.0462727975174035 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 60, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.5503501104959533, 'learning_rate': 'invscaling', 'power_t': 0.37356059480185394, 'beta_1': 0.6196768965162904, 'beta_2': 0.3366160445227279, 'epsilon': 0.7372879759070451}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:37,144] Trial 57 finished with value: 0.9675935024765758 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6114486165960301, 'learning_rate': 'invscaling', 'power_t': 0.2674875709035452, 'beta_1': 0.6878607367339568, 'beta_2': 0.41731476840245524, 'epsilon': 0.7858651040552829}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:39,876] Trial 58 finished with value: 1.0976990150446644 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.3967186605177617, 'learning_rate': 'invscaling', 'power_t': 0.42409598507321444, 'beta_1': 0.5159486006439568, 'beta_2': 0.31299975451371953, 'epsilon': 0.8470423053226472}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:42,889] Trial 59 finished with value: 0.9386684230867626 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.4877980472049646, 'learning_rate': 'invscaling', 'power_t': 0.1939410402536216, 'beta_1': 0.8014034080063615, 'beta_2': 0.37653928239926593, 'epsilon': 0.6375033368720838}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:45,489] Trial 60 finished with value: 1.1011872083968068 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5900724946927903, 'learning_rate': 'invscaling', 'power_t': 0.33438936642181877, 'beta_1': 0.5769090082347281, 'beta_2': 0.26205148985529086, 'epsilon': 0.9236425999941507}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:48,821] Trial 61 finished with value: 0.8658316286000023 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5629670607444582, 'learning_rate': 'invscaling', 'power_t': 0.4634736086039609, 'beta_1': 0.4227274098541882, 'beta_2': 0.1770196996786848, 'epsilon': 0.8619752699756017}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:52,045] Trial 62 finished with value: 0.8638922556675059 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6412608295598933, 'learning_rate': 'invscaling', 'power_t': 0.5410474814499174, 'beta_1': 0.5176747195677623, 'beta_2': 0.22785854112600082, 'epsilon': 0.7890743854693189}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:55,963] Trial 63 finished with value: 0.9364892581431258 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5187105039819588, 'learning_rate': 'invscaling', 'power_t': 0.539919451575908, 'beta_1': 0.5249214339393145, 'beta_2': 0.31260281076445307, 'epsilon': 0.7896632198559219}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:14:58,413] Trial 64 finished with value: 1.0958531093050619 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6385487894647972, 'learning_rate': 'invscaling', 'power_t': 0.3973243671615959, 'beta_1': 0.46980968835706355, 'beta_2': 0.2579682252496597, 'epsilon': 0.7449390586364216}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:01,386] Trial 65 finished with value: 0.8654866190073779 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 90, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.4654430314669243, 'learning_rate': 'invscaling', 'power_t': 0.491211142363664, 'beta_1': 0.6248863905309112, 'beta_2': 0.44712218630199496, 'epsilon': 0.705478367428997}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:04,122] Trial 66 finished with value: 1.0989932260358584 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6052199712795376, 'learning_rate': 'invscaling', 'power_t': 0.37425132391700994, 'beta_1': 0.5501755648575819, 'beta_2': 0.23569395484672817, 'epsilon': 0.895250816120773}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:07,090] Trial 67 finished with value: 0.8706014274221022 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 90, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.3239031866974166, 'learning_rate': 'invscaling', 'power_t': 0.4270574209242176, 'beta_1': 0.5124316236856328, 'beta_2': 0.2883598485557287, 'epsilon': 0.8346188350776942}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:09,097] Trial 68 finished with value: 1.2412480811307756 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7189642226781436, 'learning_rate': 'invscaling', 'power_t': 0.5452269657332254, 'beta_1': 0.5572648125103546, 'beta_2': 0.33518346568000645, 'epsilon': 0.9700546291201556}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:12,590] Trial 69 finished with value: 1.3135062757318043 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 90, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.6427719289905922, 'learning_rate': 'invscaling', 'power_t': 0.4860515807221293, 'beta_1': 0.4733243541517201, 'beta_2': 0.3727062017871245, 'epsilon': 0.762806375316564}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:15,840] Trial 70 finished with value: 0.8639489590988848 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6887255960029864, 'learning_rate': 'invscaling', 'power_t': 0.3520327570217656, 'beta_1': 0.5901553597302753, 'beta_2': 0.20619009006989103, 'epsilon': 0.8065158158742568}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:19,122] Trial 71 finished with value: 0.8637832016461392 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6868056058008536, 'learning_rate': 'invscaling', 'power_t': 0.3077369206252563, 'beta_1': 0.5959495560333181, 'beta_2': 0.19451815367601993, 'epsilon': 0.8009745555976712}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:22,107] Trial 72 finished with value: 0.93664365365573 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6948926175338359, 'learning_rate': 'invscaling', 'power_t': 0.3381391820706024, 'beta_1': 0.6005072884077428, 'beta_2': 0.18392994607975058, 'epsilon': 0.798409387130418}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:25,545] Trial 73 finished with value: 1.0990189369464036 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7237077640301681, 'learning_rate': 'invscaling', 'power_t': 0.2971788926165181, 'beta_1': 0.6682259835351286, 'beta_2': 0.11881451378244857, 'epsilon': 0.7131768556480776}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:28,356] Trial 74 finished with value: 0.8671992969037188 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 90, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6756375576280905, 'learning_rate': 'invscaling', 'power_t': 0.2133508205772106, 'beta_1': 0.6374475011934175, 'beta_2': 0.20353709502209993, 'epsilon': 0.7590903000488535}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:31,239] Trial 75 finished with value: 1.1786196739146937 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 95, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.6423566693962109, 'learning_rate': 'invscaling', 'power_t': 0.3952746883092527, 'beta_1': 0.7028744183072051, 'beta_2': 0.2804528566613789, 'epsilon': 0.6789933186862864}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:34,362] Trial 76 finished with value: 0.9369946089885645 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7747156027191647, 'learning_rate': 'constant', 'power_t': 0.17346148084754576, 'beta_1': 0.5897263865494796, 'beta_2': 0.15614065849046133, 'epsilon': 0.8886648185734161}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:36,584] Trial 77 finished with value: 1.3692828975411573 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0006688493081924429, 'learning_rate': 'invscaling', 'power_t': 0.35319403873320115, 'beta_1': 0.569224867558522, 'beta_2': 0.23590002965784695, 'epsilon': 0.6456438160722849}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:39,133] Trial 78 finished with value: 0.9555581728337199 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5918804505924191, 'learning_rate': 'invscaling', 'power_t': 0.24148208880450842, 'beta_1': 0.46365272656004164, 'beta_2': 0.17664751036197188, 'epsilon': 0.8031287108674597}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:40,961] Trial 79 finished with value: 1.2443642093969063 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 100, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7514591938031643, 'learning_rate': 'invscaling', 'power_t': 0.575546805757162, 'beta_1': 0.49532847142499753, 'beta_2': 0.2690969463008148, 'epsilon': 0.8271716473201739}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:42,504] Trial 80 finished with value: 1.1756317075852882 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6977289099437997, 'learning_rate': 'invscaling', 'power_t': 0.43442380164744254, 'beta_1': 0.5477308938473892, 'beta_2': 0.1335445199318842, 'epsilon': 0.9430624427552304}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:45,739] Trial 81 finished with value: 0.8653927812564053 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5329409223825413, 'learning_rate': 'invscaling', 'power_t': 0.31391047429144764, 'beta_1': 0.5118173199201306, 'beta_2': 0.30305182110743234, 'epsilon': 0.8506785005459017}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:49,060] Trial 82 finished with value: 0.8634451522735904 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5605498689831778, 'learning_rate': 'invscaling', 'power_t': 0.5198014555305037, 'beta_1': 0.6034182768159327, 'beta_2': 0.42512289492004723, 'epsilon': 0.7702180120121498}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:51,939] Trial 83 finished with value: 0.8665210420922744 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 90, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6257527254373556, 'learning_rate': 'invscaling', 'power_t': 0.4856961602692736, 'beta_1': 0.6402061038637611, 'beta_2': 0.427593421206442, 'epsilon': 0.7424157620129255}. Best is trial 27 with value: 0.8338503906692363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:15:56,540] Trial 84 finished with value: 0.8268120935084502 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5776640451067258, 'learning_rate': 'invscaling', 'power_t': 0.5237159182714396, 'beta_1': 0.6031555193779572, 'beta_2': 0.38348157637125535, 'epsilon': 0.7825312534003723}. Best is trial 84 with value: 0.8268120935084502.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:00,089] Trial 85 finished with value: 0.8265602505450179 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6827284194196025, 'learning_rate': 'constant', 'power_t': 0.6459117049409494, 'beta_1': 0.7242119291395503, 'beta_2': 0.37941149340407615, 'epsilon': 0.770025578592326}. Best is trial 85 with value: 0.8265602505450179.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:03,748] Trial 86 finished with value: 0.8186207557221927 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.669006273158101, 'learning_rate': 'constant', 'power_t': 0.515437219039179, 'beta_1': 0.7415721574615495, 'beta_2': 0.3843191938964469, 'epsilon': 0.6910239638940974}. Best is trial 86 with value: 0.8186207557221927.\n",
      "[I 2024-08-11 13:16:03,966] Trial 87 finished with value: 1.0329843501389306 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.6583824196390133, 'learning_rate': 'constant', 'power_t': 0.6485557947432634, 'beta_1': 0.7819457448637904, 'beta_2': 0.48102671032475275, 'epsilon': 0.6012551562366228}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:06,225] Trial 88 finished with value: 1.0925205098582151 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 65, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.586562557456186, 'learning_rate': 'constant', 'power_t': 0.5303573074650463, 'beta_1': 0.8020949099263087, 'beta_2': 0.39035356792287174, 'epsilon': 0.6701313729605435}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:07,605] Trial 89 finished with value: 1.7559172285095876 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.5060705786315534, 'learning_rate': 'constant', 'power_t': 0.5859230932801336, 'beta_1': 0.8846406679857778, 'beta_2': 0.35601560136063004, 'epsilon': 0.7727604473376409}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:09,819] Trial 90 finished with value: 1.1444270911402121 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6162693240222671, 'learning_rate': 'constant', 'power_t': 0.5622192172296876, 'beta_1': 0.7433269161347582, 'beta_2': 0.5165609217812177, 'epsilon': 0.7193650538503794}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:13,443] Trial 91 finished with value: 0.8322711890235952 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6816230929887126, 'learning_rate': 'constant', 'power_t': 0.49106315978321463, 'beta_1': 0.8483760700705664, 'beta_2': 0.44048621042013414, 'epsilon': 0.8025122970979649}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:16,972] Trial 92 finished with value: 0.821467918512065 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7352625943878791, 'learning_rate': 'constant', 'power_t': 0.5075515095909611, 'beta_1': 0.8534149294290069, 'beta_2': 0.4637170704317054, 'epsilon': 0.6965325684820389}. Best is trial 86 with value: 0.8186207557221927.\n",
      "[I 2024-08-11 13:16:17,455] Trial 93 finished with value: 1.358301485614092 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7301858890594717, 'learning_rate': 'constant', 'power_t': 0.5137767458920696, 'beta_1': 0.96291076595494, 'beta_2': 0.46129841462714877, 'epsilon': 0.7443023360104056}. Best is trial 86 with value: 0.8186207557221927.\n",
      "[I 2024-08-11 13:16:17,910] Trial 94 finished with value: 1.2044913157938135 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 65, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.7790766561380239, 'learning_rate': 'constant', 'power_t': 0.5278802953949395, 'beta_1': 0.949487060628814, 'beta_2': 0.4333996937662925, 'epsilon': 0.538204687789726}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:20,032] Trial 95 finished with value: 1.0240064103411763 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6638463389764381, 'learning_rate': 'constant', 'power_t': 0.6367046114891741, 'beta_1': 0.8107877918250961, 'beta_2': 0.39923596428688374, 'epsilon': 0.6937627483668539}. Best is trial 86 with value: 0.8186207557221927.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:24,590] Trial 96 finished with value: 0.8121128541077948 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.568674033365129, 'learning_rate': 'constant', 'power_t': 0.5939179859719839, 'beta_1': 0.7682300406257694, 'beta_2': 0.5333136354178948, 'epsilon': 0.6196593562061333}. Best is trial 96 with value: 0.8121128541077948.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:27,059] Trial 97 finished with value: 1.0836147664138385 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.5706335031478427, 'learning_rate': 'constant', 'power_t': 0.5899197981312916, 'beta_1': 0.8261616380870885, 'beta_2': 0.55158053479653, 'epsilon': 0.6158875063996228}. Best is trial 96 with value: 0.8121128541077948.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:29,095] Trial 98 finished with value: 0.8567006553320786 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 65, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6747320831666859, 'learning_rate': 'constant', 'power_t': 0.9673103510618587, 'beta_1': 0.8518813411739337, 'beta_2': 0.4643789324261359, 'epsilon': 0.6926667157275762}. Best is trial 96 with value: 0.8121128541077948.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2024-08-11 13:16:31,432] A new study created in RDB with name: Hyperparameter Tuning - MLP + Station 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:31,375] Trial 99 finished with value: 1.0241604901077797 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 60, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.6094778606961493, 'learning_rate': 'constant', 'power_t': 0.9660218896825568, 'beta_1': 0.8429221850425154, 'beta_2': 0.4637215729405064, 'epsilon': 0.6519405765061799}. Best is trial 96 with value: 0.8121128541077948.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45d926bfcd84641beaecd8d8db5e607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:33,749] Trial 0 finished with value: 1.2994711804074168 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 55, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.7827692194722012, 'learning_rate': 'invscaling', 'power_t': 0.7642157624810318, 'beta_1': 0.762938009377711, 'beta_2': 0.3286965516846911, 'epsilon': 0.7413073578141726}. Best is trial 0 with value: 1.2994711804074168.\n",
      "[I 2024-08-11 13:16:33,934] Trial 1 finished with value: 2.2616346998096506 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 70, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.28907227395782054, 'learning_rate': 'invscaling', 'power_t': 0.14718034528484725, 'beta_1': 0.9987113892793155, 'beta_2': 0.9329897495839006, 'epsilon': 0.047676642350684945}. Best is trial 0 with value: 1.2994711804074168.\n",
      "[I 2024-08-11 13:16:34,264] Trial 2 finished with value: 1.4692662634045939 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 95, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.9219929896976066, 'learning_rate': 'constant', 'power_t': 0.4861853181428558, 'beta_1': 0.48094733504719955, 'beta_2': 0.40979241115871434, 'epsilon': 0.562263329897514}. Best is trial 0 with value: 1.2994711804074168.\n",
      "[I 2024-08-11 13:16:34,814] Trial 3 finished with value: 1.2969641378351222 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.8345743814116158, 'learning_rate': 'invscaling', 'power_t': 0.11498163157374942, 'beta_1': 0.555095939208029, 'beta_2': 0.3099893771388068, 'epsilon': 0.15741298312281707}. Best is trial 3 with value: 1.2969641378351222.\n",
      "[I 2024-08-11 13:16:36,699] Trial 4 finished with value: 1.0708358745972526 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 65, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.4291409683928262, 'learning_rate': 'invscaling', 'power_t': 0.37652652405951204, 'beta_1': 0.3732892982613235, 'beta_2': 0.8774802517716428, 'epsilon': 0.35585262632726244}. Best is trial 4 with value: 1.0708358745972526.\n",
      "[I 2024-08-11 13:16:37,561] Trial 5 finished with value: 1.2106443885564793 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 80, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.24997087596688822, 'learning_rate': 'adaptive', 'power_t': 0.9043702412591215, 'beta_1': 0.9666766846808997, 'beta_2': 0.4397473454424694, 'epsilon': 0.3575414347128803}. Best is trial 4 with value: 1.0708358745972526.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:41,058] Trial 6 finished with value: 1.0248984123237626 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.5054821758101073, 'learning_rate': 'adaptive', 'power_t': 0.4999964243080346, 'beta_1': 0.5830491745966377, 'beta_2': 0.8097503460428133, 'epsilon': 0.41163046087009203}. Best is trial 6 with value: 1.0248984123237626.\n",
      "[I 2024-08-11 13:16:42,294] Trial 7 finished with value: 1.0098924504179452 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 55, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.5828403378409889, 'learning_rate': 'invscaling', 'power_t': 0.4757329305514041, 'beta_1': 0.35672140946442654, 'beta_2': 0.6586998570115115, 'epsilon': 0.41091569750305523}. Best is trial 7 with value: 1.0098924504179452.\n",
      "[I 2024-08-11 13:16:43,304] Trial 8 finished with value: 1.0912519166262378 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 90, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8373778793749842, 'learning_rate': 'invscaling', 'power_t': 0.8796399644312448, 'beta_1': 0.793361861601442, 'beta_2': 0.4320207446002665, 'epsilon': 0.8539810907738946}. Best is trial 7 with value: 1.0098924504179452.\n",
      "[I 2024-08-11 13:16:44,120] Trial 9 finished with value: 1.1291906700378564 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 50, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.7077777894885364, 'learning_rate': 'adaptive', 'power_t': 0.741088208279198, 'beta_1': 0.27406590998589286, 'beta_2': 0.8073232689876277, 'epsilon': 0.8376278022801633}. Best is trial 7 with value: 1.0098924504179452.\n",
      "[I 2024-08-11 13:16:45,364] Trial 10 finished with value: 1.0544439429561077 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 60, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.007344615797127918, 'learning_rate': 'constant', 'power_t': 0.37049974956048687, 'beta_1': 0.1437400497624129, 'beta_2': 0.11431174846988895, 'epsilon': 0.5998483743945915}. Best is trial 7 with value: 1.0098924504179452.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:48,328] Trial 11 finished with value: 1.1451935942104723 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 85, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.5308299259469136, 'learning_rate': 'adaptive', 'power_t': 0.6117986700787367, 'beta_1': 0.6099278263597283, 'beta_2': 0.6714658506401562, 'epsilon': 0.33003988594572353}. Best is trial 7 with value: 1.0098924504179452.\n",
      "[I 2024-08-11 13:16:49,415] Trial 12 finished with value: 1.086028491572985 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 65, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.5521875899971528, 'learning_rate': 'adaptive', 'power_t': 0.5325487369447944, 'beta_1': 0.38185132073694505, 'beta_2': 0.6771883118547735, 'epsilon': 0.4826104013385423}. Best is trial 7 with value: 1.0098924504179452.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:52,265] Trial 13 finished with value: 1.0011334765431925 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.6747338867922914, 'learning_rate': 'adaptive', 'power_t': 0.3076455071507617, 'beta_1': 0.6771579214395176, 'beta_2': 0.7220201001832988, 'epsilon': 0.21250943374103232}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:16:53,497] Trial 14 finished with value: 1.0863227443718917 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.9973238200719481, 'learning_rate': 'constant', 'power_t': 0.27818854109557656, 'beta_1': 0.7472138796375589, 'beta_2': 0.6282910074760404, 'epsilon': 0.18435228322188862}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:16:56,392] Trial 15 finished with value: 1.0459690416319976 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.6436917854988153, 'learning_rate': 'invscaling', 'power_t': 0.2589107369079252, 'beta_1': 0.11514967631359693, 'beta_2': 0.6056465116818676, 'epsilon': 0.18693503749148233}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:16:57,690] Trial 16 finished with value: 1.0308885093326547 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 60, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.32220524574132914, 'learning_rate': 'adaptive', 'power_t': 0.6544377498073435, 'beta_1': 0.27004045381442165, 'beta_2': 0.7717915073558356, 'epsilon': 0.028625156209032776}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:16:58,745] Trial 17 finished with value: 1.0633983350156497 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 50, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.6528116118923578, 'learning_rate': 'invscaling', 'power_t': 0.39565354397403973, 'beta_1': 0.6703766233486659, 'beta_2': 0.5193878978464489, 'epsilon': 0.22122412824372428}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:00,094] Trial 18 finished with value: 1.180384790174846 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 60, 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.14667601795622387, 'learning_rate': 'adaptive', 'power_t': 0.22449443569373478, 'beta_1': 0.4556181810084984, 'beta_2': 0.9864907385381589, 'epsilon': 0.631571757934611}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:03,350] Trial 19 finished with value: 1.3184470742089514 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.40567761171781036, 'learning_rate': 'constant', 'power_t': 0.42282116235677447, 'beta_1': 0.8406477331279232, 'beta_2': 0.7343297736035488, 'epsilon': 0.9603894694572707}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:04,337] Trial 20 finished with value: 1.0199656391153549 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 75, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.723648498139331, 'learning_rate': 'adaptive', 'power_t': 0.29589857850885115, 'beta_1': 0.24112958945532842, 'beta_2': 0.5378454913037458, 'epsilon': 0.29581795606396244}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:05,354] Trial 21 finished with value: 1.0307504960507825 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 100, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.7161576961832427, 'learning_rate': 'adaptive', 'power_t': 0.30657014114836967, 'beta_1': 0.24539845318681294, 'beta_2': 0.5347143741116032, 'epsilon': 0.3057931368110313}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:06,363] Trial 22 finished with value: 1.0674441983165646 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 80, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.6043164973361581, 'learning_rate': 'adaptive', 'power_t': 0.18236931707083756, 'beta_1': 0.19873770865016757, 'beta_2': 0.5638164325754728, 'epsilon': 0.48080454990166177}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:07,185] Trial 23 finished with value: 1.1226625805616453 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 70, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.7680151067385118, 'learning_rate': 'adaptive', 'power_t': 0.3300677999816352, 'beta_1': 0.339157567079603, 'beta_2': 0.7085215706944386, 'epsilon': 0.10629448814650447}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:08,221] Trial 24 finished with value: 1.0673551152456373 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 65, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.4446060582821593, 'learning_rate': 'adaptive', 'power_t': 0.45666065427825087, 'beta_1': 0.47745462707579894, 'beta_2': 0.4979390675544409, 'epsilon': 0.26347965704156506}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:09,423] Trial 25 finished with value: 1.010340261646054 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 55, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.6027503371182218, 'learning_rate': 'invscaling', 'power_t': 0.6187689524822626, 'beta_1': 0.6554132016084102, 'beta_2': 0.8748632050981566, 'epsilon': 0.41043500501719987}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:13,068] Trial 26 finished with value: 6.545514960501985 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 55, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.607237608878894, 'learning_rate': 'invscaling', 'power_t': 0.6415674503269608, 'beta_1': 0.6829695473406248, 'beta_2': 0.887727147893337, 'epsilon': 0.41106104904047425}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:14,054] Trial 27 finished with value: 1.3455612726431876 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 50, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.37526834121918484, 'learning_rate': 'invscaling', 'power_t': 0.6011020224825088, 'beta_1': 0.8884567576681239, 'beta_2': 0.8586034538238055, 'epsilon': 0.5291936540741099}. Best is trial 13 with value: 1.0011334765431925.\n",
      "[I 2024-08-11 13:17:15,754] Trial 28 finished with value: 1.054702466055971 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 60, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.5640830523725179, 'learning_rate': 'invscaling', 'power_t': 0.7322018215632495, 'beta_1': 0.6685501494920874, 'beta_2': 0.9881282638492122, 'epsilon': 0.6857070624456567}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:17,561] Trial 29 finished with value: 1.304309731177043 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 55, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.47433587478921213, 'learning_rate': 'invscaling', 'power_t': 0.8284222860085096, 'beta_1': 0.7379033784753309, 'beta_2': 0.7410445357017398, 'epsilon': 0.45592764886245696}. Best is trial 13 with value: 1.0011334765431925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:19,986] Trial 30 finished with value: 0.96513670933273 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8275275700984093, 'learning_rate': 'invscaling', 'power_t': 0.5550696128288299, 'beta_1': 0.5137460373433684, 'beta_2': 0.8109792411058384, 'epsilon': 0.12407292224131627}. Best is trial 30 with value: 0.96513670933273.\n",
      "[I 2024-08-11 13:17:23,698] Trial 31 finished with value: 0.9643893113028126 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8290252296750256, 'learning_rate': 'invscaling', 'power_t': 0.5571684446857823, 'beta_1': 0.5121719287265671, 'beta_2': 0.8193348296693652, 'epsilon': 0.07359401614510264}. Best is trial 31 with value: 0.9643893113028126.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:30,594] Trial 32 finished with value: 0.9830661573415437 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8926499083349078, 'learning_rate': 'invscaling', 'power_t': 0.5227296314741692, 'beta_1': 0.5106956951483752, 'beta_2': 0.6378101837488677, 'epsilon': 0.08907950893479731}. Best is trial 31 with value: 0.9643893113028126.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:34,387] Trial 33 finished with value: 0.9567028896749239 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8926627969804695, 'learning_rate': 'invscaling', 'power_t': 0.6925651855677544, 'beta_1': 0.5269376327932365, 'beta_2': 0.8054871929331829, 'epsilon': 0.08377946494369572}. Best is trial 33 with value: 0.9567028896749239.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:38,117] Trial 34 finished with value: 0.9433466888402497 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9052761038290978, 'learning_rate': 'invscaling', 'power_t': 0.6958775707527675, 'beta_1': 0.4938941140922418, 'beta_2': 0.8091234715774228, 'epsilon': 0.09069100582414569}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:41,439] Trial 35 finished with value: 7.029491687544812 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.9821142819382083, 'learning_rate': 'invscaling', 'power_t': 0.9867515789416362, 'beta_1': 0.42562147621242336, 'beta_2': 0.9331541942202515, 'epsilon': 0.11075375391367279}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:43,145] Trial 36 finished with value: 1.2174458835784052 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 60, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9095383933819055, 'learning_rate': 'invscaling', 'power_t': 0.6875214260591561, 'beta_1': 0.5281432176932798, 'beta_2': 0.8069760087526775, 'epsilon': 0.046387470040660087}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:45,363] Trial 37 finished with value: 1.0772240142193255 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 65, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8283794184110592, 'learning_rate': 'invscaling', 'power_t': 0.8049159082204052, 'beta_1': 0.5669749823415992, 'beta_2': 0.9304788827322208, 'epsilon': 0.14004429077991726}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:49,408] Trial 38 finished with value: 2.730176527311786 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 55, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.8026367457025901, 'learning_rate': 'invscaling', 'power_t': 0.564018203196209, 'beta_1': 0.41800704061045046, 'beta_2': 0.8348349451320178, 'epsilon': 0.0016296042381292314}. Best is trial 34 with value: 0.9433466888402497.\n",
      "[I 2024-08-11 13:17:50,754] Trial 39 finished with value: 1.183745485974052 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9489794903322626, 'learning_rate': 'invscaling', 'power_t': 0.6873171132532023, 'beta_1': 0.6109632653702878, 'beta_2': 0.24950631307214316, 'epsilon': 0.08435794369411927}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:53,749] Trial 40 finished with value: 1.0733010402922682 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 60, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8747264667473414, 'learning_rate': 'invscaling', 'power_t': 0.563081791683604, 'beta_1': 0.5149976159847875, 'beta_2': 0.9248828500459715, 'epsilon': 0.2374351113504371}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:56,285] Trial 41 finished with value: 0.9780206887908586 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8873359296311322, 'learning_rate': 'invscaling', 'power_t': 0.5246170460327912, 'beta_1': 0.508915797778479, 'beta_2': 0.7816710292811088, 'epsilon': 0.09608028726980332}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:17:57,910] Trial 42 finished with value: 1.197859306212256 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.7769426655927285, 'learning_rate': 'invscaling', 'power_t': 0.6877300715847059, 'beta_1': 0.439689983431496, 'beta_2': 0.7652798865107042, 'epsilon': 0.155716236863645}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:00,647] Trial 43 finished with value: 0.9619479438188883 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8639826194594252, 'learning_rate': 'invscaling', 'power_t': 0.7859023804513421, 'beta_1': 0.5540037992768908, 'beta_2': 0.7876463562403463, 'epsilon': 0.06609637843248325}. Best is trial 34 with value: 0.9433466888402497.\n",
      "[I 2024-08-11 13:18:01,807] Trial 44 finished with value: 0.9860917731986539 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9529463507001779, 'learning_rate': 'invscaling', 'power_t': 0.7603922784778872, 'beta_1': 0.6004395434658052, 'beta_2': 0.837626185861005, 'epsilon': 0.00943050909091607}. Best is trial 34 with value: 0.9433466888402497.\n",
      "[I 2024-08-11 13:18:02,873] Trial 45 finished with value: 1.1441234649961298 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 55, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.8399647772707174, 'learning_rate': 'constant', 'power_t': 0.8136707534764075, 'beta_1': 0.5595606818332017, 'beta_2': 0.8940665946430895, 'epsilon': 0.06798327606984489}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:05,902] Trial 46 finished with value: 6.727102315517316 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 50, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.8609142850175028, 'learning_rate': 'invscaling', 'power_t': 0.8767013738698682, 'beta_1': 0.3904501371982063, 'beta_2': 0.6980186276179976, 'epsilon': 0.1393086749249815}. Best is trial 34 with value: 0.9433466888402497.\n",
      "[I 2024-08-11 13:18:07,654] Trial 47 finished with value: 1.1329035826592875 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.7573693537559607, 'learning_rate': 'invscaling', 'power_t': 0.7222472810621686, 'beta_1': 0.48611933110975075, 'beta_2': 0.8164315161355671, 'epsilon': 0.048690089198494164}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:09,573] Trial 48 finished with value: 1.3058900540135203 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9439219952399514, 'learning_rate': 'invscaling', 'power_t': 0.7728704454676488, 'beta_1': 0.6287831440503568, 'beta_2': 0.7822651156166297, 'epsilon': 0.1726193264170127}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:12,291] Trial 49 finished with value: 0.9795467193580818 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8077745864650877, 'learning_rate': 'constant', 'power_t': 0.5877576002521485, 'beta_1': 0.33193988105387795, 'beta_2': 0.3764210944040499, 'epsilon': 0.24267351712668928}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:14,485] Trial 50 finished with value: 1.140132353624043 and parameters: {'n_layers': 2, 'n_units_0': 55, 'n_units_1': 70, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.743653731047476, 'learning_rate': 'invscaling', 'power_t': 0.9160138997748354, 'beta_1': 0.5570705388839136, 'beta_2': 0.7496747344165846, 'epsilon': 0.12954311083016462}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:16,425] Trial 51 finished with value: 0.9969604075433021 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8970960632187505, 'learning_rate': 'invscaling', 'power_t': 0.5015786986372832, 'beta_1': 0.4790862109285237, 'beta_2': 0.7803891647802436, 'epsilon': 0.0636570545137195}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:18,170] Trial 52 finished with value: 1.1955735730118175 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 55, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8647756201539435, 'learning_rate': 'invscaling', 'power_t': 0.4632824173145761, 'beta_1': 0.5311335233155959, 'beta_2': 0.8423511833531366, 'epsilon': 0.1901000367974287}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:19,890] Trial 53 finished with value: 1.2217532094227241 and parameters: {'n_layers': 2, 'n_units_0': 50, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9839604553015815, 'learning_rate': 'invscaling', 'power_t': 0.6540679059329129, 'beta_1': 0.5754745290518463, 'beta_2': 0.9523894816892793, 'epsilon': 0.037284389688696254}. Best is trial 34 with value: 0.9433466888402497.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:24,277] Trial 54 finished with value: 0.9375478568586508 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9368628532626362, 'learning_rate': 'invscaling', 'power_t': 0.5318301951425639, 'beta_1': 0.40327788598153247, 'beta_2': 0.6859715254030048, 'epsilon': 0.09954588982824658}. Best is trial 54 with value: 0.9375478568586508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:27,321] Trial 55 finished with value: 1.0602465102789904 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 60, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9121084595252775, 'learning_rate': 'invscaling', 'power_t': 0.43266333298038684, 'beta_1': 0.3786621447054751, 'beta_2': 0.6839466717747127, 'epsilon': 0.20815637465640577}. Best is trial 54 with value: 0.9375478568586508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:29,480] Trial 56 finished with value: 0.9473337970200213 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.8080184755420239, 'learning_rate': 'invscaling', 'power_t': 0.5714243606101874, 'beta_1': 0.41411704979807673, 'beta_2': 0.7251090928152318, 'epsilon': 0.266986537562446}. Best is trial 54 with value: 0.9375478568586508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:31,806] Trial 57 finished with value: 1.0155712995788744 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 55, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.6921026298401189, 'learning_rate': 'constant', 'power_t': 0.7020485735842198, 'beta_1': 0.4558109603427894, 'beta_2': 0.5821543874748588, 'epsilon': 0.3540292274735881}. Best is trial 54 with value: 0.9375478568586508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:35,173] Trial 58 finished with value: 2.805508685303235 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 60, 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.9389080337455563, 'learning_rate': 'invscaling', 'power_t': 0.6470188447557924, 'beta_1': 0.3292883915378165, 'beta_2': 0.6610027373642549, 'epsilon': 0.2976260513610538}. Best is trial 54 with value: 0.9375478568586508.\n",
      "[I 2024-08-11 13:18:36,889] Trial 59 finished with value: 1.0458077785142212 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 65, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.19820467534118408, 'learning_rate': 'invscaling', 'power_t': 0.771734453477916, 'beta_1': 0.29595525354314867, 'beta_2': 0.7129258231365091, 'epsilon': 0.2593658439688187}. Best is trial 54 with value: 0.9375478568586508.\n",
      "[I 2024-08-11 13:18:37,785] Trial 60 finished with value: 1.1180795991262937 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 55, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.7903743305395509, 'learning_rate': 'invscaling', 'power_t': 0.6196754949234559, 'beta_1': 0.46205832919211554, 'beta_2': 0.6514629512455266, 'epsilon': 0.015011838048356083}. Best is trial 54 with value: 0.9375478568586508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:40,610] Trial 61 finished with value: 0.9285016960830677 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8590806239644453, 'learning_rate': 'invscaling', 'power_t': 0.5467509124498205, 'beta_1': 0.49685499907946695, 'beta_2': 0.8074276648152529, 'epsilon': 0.117906416960889}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:43,489] Trial 62 finished with value: 0.9544029004226113 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9965227707334455, 'learning_rate': 'invscaling', 'power_t': 0.5815199590575078, 'beta_1': 0.4054647977092006, 'beta_2': 0.608848619627829, 'epsilon': 0.1607314195129026}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:45,985] Trial 63 finished with value: 0.9494138180497552 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9999376353922961, 'learning_rate': 'invscaling', 'power_t': 0.4993488041850139, 'beta_1': 0.4024916793413996, 'beta_2': 0.598870145024084, 'epsilon': 0.14928227401791846}. Best is trial 61 with value: 0.9285016960830677.\n",
      "[I 2024-08-11 13:18:47,736] Trial 64 finished with value: 0.968004825734285 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.9877043213781608, 'learning_rate': 'invscaling', 'power_t': 0.4922544915253002, 'beta_1': 0.41447373840944124, 'beta_2': 0.5686599175394766, 'epsilon': 0.16856545406542592}. Best is trial 61 with value: 0.9285016960830677.\n",
      "[I 2024-08-11 13:18:50,298] Trial 65 finished with value: 1.0272501114978208 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 85, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9613890117405167, 'learning_rate': 'invscaling', 'power_t': 0.37391881629201446, 'beta_1': 0.3915463807296633, 'beta_2': 0.633145046508938, 'epsilon': 0.2040444094207805}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:53,365] Trial 66 finished with value: 0.9530392082450815 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9180222591063081, 'learning_rate': 'invscaling', 'power_t': 0.5935480302577175, 'beta_1': 0.35431197790029284, 'beta_2': 0.47437202325502503, 'epsilon': 0.15546165512208382}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:18:56,493] Trial 67 finished with value: 1.1678015871559748 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9316871182280813, 'learning_rate': 'invscaling', 'power_t': 0.5930127038928013, 'beta_1': 0.3009975593115378, 'beta_2': 0.47945409535887573, 'epsilon': 0.2813421176699662}. Best is trial 61 with value: 0.9285016960830677.\n",
      "[I 2024-08-11 13:18:58,008] Trial 68 finished with value: 1.0867932101012898 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 85, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.9935227183777094, 'learning_rate': 'constant', 'power_t': 0.4352869674286233, 'beta_1': 0.34595057329111056, 'beta_2': 0.43667810681299835, 'epsilon': 0.32672117157134234}. Best is trial 61 with value: 0.9285016960830677.\n",
      "[I 2024-08-11 13:18:59,258] Trial 69 finished with value: 0.9890730995318066 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 75, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.9221224880706925, 'learning_rate': 'invscaling', 'power_t': 0.5187144431218382, 'beta_1': 0.36382924852691345, 'beta_2': 0.5980440757917457, 'epsilon': 0.15735664655015857}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:04,451] Trial 70 finished with value: 4.992743723839443 and parameters: {'n_layers': 2, 'n_units_0': 100, 'n_units_1': 80, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.9231894669208298, 'learning_rate': 'invscaling', 'power_t': 0.583679801595279, 'beta_1': 0.18699134420757652, 'beta_2': 0.5007605305281799, 'epsilon': 0.11665745121360505}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:07,264] Trial 71 finished with value: 0.9837644929480784 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9629509932103542, 'learning_rate': 'invscaling', 'power_t': 0.6239020163010489, 'beta_1': 0.4100944008674945, 'beta_2': 0.4012739421510101, 'epsilon': 0.23251442444295767}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:09,349] Trial 72 finished with value: 1.2107574827541474 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 85, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8494159609097401, 'learning_rate': 'invscaling', 'power_t': 0.5482247999285319, 'beta_1': 0.3145213437516336, 'beta_2': 0.6155894098399342, 'epsilon': 0.11289646300530443}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:12,638] Trial 73 finished with value: 0.9329348346150453 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8981798963841667, 'learning_rate': 'invscaling', 'power_t': 0.6684890293619441, 'beta_1': 0.4339078162198705, 'beta_2': 0.5488062800267252, 'epsilon': 0.18998507466276388}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:14,971] Trial 74 finished with value: 1.497297719218858 and parameters: {'n_layers': 2, 'n_units_0': 90, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.996094809521157, 'learning_rate': 'invscaling', 'power_t': 0.49095801554411744, 'beta_1': 0.4290272563978721, 'beta_2': 0.4668126188149593, 'epsilon': 0.17361057339574462}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:17,457] Trial 75 finished with value: 1.1044952199908125 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9627259261553855, 'learning_rate': 'invscaling', 'power_t': 0.6626964212835947, 'beta_1': 0.36245635756106953, 'beta_2': 0.5421413258769507, 'epsilon': 0.19778765185956096}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:20,497] Trial 76 finished with value: 1.024998183715835 and parameters: {'n_layers': 2, 'n_units_0': 80, 'n_units_1': 100, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9108817668398044, 'learning_rate': 'invscaling', 'power_t': 0.10142656702015651, 'beta_1': 0.4482238859677815, 'beta_2': 0.5964817944533314, 'epsilon': 0.14501864639788972}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:23,484] Trial 77 finished with value: 1.069261812378599 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8103892251191087, 'learning_rate': 'invscaling', 'power_t': 0.40377534735594467, 'beta_1': 0.3998473157270964, 'beta_2': 0.5150430153507873, 'epsilon': 0.26883129281283347}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:27,959] Trial 78 finished with value: 1.152850916066806 and parameters: {'n_layers': 2, 'n_units_0': 85, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8866385935485662, 'learning_rate': 'invscaling', 'power_t': 0.5387656633537168, 'beta_1': 0.49324783018855495, 'beta_2': 0.6795995692044233, 'epsilon': 0.8366277898718281}. Best is trial 61 with value: 0.9285016960830677.\n",
      "[I 2024-08-11 13:19:29,510] Trial 79 finished with value: 1.0941599808120603 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 85, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.9683500943680377, 'learning_rate': 'adaptive', 'power_t': 0.46406466659047607, 'beta_1': 0.46602639335585344, 'beta_2': 0.7347931372308009, 'epsilon': 0.09977281569622556}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:32,563] Trial 80 finished with value: 0.9798887130121197 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9302982139361141, 'learning_rate': 'invscaling', 'power_t': 0.6358201416750978, 'beta_1': 0.2325809746980765, 'beta_2': 0.5604356208052604, 'epsilon': 0.221854756991563}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:35,301] Trial 81 finished with value: 0.9812538607344774 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.04657128662813631, 'learning_rate': 'invscaling', 'power_t': 0.5787286235675675, 'beta_1': 0.43650664315728693, 'beta_2': 0.46698503071343006, 'epsilon': 0.034497228260746846}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:37,903] Trial 82 finished with value: 0.9417612170590892 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8727760686475687, 'learning_rate': 'invscaling', 'power_t': 0.672276306410242, 'beta_1': 0.37234755449921386, 'beta_2': 0.7527611699238231, 'epsilon': 0.08636981549869083}. Best is trial 61 with value: 0.9285016960830677.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:40,897] Trial 83 finished with value: 0.9145953038244417 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8751368266957198, 'learning_rate': 'invscaling', 'power_t': 0.6127636086047581, 'beta_1': 0.28366952165372555, 'beta_2': 0.6975965848629179, 'epsilon': 0.1466249599714001}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:43,044] Trial 84 finished with value: 1.024994953966269 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8403050838836159, 'learning_rate': 'invscaling', 'power_t': 0.6621584725212561, 'beta_1': 0.2699284215700991, 'beta_2': 0.718779480510117, 'epsilon': 0.09050952634008239}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:45,305] Trial 85 finished with value: 1.1968793243874836 and parameters: {'n_layers': 2, 'n_units_0': 95, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8877394607058524, 'learning_rate': 'invscaling', 'power_t': 0.7268277363181219, 'beta_1': 0.3704524146092611, 'beta_2': 0.689642862953467, 'epsilon': 0.12610404059148023}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:46,196] Trial 86 finished with value: 1.55268959175959 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.7376622927992615, 'learning_rate': 'invscaling', 'power_t': 0.6159781073525615, 'beta_1': 0.28843402315246136, 'beta_2': 0.7525035685058917, 'epsilon': 0.18408410581425028}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:47,580] Trial 87 finished with value: 0.9992494270473534 and parameters: {'n_layers': 2, 'n_units_0': 75, 'n_units_1': 75, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.8177432765729291, 'learning_rate': 'invscaling', 'power_t': 0.5165424822513823, 'beta_1': 0.33818309124209955, 'beta_2': 0.6544045098988318, 'epsilon': 0.06209905575449315}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:48,501] Trial 88 finished with value: 1.2466269578808458 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 70, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.781312197764993, 'learning_rate': 'constant', 'power_t': 0.6046899579405843, 'beta_1': 0.25980620662420595, 'beta_2': 0.7584872124673028, 'epsilon': 0.5969121435199009}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:51,151] Trial 89 finished with value: 1.0511362822343444 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 65, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8676922773200406, 'learning_rate': 'invscaling', 'power_t': 0.7023560560131406, 'beta_1': 0.350116392388207, 'beta_2': 0.8662785632795955, 'epsilon': 0.137848650009886}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:19:53,269] Trial 90 finished with value: 1.0961385248586455 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.8516221907413138, 'learning_rate': 'invscaling', 'power_t': 0.6675227588088088, 'beta_1': 0.1914866414207043, 'beta_2': 0.19221915970623688, 'epsilon': 0.028818228249296175}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:56,582] Trial 91 finished with value: 0.9534190985643104 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9003650178561919, 'learning_rate': 'invscaling', 'power_t': 0.5642542226984039, 'beta_1': 0.3991524656388017, 'beta_2': 0.6137565946988026, 'epsilon': 0.15559771950450763}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:19:58,597] Trial 92 finished with value: 0.9775566050415284 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 75, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9051678815835833, 'learning_rate': 'invscaling', 'power_t': 0.5693675537076318, 'beta_1': 0.32049417055385493, 'beta_2': 0.6303704049334757, 'epsilon': 0.1036193941232845}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:20:01,156] Trial 93 finished with value: 1.1306242921542111 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9436412827465714, 'learning_rate': 'invscaling', 'power_t': 0.5430209206970448, 'beta_1': 0.3783362374971246, 'beta_2': 0.725555120281012, 'epsilon': 0.24705163219880932}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:20:03,122] Trial 94 finished with value: 1.1490963665801883 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 85, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.88186364375657, 'learning_rate': 'invscaling', 'power_t': 0.6289648904172233, 'beta_1': 0.21810821213838766, 'beta_2': 0.6694425967766797, 'epsilon': 0.2184159115691794}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:20:05,702] Trial 95 finished with value: 0.9480584393047877 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.9172657921061301, 'learning_rate': 'invscaling', 'power_t': 0.6771009314983071, 'beta_1': 0.4928053037847258, 'beta_2': 0.7005522483797156, 'epsilon': 0.14228101924409517}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:20:06,411] Trial 96 finished with value: 1.014393483013144 and parameters: {'n_layers': 2, 'n_units_0': 70, 'n_units_1': 75, 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.8718846099698805, 'learning_rate': 'adaptive', 'power_t': 0.6774552118840761, 'beta_1': 0.5351277337027204, 'beta_2': 0.7029855648850276, 'epsilon': 0.05681179750570656}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:20:06,622] Trial 97 finished with value: 2.032150635360703 and parameters: {'n_layers': 2, 'n_units_0': 60, 'n_units_1': 80, 'activation': 'identity', 'solver': 'adam', 'alpha': 0.9705301768417923, 'learning_rate': 'invscaling', 'power_t': 0.7151522429190285, 'beta_1': 0.9975836610452, 'beta_2': 0.7923723588315524, 'epsilon': 0.08054557409173752}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-08-11 13:20:09,474] Trial 98 finished with value: 0.9177201853869097 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.32072325785724054, 'learning_rate': 'invscaling', 'power_t': 0.7479061068377202, 'beta_1': 0.48728054335446636, 'beta_2': 0.7341307278915065, 'epsilon': 0.12525876481351822}. Best is trial 83 with value: 0.9145953038244417.\n",
      "[I 2024-08-11 13:20:11,769] Trial 99 finished with value: 1.051738033778396 and parameters: {'n_layers': 2, 'n_units_0': 65, 'n_units_1': 65, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.3398223861707585, 'learning_rate': 'invscaling', 'power_t': 0.341392323852658, 'beta_1': 0.4826465524377923, 'beta_2': 0.7360200100163135, 'epsilon': 0.1210461211393488}. Best is trial 83 with value: 0.9145953038244417.\n"
     ]
    }
   ],
   "source": [
    "mlp_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "            \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "\n",
    "    if os.path.exists(f\"MLP-Station{station_id}-Extended.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - MLP\"\n",
    "        + \" + \"\n",
    "        + f\"Station {station_id}\",\n",
    "        storage=f\"sqlite:///MLP-Station{station_id}-Extended.sqlite3\",\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///MLP-Station{station_id}-Extended.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - MLP\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "            \n",
    "    mlp_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/massimilianoarca/Library/Caches/pypoetry/virtualenvs/climate-change-MEYtuKH4-py3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = mlp_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        # Copy since we will be modifying the params\n",
    "        params_copy = params.copy()\n",
    "        \n",
    "        # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        \n",
    "        hidden_layer_sizes = [\n",
    "            params_copy[f\"n_units_{k}\"] for k in range(params_copy[\"n_layers\"])\n",
    "        ]\n",
    "\n",
    "        for j in range(params_copy[\"n_layers\"]):\n",
    "            params_copy.pop(f\"n_units_{j}\")\n",
    "\n",
    "        params_copy.pop(\"n_layers\")\n",
    "            \n",
    "        model = MLPRegressor(\n",
    "            random_state=42,\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            max_iter=1000,\n",
    "        )\n",
    "    \n",
    "        model.set_params(**params_copy)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        model.fit(X_resampled, y_resampled.values.ravel())\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    mlp_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Station 105 ===\n",
      "Linear Regression RMSE: 1.4523545291338733\n",
      "Random Forest RMSE: 0.47947562127127646\n",
      "XGBoost RMSE: 0.4434667483009191\n",
      "LightGBM RMSE: 0.4729026078070028\n",
      "Neural Network RMSE: 0.7502811502385773\n",
      "\n",
      "Linear Regression R2: -5.60415433597191\n",
      "Random Forest R2: 0.43202495458589885\n",
      "XGBoost R2: 0.4677063430034678\n",
      "LightGBM R2: 0.5174024740259568\n",
      "Neural Network R2: -0.7433654980645912\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "black"
         },
         "mode": "lines",
         "name": "True",
         "type": "scatter",
         "x": [
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          5.3,
          7,
          6.1,
          5.9,
          6.3,
          5.5,
          5.3,
          5.5,
          5.2,
          5.544444444444444,
          5.855555555555556,
          6.2,
          5.6,
          5.4,
          5.8,
          5.9,
          7.9,
          6.2,
          6.1,
          6.1,
          5.9,
          6.1,
          6.225842696629213,
          6.365168539325842,
          6.5,
          6.6,
          6.5,
          6.6,
          7,
          6.4,
          6.6,
          4.7,
          5.2,
          5.2,
          5.152542372881356,
          5.1,
          6.6,
          6,
          6.1,
          5.7,
          5.7,
          6,
          5.5,
          3.2,
          5.2,
          4.8,
          5.8,
          5.7,
          5.7,
          5.7,
          6.6,
          6.7,
          6.5,
          6.5,
          5.5,
          6,
          5.15,
          4.3,
          4.646067415730337,
          5.029213483146068,
          5.4,
          5.8,
          5.75,
          6.4,
          5.7,
          5.650819672131147,
          5.6,
          5.1,
          4.9,
          5.2,
          5.8,
          5.4,
          5.4,
          6.4,
          5.7,
          5.8,
          6,
          6.5,
          5.890163934426229,
          5.3,
          4.9,
          5.1,
          5.257303370786516,
          5.431460674157303,
          5.6,
          6.3,
          6.9,
          6.5,
          7.1,
          6.362295081967213,
          5.6,
          5.9
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines",
         "name": "Linear Regression",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.1032193448917536,
          6.62228327670774,
          6.832259067707712,
          4.619799308112118,
          4.700966481314948,
          6.386986936338181,
          4.616901644954177,
          3.3741398801974034,
          3.801782033980522,
          6.738377729481201,
          7.666033108628676,
          8.115861855395227,
          5.641304075127948,
          6.269086832732798,
          5.749631102410612,
          4.990642349045838,
          3.538337127798517,
          5.617212229433885,
          6.436622834624359,
          7.361473790490227,
          9.306643863745052,
          8.895831846800561,
          5.303614415889066,
          6.061584472465109,
          6.20653857901943,
          8.165047155799982,
          4.0899856472447915,
          5.375670275006865
         ]
        },
        {
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          4.0339386317363015,
          4.6722480989906705,
          4.433506725498118,
          2.0718758123635617,
          1.60621930532132,
          2.9100807338380688,
          2.379594719908771,
          0.6297594583748753,
          1.7922652760654758,
          4.710410892645338,
          5.288091853399036,
          4.492456696769874,
          3.240397696626796,
          2.8830277645248112,
          3.169553508060893,
          2.2125015050620265,
          0.0733509060182489,
          1.619975619627514,
          2.455835045973993,
          1.8372330981507634,
          2.460341917971716,
          4.432474274101048,
          0.9197754851723747,
          2.301838094368091,
          2.6683999477665252,
          4.0831970117087435,
          -3.030701717652711,
          0.231094705846834
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,0,255,0.2)",
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          8.172500058047206,
          8.57231845442481,
          9.231011409917306,
          7.1677228038606735,
          7.795713657308577,
          9.863893138838293,
          6.8542085699995825,
          6.118520302019931,
          5.8112987918955685,
          8.766344566317063,
          10.043974363858316,
          11.73926701402058,
          8.0422104536291,
          9.655145900940784,
          8.329708696760331,
          7.768783193029649,
          7.003323349578785,
          9.614448839240255,
          10.417410623274725,
          12.885714482829691,
          16.152945809518386,
          13.359189419500076,
          9.687453346605757,
          9.821330850562127,
          9.744677210272334,
          12.24689729989122,
          11.210673012142294,
          10.520245844166896
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Random Forest",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.071429588014986,
          5.9512140819373265,
          5.842759174570652,
          5.359437732896705,
          5.2643123009592925,
          5.167036660960772,
          5.530616139965311,
          5.568500212183631,
          5.508853343533916,
          5.8647358942759364,
          6.111796622974932,
          6.303804207518218,
          6.170693293851714,
          5.826189631312698,
          5.729002775132779,
          5.456850582994231,
          5.135429294511331,
          5.608084851307281,
          5.752349144114863,
          5.81718386904592,
          5.809178030639668,
          6.034074212319346,
          6.000503869396157,
          6.145259694194364,
          6.003572835095821,
          6.144393545108362,
          5.90388119161393,
          5.531311742007177
         ]
        },
        {
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          5.765015185759178,
          5.521233006156101,
          5.331222249128246,
          4.611489350596251,
          4.916325076103022,
          4.80349395061404,
          5.162739958398726,
          5.169790646882811,
          5.219027675990925,
          5.608766918507045,
          5.757233872530035,
          5.9687471425804,
          5.6989299367170485,
          5.095055496362315,
          5.280965398034019,
          4.968871023289866,
          4.831906582289387,
          5.269640300041371,
          5.415943223458269,
          5.477303260882256,
          5.464624558558816,
          5.65524778549753,
          5.780133991992763,
          5.830180022816311,
          5.738306373498647,
          5.672587853545146,
          5.598180259589304,
          4.71780395359834
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,0,0,0.2)",
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.4807112068965615,
          6.189234120065439,
          6.424156383184807,
          5.938352382797355,
          5.589254712111046,
          5.545409337010067,
          5.835537380218039,
          6.04400693965608,
          5.788262950279126,
          6.28413255521116,
          6.482358442033643,
          6.5776567706315285,
          6.607594876017044,
          6.283006435920614,
          6.0847144617044275,
          5.916911397555362,
          5.429012164896898,
          5.833218563732791,
          5.973558789659474,
          6.026421766406839,
          6.024948632663294,
          6.479529795729468,
          6.579103738860916,
          6.543222345989944,
          6.27781131344441,
          6.490029542812868,
          6.437605143355292,
          6.126585738731746
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines",
         "name": "XGBoost",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.129411749839782,
          5.976068019866943,
          5.796537637710571,
          5.442411823272705,
          5.137151551246643,
          5.231981830596924,
          5.389393911361695,
          5.462418451309204,
          5.3729040145874025,
          5.779131183624267,
          6.020998592376709,
          6.01494137763977,
          5.95838351726532,
          5.625937328338623,
          5.539319014549255,
          5.450615916252136,
          5.0133864164352415,
          5.305688829421997,
          5.442609376907349,
          5.531764621734619,
          5.611626062393189,
          5.8772820425033565,
          6.01331485748291,
          6.286449670791626,
          5.971607360839844,
          6.078643703460694,
          5.611772656440735,
          5.483459963798523
         ]
        },
        {
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          5.764611530303955,
          5.640785145759582,
          5.449381399154663,
          5.055895459651947,
          4.672617399692536,
          4.982554817199707,
          5.164404511451721,
          5.201253998279571,
          5.111618423461914,
          5.544314420223236,
          5.791041553020477,
          5.720517838001252,
          5.702947688102722,
          5.199812638759613,
          5.1484945058822635,
          4.97700834274292,
          4.499386179447174,
          5.043661439418793,
          5.153505349159241,
          5.206026065349579,
          5.269018924236297,
          5.5813210010528564,
          5.7871633768081665,
          6.032803654670715,
          5.533577930927277,
          5.648986840248108,
          5.193608295917511,
          4.967469334602356
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,255,0,0.2)",
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.458974516391754,
          6.2580908060073845,
          6.071249389648437,
          5.741870701313019,
          5.45527970790863,
          5.51272748708725,
          5.610896062850952,
          5.720411098003387,
          5.632654547691345,
          6.0017629861831665,
          6.238322651386261,
          6.301236140727997,
          6.259228086471557,
          5.963639557361603,
          5.843559956550598,
          5.7786483526229855,
          5.364059698581696,
          5.576857852935791,
          5.66732017993927,
          5.7621398806571955,
          5.822599172592163,
          6.178451466560364,
          6.287867486476898,
          6.59248960018158,
          6.314613425731658,
          6.4210076332092285,
          5.995839941501617,
          5.941324794292449
         ]
        },
        {
         "line": {
          "color": "purple"
         },
         "mode": "lines",
         "name": "LightGBM",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.347490885612482,
          6.108849267038006,
          5.950293974025675,
          5.555299026541684,
          5.296191946908839,
          5.341431735370037,
          5.3285765074661855,
          5.508555895559937,
          5.332216039425025,
          6.2636589814128,
          6.130109969616098,
          6.197440989503723,
          6.180424499919155,
          5.990418107613261,
          5.818572649014076,
          5.821007616596328,
          5.316118660655455,
          5.453516038583148,
          5.482528289954896,
          5.573515796927614,
          5.53608790776541,
          6.063864529395956,
          6.117923744384797,
          6.232892009471573,
          6.292187294462633,
          6.198859030302438,
          6.061935835661279,
          5.820004036073005
         ]
        },
        {
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          5.950942493806466,
          5.4988783535539705,
          5.37927117738921,
          4.959554834634683,
          4.91198482573679,
          4.941375436570545,
          4.749929097438122,
          4.960629813539667,
          4.9410013462329365,
          5.459974511590921,
          5.748951770276278,
          5.889513342380965,
          5.832640855768355,
          5.344903404988598,
          5.0938545679213405,
          5.135111364100159,
          4.950448713592944,
          5.1803975696523255,
          4.933343318279164,
          5.176684210646217,
          5.204957529373758,
          5.418285829522709,
          5.844962524369064,
          6.018439223736496,
          6.021280637095514,
          5.89190126864607,
          5.446476763641614,
          5.191884068020873
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(128,0,128,0.2)",
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.815355237052049,
          6.4533471893841385,
          6.358927793071616,
          6.238778954353895,
          5.651580052279706,
          5.719850052387894,
          5.955911801269833,
          6.3093134460940075,
          5.930596806961784,
          6.923382558647256,
          6.3752486728291275,
          6.675952140185417,
          6.713421376581033,
          6.404997377040141,
          6.315005614969992,
          6.321254490819684,
          5.645113946853821,
          5.952463488795438,
          6.346396358250222,
          6.138591730725507,
          5.990871395207761,
          6.447895347031299,
          6.38701680643071,
          6.5969389856380305,
          6.589952057354785,
          6.672829703819115,
          6.382241983264783,
          6.308157586903216
         ]
        },
        {
         "line": {
          "color": "orange"
         },
         "mode": "lines",
         "name": "Neural Network",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.936732543288145,
          6.76373093684403,
          6.614194123859023,
          6.550985706930034,
          5.97315321125019,
          4.86449109357436,
          5.728946997623877,
          5.6918826835005065,
          5.657330401052418,
          6.382144069544553,
          6.463598819481418,
          6.22384024358283,
          6.408994450682391,
          6.533028257408066,
          6.446148932104333,
          6.4073689061046615,
          5.878269190132219,
          5.594717163117921,
          5.917479861495957,
          6.190058135233833,
          6.278441098936968,
          6.565988649959313,
          6.871744417493729,
          6.962354574413147,
          6.855492676199573,
          6.943125765543879,
          6.866777769450875,
          6.817047092943361
         ]
        },
        {
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          6.755735020926392,
          6.585140427293129,
          6.458676988778693,
          6.414354912790489,
          5.813228516851534,
          4.621720665349182,
          5.491979730210056,
          5.481933967409456,
          5.466466461024929,
          6.228381579145833,
          6.269860635242143,
          6.024159209161916,
          6.226525759805649,
          6.3704165247743365,
          6.301583866542261,
          6.264895464192316,
          5.710893535574391,
          5.367353010861806,
          5.702061793823101,
          5.980324809226107,
          6.060234757045158,
          6.3796979059005565,
          6.714628674237866,
          6.779403523720752,
          6.673727110179009,
          6.769529024494636,
          6.727164224551139,
          6.67899423352241
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,165,0,0.2)",
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00"
         ],
         "y": [
          7.094571554151327,
          6.93402146505072,
          6.778931666529033,
          6.716073960808161,
          6.1570981850531545,
          5.137656141125406,
          5.985148872797375,
          5.914549574481827,
          5.841636027938546,
          6.539801429863505,
          6.615287480266613,
          6.4286892008819025,
          6.623270972770341,
          6.70465913197841,
          6.608264915418972,
          6.560420716525592,
          6.050778939784312,
          5.814229268409791,
          6.137929755768968,
          6.410711330601523,
          6.498033653016028,
          6.739949594856911,
          7.021628578099024,
          7.128378351681281,
          7.020041113805154,
          7.096649189361102,
          7.027423222935477,
          6.979845392387153
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "DOC (mg/l) - Station 105"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "title": {
          "text": "DOC (mg/l)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Station 305 ===\n",
      "Linear Regression RMSE: 1.1645765110772852\n",
      "Random Forest RMSE: 1.0386456382676683\n",
      "XGBoost RMSE: 1.1060707602592261\n",
      "LightGBM RMSE: 1.2591437629168016\n",
      "Neural Network RMSE: 1.1430613713701858\n",
      "\n",
      "Linear Regression R2: 0.03535718607322469\n",
      "Random Forest R2: 0.26260160333982396\n",
      "XGBoost R2: 0.13938134651117895\n",
      "LightGBM R2: 0.2184771531399231\n",
      "Neural Network R2: 0.07947169913624186\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "black"
         },
         "mode": "lines",
         "name": "True",
         "type": "scatter",
         "x": [
          "2003-01-31T00:00:00",
          "2003-02-28T00:00:00",
          "2003-03-31T00:00:00",
          "2003-04-30T00:00:00",
          "2003-05-31T00:00:00",
          "2003-06-30T00:00:00",
          "2003-07-31T00:00:00",
          "2003-08-31T00:00:00",
          "2003-09-30T00:00:00",
          "2003-10-31T00:00:00",
          "2003-11-30T00:00:00",
          "2003-12-31T00:00:00",
          "2004-01-31T00:00:00",
          "2004-02-29T00:00:00",
          "2004-03-31T00:00:00",
          "2004-04-30T00:00:00",
          "2004-05-31T00:00:00",
          "2004-06-30T00:00:00",
          "2004-07-31T00:00:00",
          "2004-08-31T00:00:00",
          "2004-09-30T00:00:00",
          "2004-10-31T00:00:00",
          "2004-11-30T00:00:00",
          "2004-12-31T00:00:00",
          "2005-01-31T00:00:00",
          "2005-02-28T00:00:00",
          "2005-03-31T00:00:00",
          "2005-04-30T00:00:00",
          "2005-05-31T00:00:00",
          "2005-06-30T00:00:00",
          "2005-07-31T00:00:00",
          "2005-08-31T00:00:00",
          "2005-09-30T00:00:00",
          "2005-10-31T00:00:00",
          "2005-11-30T00:00:00",
          "2005-12-31T00:00:00",
          "2006-01-31T00:00:00",
          "2006-02-28T00:00:00",
          "2006-03-31T00:00:00",
          "2006-04-30T00:00:00",
          "2006-05-31T00:00:00",
          "2006-06-30T00:00:00",
          "2006-07-31T00:00:00",
          "2006-08-31T00:00:00",
          "2006-09-30T00:00:00",
          "2006-10-31T00:00:00",
          "2006-11-30T00:00:00",
          "2006-12-31T00:00:00",
          "2007-01-31T00:00:00",
          "2007-02-28T00:00:00",
          "2007-03-31T00:00:00",
          "2007-04-30T00:00:00",
          "2007-05-31T00:00:00",
          "2007-06-30T00:00:00",
          "2007-07-31T00:00:00",
          "2007-08-31T00:00:00",
          "2007-09-30T00:00:00",
          "2007-10-31T00:00:00",
          "2007-11-30T00:00:00",
          "2007-12-31T00:00:00",
          "2008-01-31T00:00:00",
          "2008-02-29T00:00:00",
          "2008-03-31T00:00:00",
          "2008-04-30T00:00:00",
          "2008-05-31T00:00:00",
          "2008-06-30T00:00:00",
          "2008-07-31T00:00:00",
          "2008-08-31T00:00:00",
          "2008-09-30T00:00:00",
          "2008-10-31T00:00:00",
          "2008-11-30T00:00:00",
          "2008-12-31T00:00:00",
          "2009-01-31T00:00:00",
          "2009-02-28T00:00:00",
          "2009-03-31T00:00:00",
          "2009-04-30T00:00:00",
          "2009-05-31T00:00:00",
          "2009-06-30T00:00:00",
          "2009-07-31T00:00:00",
          "2009-08-31T00:00:00",
          "2009-09-30T00:00:00",
          "2009-10-31T00:00:00",
          "2009-11-30T00:00:00",
          "2009-12-31T00:00:00",
          "2010-01-31T00:00:00",
          "2010-02-28T00:00:00",
          "2010-03-31T00:00:00",
          "2010-04-30T00:00:00",
          "2010-05-31T00:00:00",
          "2010-06-30T00:00:00",
          "2010-07-31T00:00:00",
          "2010-08-31T00:00:00",
          "2010-09-30T00:00:00",
          "2010-10-31T00:00:00",
          "2010-11-30T00:00:00",
          "2010-12-31T00:00:00",
          "2011-01-31T00:00:00",
          "2011-02-28T00:00:00",
          "2011-03-31T00:00:00",
          "2011-04-30T00:00:00",
          "2011-05-31T00:00:00",
          "2011-06-30T00:00:00",
          "2011-07-31T00:00:00",
          "2011-08-31T00:00:00",
          "2011-09-30T00:00:00",
          "2011-10-31T00:00:00",
          "2011-11-30T00:00:00",
          "2011-12-31T00:00:00",
          "2012-01-31T00:00:00",
          "2012-02-29T00:00:00",
          "2012-03-31T00:00:00",
          "2012-04-30T00:00:00",
          "2012-05-31T00:00:00",
          "2012-06-30T00:00:00",
          "2012-07-31T00:00:00",
          "2012-08-31T00:00:00",
          "2012-09-30T00:00:00",
          "2012-10-31T00:00:00",
          "2012-11-30T00:00:00",
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          7.7,
          8.2,
          7.65,
          8.35,
          8.3,
          10.3,
          9.5,
          10.3,
          10.25,
          9.2,
          8.1,
          8.9,
          9.2,
          10.7,
          7.9,
          8.3,
          8.2,
          8.3,
          8,
          8.3,
          7.5,
          8.05,
          7.4,
          7.7,
          7.9,
          8.1,
          8.45,
          8.1,
          8.5,
          8.85,
          8.75,
          9.15,
          8.75,
          7.9,
          7.3,
          7.3,
          8.3,
          7.8,
          8.3,
          8.5,
          8.55,
          8.7,
          9.2,
          8.6,
          8.3,
          8,
          7.5,
          7.5,
          7.6,
          7.6,
          9.4,
          9.1,
          8.9,
          9.5,
          10,
          9.7,
          9.5,
          9.2,
          8.3,
          9.7,
          9.1,
          9.2,
          10,
          10.2,
          10.5,
          10.1,
          9.9,
          9.2,
          8.708196721311475,
          8.2,
          8.8,
          8.7,
          8.4,
          7.6,
          8.8,
          8.8,
          8.8,
          8.5,
          8.5,
          9,
          9.2,
          8.1,
          8.5,
          8.7,
          9.5,
          8.5,
          10.4,
          8.6,
          10,
          10.6,
          10,
          10,
          8.7,
          8.8,
          9.2,
          10.3,
          10.4,
          10.3,
          8.6,
          9.6,
          8.6,
          8.5,
          8.9,
          12.6,
          10.2,
          9.2,
          9.1,
          8.2,
          10.3,
          9.8,
          9.6,
          9.8,
          9.3,
          9.5,
          9.3,
          9.5,
          11.7,
          8.7,
          8.3,
          8.5,
          8.3,
          9,
          9.3,
          8.9,
          9.4,
          9.8,
          9.2,
          9.1,
          8,
          8.7,
          8.4,
          8.2,
          8.46271186440678,
          8.7,
          8.4,
          8.7,
          8.4,
          9,
          8.3,
          8.9,
          8.7,
          8.8,
          8,
          7.3,
          8.7,
          8.2,
          7.8,
          7.5,
          8.4,
          8.3,
          8.2,
          8.250819672131147,
          8.3,
          7,
          7.5,
          8,
          8.3,
          8.2,
          8.2,
          9,
          8.7,
          8.2,
          8.8,
          8.4,
          8,
          8.2,
          7.9,
          7.5,
          7.7,
          7.984745762711865,
          8.3,
          8.4,
          8.1,
          8.3,
          11,
          11,
          9.1,
          9,
          10,
          9.3,
          9.1,
          9.7,
          8.6,
          9.8,
          11,
          11,
          9.7,
          10,
          9.7,
          8.3,
          8.4,
          7.6,
          8,
          8,
          8.1,
          8.6,
          8.2,
          6.7,
          9.2,
          8.7,
          8,
          5.5,
          5,
          4.4,
          8,
          8.7,
          9.1,
          8.804918032786885,
          8.5,
          9.3,
          9.5,
          8.85,
          7.841803278688524,
          6.8,
          8,
          7.9,
          8.3,
          7.2,
          8.1,
          8,
          8
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines",
         "name": "Linear Regression",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          7.477361294694861,
          8.907043748609416,
          7.283176077871086,
          8.335764280212361,
          8.85185951591493,
          7.807860656204847,
          8.233156507940288,
          8.3129886025038,
          7.982219990205116,
          8.20481185147108,
          8.381301654446283,
          6.639163770907863,
          5.8846551219457055,
          7.256890471747904,
          6.999916483480679,
          7.329592746980735,
          7.8734749260496,
          7.868292492892087,
          7.939375748812963,
          7.585521936054469,
          10.307601611803078,
          10.810313408436224,
          8.86302654122175,
          9.192217303450123,
          9.694591188814307,
          11.384159406914256,
          11.133585655042044,
          11.146924199674809,
          10.217477794106175,
          10.568645983456411,
          9.08674152483965,
          8.80260746480369,
          8.564491055736555,
          8.943987827779003,
          7.981771415574062,
          7.491612918389713,
          7.436823226826929,
          6.9782997783560665,
          7.948717820872206,
          7.135913022635097,
          8.204405845120966,
          7.859365658375888,
          8.142881684096727,
          8.72583246186501,
          8.146206746089605,
          8.81414209477872,
          7.9520514833657066,
          6.755307627580321,
          7.500041200993451,
          8.545491296667917,
          8.510451667561528,
          9.096635366537235,
          9.569209313801077,
          8.5420136776272,
          7.481546589905148,
          8.00990315938594,
          8.150348599986378,
          8.27150025093621,
          7.595312338349712,
          6.831731047329342,
          6.558307978687577,
          6.5642800351244,
          8.28590361422963,
          4.895332039922975,
          7.618524097321562,
          7.739156910019306,
          8.79815285546781
         ]
        },
        {
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          6.203058751691872,
          7.843776510809109,
          6.060532407943054,
          7.43828956218502,
          8.101488154735303,
          7.105922941732739,
          7.526218277331815,
          7.540143293259401,
          7.312422770483297,
          7.557815242711498,
          7.531259045232333,
          5.581602479118851,
          4.39333971071645,
          6.185081339019767,
          5.694249915066398,
          6.368893571075078,
          6.989461433464741,
          7.062654408597971,
          7.043925497454035,
          4.993448500818416,
          8.721576801970034,
          9.441168469486911,
          7.918483493416411,
          7.887344487945923,
          8.394846972026516,
          9.334120258720855,
          9.421887449832523,
          9.339354858376907,
          8.827241330748906,
          8.978331133010053,
          7.807389314780904,
          7.685424821719672,
          7.491199535427459,
          7.64169054676891,
          6.9101145066237475,
          6.390109244473599,
          6.298869591080742,
          5.723674134340504,
          6.702838504729674,
          5.821239274557044,
          6.652988843365087,
          6.561619604424005,
          6.635188870045328,
          7.294373232562591,
          6.9413724307292,
          7.320583921250217,
          6.712012808796161,
          5.396299053191166,
          5.702616573080976,
          6.736397930456359,
          6.600775325242557,
          7.308952311710069,
          7.991272688264094,
          7.081668025351881,
          6.14197762215049,
          6.590212851934099,
          6.722758379008576,
          6.812032182476976,
          6.219664209255379,
          5.4047203748718395,
          4.848496330231355,
          4.774112055223649,
          6.522348894926781,
          2.268852381295686,
          5.7447052945201085,
          5.973614676251149,
          6.601865908869755
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,0,255,0.2)",
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.75166383769785,
          9.970310986409723,
          8.505819747799118,
          9.233238998239703,
          9.602230877094556,
          8.509798370676954,
          8.940094738548762,
          9.0858339117482,
          8.652017209926935,
          8.85180846023066,
          9.231344263660231,
          7.696725062696874,
          7.375970533174961,
          8.328699604476041,
          8.30558305189496,
          8.290291922886393,
          8.75748841863446,
          8.673930577186205,
          8.83482600017189,
          10.177595371290522,
          11.893626421636121,
          12.179458347385538,
          9.80756958902709,
          10.497090118954322,
          10.994335405602099,
          13.434198555107656,
          12.845283860251564,
          12.95449354097271,
          11.607714257463444,
          12.15896083390277,
          10.366093734898397,
          9.91979010788771,
          9.637782576045652,
          10.246285108789095,
          9.053428324524376,
          8.593116592305826,
          8.574776862573117,
          8.232925422371629,
          9.194597137014737,
          8.45058677071315,
          9.755822846876846,
          9.157111712327772,
          9.650574498148126,
          10.15729169116743,
          9.35104106145001,
          10.307700268307224,
          9.192090157935251,
          8.114316201969476,
          9.297465828905926,
          10.354584662879475,
          10.420128009880498,
          10.884318421364402,
          11.147145939338058,
          10.002359329902518,
          8.821115557659805,
          9.429593466837783,
          9.57793882096418,
          9.730968319395444,
          8.970960467444044,
          8.258741719786844,
          8.2681196271438,
          8.354448015025152,
          10.04945833353248,
          7.521811698550264,
          9.492342900123017,
          9.504699143787462,
          10.994439802065866
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Random Forest",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.338455936379836,
          8.420802087534714,
          8.34706007378167,
          8.236927626981654,
          8.41490911667315,
          8.384190663722798,
          8.860663537276222,
          8.624616645260955,
          8.610952524308662,
          8.695838706181274,
          8.739257871986961,
          8.008156003521437,
          7.909881237236378,
          8.026084159356747,
          8.334566632835957,
          8.431602505284845,
          8.353997631427427,
          8.314701969399756,
          8.65109505975471,
          9.120779336369207,
          9.316316989000228,
          9.556659775440757,
          8.925435520560375,
          8.81844620295022,
          8.841157490173078,
          9.412110476478674,
          9.506332536095181,
          9.401140393707927,
          9.424536358470487,
          9.58095010199978,
          9.180995521447679,
          9.191798286851437,
          9.212757735231985,
          9.253023976919058,
          8.864708763022886,
          8.261504264972627,
          8.154795837001235,
          7.971935670911726,
          8.368401173591357,
          8.407978867375032,
          8.35226776729934,
          8.432143894344192,
          8.528365279293848,
          9.16111708310743,
          8.908603301772093,
          9.164364088317601,
          8.936822351664375,
          8.31093315562585,
          8.432684675173626,
          8.407488730294752,
          8.615596270216102,
          8.853974168128996,
          9.057630567060338,
          8.86577639136807,
          8.485231165786756,
          8.971852116029021,
          9.165383527183181,
          9.172874185976198,
          8.700330551041166,
          8.31559185669055,
          8.253574630660063,
          7.993181214112257,
          8.41975854600644,
          8.393403540068185,
          8.444847968429992,
          8.483979058134732,
          8.818503091358059
         ]
        },
        {
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.103670854173254,
          8.17564969226357,
          8.084634200586937,
          7.9921379778030035,
          8.164920708602711,
          8.055061691960699,
          8.482344200554715,
          8.356961095504293,
          8.376705281153662,
          8.41704507260821,
          8.268389151670297,
          7.641117494889195,
          7.622148572701685,
          7.728977969089451,
          8.073196474897932,
          8.144224532475455,
          8.076683376287008,
          8.002517163057828,
          8.36200023160821,
          8.61352557949278,
          8.89201506146914,
          9.011715729639459,
          8.70847557173494,
          8.500282458493485,
          8.466264403914298,
          8.831624419376777,
          8.967459388669361,
          8.815808173354553,
          8.981301193802619,
          8.97485438521147,
          8.80468495037133,
          8.76506470743517,
          8.830765477598373,
          8.763032464284503,
          8.502212051642948,
          8.011271349901893,
          7.976792075394694,
          7.692359217423782,
          8.116541047694861,
          8.145862618009907,
          8.067933144123593,
          8.117694472528944,
          8.185696068745559,
          8.705368771246542,
          8.591550155156611,
          8.696482948174388,
          8.580840950838587,
          8.045566438550368,
          8.142995859436827,
          8.213953339457795,
          8.263558038601248,
          8.487811752981402,
          8.668781727740848,
          8.501215416964234,
          8.21629460211395,
          8.521394595238421,
          8.701075790184754,
          8.796270701293164,
          8.411221885737676,
          8.091509388067704,
          8.056069544740152,
          7.7357034101037625,
          8.123393542917107,
          8.140676098635032,
          8.149792873740045,
          8.225798089227133,
          8.497327136655272
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,0,0,0.2)",
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.614868427134642,
          8.670797705595445,
          8.619470160104006,
          8.522489781920385,
          8.733589196894282,
          8.781936794644434,
          9.474494421130688,
          8.949498804091236,
          8.927812402882115,
          9.0930802383782,
          9.357232213545311,
          8.388972194445914,
          8.323711787030861,
          8.390243651265738,
          8.590594655421993,
          8.74225322149884,
          8.63291298308026,
          8.64106848907744,
          9.197200917467013,
          9.734932735389346,
          10.110390407608325,
          10.348559694245271,
          9.346281105844183,
          9.16227271695929,
          9.482789479845385,
          10.125976289810136,
          10.061111747564672,
          10.100769498507816,
          9.931777128659634,
          10.153533257034567,
          9.674804117975317,
          9.5746014175394,
          9.594318505884294,
          9.745144737273439,
          9.302785022344583,
          8.630887302156928,
          8.452011018137231,
          8.338416898820805,
          8.6144410086027,
          8.679962040675921,
          8.695996260689164,
          8.823312449026211,
          8.905992852425696,
          9.524865726156799,
          9.220139074282836,
          9.827692945309522,
          9.375950648644984,
          8.623639041975952,
          8.730362528644992,
          8.580909925677801,
          8.986711607205505,
          9.292604473034956,
          9.481323952251302,
          9.165935836217802,
          8.8348212724738,
          9.430239748736751,
          9.562912216855269,
          9.558739108854954,
          9.03709929545811,
          8.62415885344866,
          8.542609421543023,
          8.340341063979498,
          8.893883111209332,
          8.705094445623061,
          8.838285093173456,
          8.751409828288853,
          9.314759942137611
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines",
         "name": "XGBoost",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.51009181022644,
          8.556493854522705,
          8.48091329574585,
          8.639001455307007,
          8.682016496658326,
          8.655031089782716,
          8.90471004486084,
          8.869706287384034,
          8.88154501914978,
          8.868443613052369,
          8.890734710693359,
          8.410263023376466,
          8.133837385177612,
          8.246824131011962,
          8.468726263046264,
          8.517596464157105,
          8.629052257537841,
          8.669801435470582,
          8.798882923126222,
          8.989655504226684,
          9.395628213882446,
          9.611925687789917,
          9.079960403442383,
          8.948913049697875,
          8.833563680648803,
          8.955200300216674,
          9.210959606170654,
          9.177091369628906,
          9.22102144241333,
          9.492057781219483,
          9.260351142883302,
          9.152506885528565,
          9.171034774780274,
          9.173617181777955,
          8.92913953781128,
          8.669212713241578,
          8.504840030670167,
          8.298788328170776,
          8.710072355270386,
          8.569912900924683,
          8.69852620124817,
          8.690999670028686,
          8.76044472694397,
          9.194148988723756,
          9.026212224960327,
          9.306707105636598,
          9.003109302520752,
          8.618527164459229,
          8.569658393859862,
          8.613392467498779,
          8.869762697219848,
          8.924211940765382,
          9.024870977401733,
          8.952750387191772,
          8.85279341697693,
          9.026636571884156,
          9.112957057952881,
          9.130798988342285,
          8.906969242095947,
          8.674830932617187,
          8.526713008880614,
          8.328869829177856,
          8.705175867080689,
          8.476249294281006,
          8.656916828155518,
          8.768103771209717,
          9.082907886505128
         ]
        },
        {
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.298947024345399,
          8.38272421360016,
          8.297382569313049,
          8.468854546546936,
          8.531680679321289,
          8.532068943977356,
          8.748217701911926,
          8.715373086929322,
          8.693586277961732,
          8.67140564918518,
          8.648934864997864,
          8.246762895584107,
          7.950655460357666,
          8.065523028373718,
          8.269354581832886,
          8.342472362518311,
          8.475829482078552,
          8.535779643058778,
          8.643379187583923,
          8.67864124774933,
          9.073553109169007,
          9.308411049842835,
          8.857663083076478,
          8.739167833328247,
          8.601796126365661,
          8.653597450256347,
          8.937212252616883,
          8.96195924282074,
          9.00735001564026,
          9.249823570251465,
          9.03899700641632,
          8.935644912719727,
          8.932942628860474,
          8.897107124328613,
          8.694844341278076,
          8.47767369747162,
          8.330163931846618,
          8.107128643989563,
          8.524341988563538,
          8.390673303604126,
          8.535522389411927,
          8.53505356311798,
          8.577290225028992,
          8.912455892562866,
          8.806172585487365,
          8.963512778282166,
          8.74856939315796,
          8.430021834373473,
          8.374104142189026,
          8.416798377037049,
          8.683194565773011,
          8.713853240013123,
          8.79421226978302,
          8.785112023353577,
          8.69366488456726,
          8.817256355285645,
          8.86110212802887,
          8.85652940273285,
          8.67286274433136,
          8.464136433601379,
          8.333348274230957,
          8.131115794181824,
          8.497295761108399,
          8.288594269752503,
          8.479870772361755,
          8.598932123184204,
          8.872095537185668
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,255,0,0.2)",
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.72899739742279,
          8.801268577575684,
          8.694376158714295,
          8.809723258018494,
          8.853659343719482,
          8.817164778709412,
          9.10776662826538,
          9.06086232662201,
          9.082131600379943,
          9.078700351715087,
          9.11681969165802,
          8.587037992477416,
          8.37561147212982,
          8.495723700523376,
          8.689450025558472,
          8.726434564590454,
          8.797741317749024,
          8.828934979438781,
          8.995861673355103,
          9.214899778366089,
          9.815205311775207,
          10.085002064704895,
          9.313998484611512,
          9.159076404571532,
          9.09113643169403,
          9.2554461479187,
          9.48282594680786,
          9.414325094223022,
          9.439833879470825,
          9.787713170051575,
          9.514723706245421,
          9.41398229598999,
          9.456001448631287,
          9.487193584442139,
          9.168798422813415,
          8.880005359649658,
          8.711607146263123,
          8.540050935745239,
          8.906762933731079,
          8.787988829612733,
          8.896761417388916,
          8.870535373687744,
          8.982954287528992,
          9.475592708587646,
          9.280538868904113,
          9.66460018157959,
          9.263484668731689,
          8.825794410705566,
          8.807896709442138,
          8.901664423942565,
          9.081504774093627,
          9.14581847190857,
          9.239498376846313,
          9.172647380828858,
          9.05266993045807,
          9.278289866447448,
          9.404830646514892,
          9.435951280593873,
          9.158360457420349,
          8.871666955947877,
          8.741754364967345,
          8.592013192176818,
          8.915679740905762,
          8.733442640304565,
          8.856874632835389,
          8.962468576431274,
          9.354354977607727
         ]
        },
        {
         "line": {
          "color": "purple"
         },
         "mode": "lines",
         "name": "LightGBM",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          7.952187106934478,
          8.100409414380788,
          7.834902256736428,
          7.746181422857514,
          7.950034054296707,
          7.997400240335061,
          8.682800720913864,
          8.186938638067765,
          8.192217014515435,
          8.370442828235795,
          8.54970086407421,
          7.687783367122774,
          7.602165577728354,
          7.605746492344796,
          7.742166718079073,
          7.815090960874772,
          7.759135142512703,
          7.741711405114999,
          8.223308803057648,
          8.388121483892254,
          8.721234136490407,
          9.285369832891709,
          8.33169464252853,
          8.160654046497136,
          8.22361369196184,
          8.822216232947326,
          8.691337783565045,
          8.603401647338508,
          8.678742910520182,
          8.86294787154941,
          8.525617579968158,
          8.59363216899093,
          8.468524164907627,
          8.641884818254578,
          8.327680917865557,
          7.547407507547491,
          7.362475166098924,
          7.330270376852431,
          7.652859872484439,
          7.560298592227317,
          7.425402109060038,
          7.585411373871129,
          7.8032211979844215,
          8.18607035041155,
          8.191461726450608,
          8.149387442606146,
          8.138951076008215,
          7.2560874177876995,
          7.560746152179052,
          7.472990477799333,
          7.7001507870373045,
          7.793204128979355,
          8.098785919030586,
          7.6911953785268805,
          7.503264449100117,
          8.181176925193194,
          8.219460737060178,
          8.138563409895989,
          7.585234142058913,
          7.151982574819434,
          7.204455147919789,
          6.940426328821779,
          7.391917316602144,
          7.228113158790462,
          7.29527067642665,
          7.177946237055294,
          7.633388570404638
         ]
        },
        {
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          7.2671240224719105,
          7.396823693086078,
          7.0313014588835125,
          6.961605242810746,
          6.951391996835955,
          7.281146036143138,
          7.795647597933957,
          7.294668399598178,
          7.265866005372758,
          7.531321675194421,
          7.569849353169086,
          6.518864024512714,
          6.696899481283737,
          6.625721386114092,
          6.781862220258958,
          6.917704598339971,
          6.897504438555861,
          6.768738462889656,
          7.079119971223254,
          7.100627752452589,
          6.9891904921912955,
          7.725188864540551,
          7.142506225869188,
          6.729562742254633,
          7.12956882243269,
          7.479869940509039,
          7.230266758331918,
          7.341519272205604,
          7.352312153516759,
          7.477451450586415,
          7.0730701180348134,
          7.450780486519014,
          7.368888738040027,
          7.357630240716832,
          7.016094268559827,
          6.1338347497168915,
          6.177394714399844,
          5.937558907074746,
          6.108041822962731,
          5.966907725144996,
          5.977245050747756,
          6.128812510764329,
          6.138516868978377,
          6.993263352119417,
          6.60455873860474,
          6.738790692074517,
          6.788755380450291,
          5.485745435800254,
          6.198749357608364,
          6.010906208651249,
          5.50693619530977,
          5.733119929704194,
          6.107956869741674,
          5.680961513579985,
          5.7293387552675465,
          6.505760931524688,
          6.666462244035673,
          6.5493318763281145,
          5.614258065435249,
          5.232247638668135,
          5.406077239889943,
          4.9726735889617375,
          5.663876819947749,
          4.732090056707181,
          5.173744531389619,
          5.1635876028038625,
          5.870803046796804
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(128,0,128,0.2)",
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.924701501431157,
          8.842441771143958,
          8.624569031608912,
          8.587826807229893,
          8.750786782816453,
          8.788504344801252,
          9.670421648326641,
          9.03157187528306,
          8.988530077348996,
          9.451765876198454,
          9.608948325023531,
          8.696270722349453,
          8.48767468815034,
          8.255978798276981,
          8.81782860781042,
          8.761557835641272,
          8.796241120342005,
          8.673575564877531,
          9.220588611017115,
          9.62402147622191,
          10.29758837103367,
          10.996594299278177,
          9.647966858360697,
          9.708386517603792,
          9.386886526521463,
          10.320359202191177,
          10.215786884292934,
          10.226434148411906,
          10.191891606224614,
          10.165360887110182,
          10.293399081813781,
          9.824084916452984,
          9.767400768902444,
          9.933154465395441,
          9.49338317355286,
          8.829409059650288,
          8.461743157409654,
          8.36900089793906,
          9.176017349835996,
          8.802266946427322,
          8.570608173288106,
          9.163191484393545,
          9.04638894670437,
          9.723585984907965,
          9.448893049227411,
          9.91783420451279,
          9.757197346245315,
          8.72585017436458,
          8.584869565618744,
          8.661045948572122,
          9.858205638666243,
          9.995430069382028,
          10.203080919850358,
          9.48391591407729,
          8.893892844622624,
          9.617003314007341,
          9.780257014818048,
          9.703761035363987,
          9.247124840170832,
          8.659010818205745,
          8.565540186782414,
          8.208019427366873,
          8.98686165732232,
          8.927768217097093,
          8.86350424467004,
          8.628035593518717,
          9.514902646736706
         ]
        },
        {
         "line": {
          "color": "orange"
         },
         "mode": "lines",
         "name": "Neural Network",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.741672809336148,
          8.858840891638923,
          7.943047821538361,
          8.416070929166056,
          8.614466160491693,
          8.604305830008924,
          8.914556237239378,
          8.996077868324946,
          9.008257076410393,
          9.008674336280084,
          8.95928827912207,
          8.522508152669651,
          7.898532775211173,
          8.165017368647609,
          7.798186507084458,
          7.989519825758179,
          8.459364758572901,
          8.63780878016847,
          8.753909701696381,
          8.691284276926346,
          9.601285211282619,
          9.820291658468204,
          9.47938965617004,
          9.372255011677883,
          9.315739488387077,
          9.30309458682418,
          9.428241490033251,
          9.410277083278588,
          9.584048693167475,
          9.834558874494315,
          9.530773370456808,
          9.299970086741148,
          9.30703099706164,
          9.299721967317767,
          9.105396991222849,
          8.90349291530953,
          8.726931174935753,
          8.461903572354055,
          8.597051314564455,
          8.259719428595458,
          8.692009310437857,
          8.7095628014429,
          8.756784286597872,
          9.198953360123044,
          9.291259901025773,
          9.333464306310459,
          9.253414312206,
          8.94594561777289,
          8.761491037829868,
          8.873970142972363,
          8.989990264915582,
          9.190150776075532,
          9.350003968437193,
          9.21229461711261,
          9.035512095488382,
          9.153207142946437,
          9.253948388575145,
          9.311250087310805,
          9.226098333561175,
          9.036491008600438,
          8.721366994836535,
          8.431222815789017,
          8.590115956401027,
          7.930702858439863,
          8.38511958721288,
          8.834554726804617,
          9.291688487154975
         ]
        },
        {
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.58538395060214,
          8.688570416748094,
          7.648050455593322,
          8.154021705880172,
          8.361812517487023,
          8.438311519051524,
          8.75274006362436,
          8.839710262943871,
          8.824255536663914,
          8.819259910053269,
          8.712097268758168,
          8.341503463080898,
          7.670861687034738,
          7.964800526246277,
          7.459479569097991,
          7.679442845325983,
          8.219012464509996,
          8.460187547909648,
          8.591663951316365,
          8.446624371902987,
          9.414799842317995,
          9.633583236366531,
          9.326961730839443,
          9.214816689729574,
          9.120629857122706,
          9.063887525085883,
          9.10340074214787,
          9.080078790167908,
          9.39070346284594,
          9.636942644787329,
          9.349519245236715,
          9.105434074455749,
          9.096204239276625,
          9.064301348202369,
          8.888228828818312,
          8.720925174424352,
          8.567804155611672,
          8.262028372303273,
          8.314615479752716,
          7.9619528037201786,
          8.46666072956984,
          8.53056290115996,
          8.58299275356716,
          8.976970289962322,
          9.11722251078803,
          9.063449010136786,
          9.052616111801607,
          8.777916081118924,
          8.574052054422612,
          8.677147356436048,
          8.759804600051158,
          8.950442056162561,
          9.12374054644474,
          9.032112495400629,
          8.869150674783635,
          8.971577475364265,
          9.047038583080298,
          9.089578872371726,
          9.056047835388691,
          8.879539815959545,
          8.551443562660344,
          8.234842806308862,
          8.303092552819441,
          7.606887702214942,
          8.107691008764826,
          8.627602006949937,
          9.120724828441594
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,165,0,0.2)",
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00"
         ],
         "y": [
          8.928738850179073,
          9.04737922916184,
          8.26297555710453,
          8.651818489202745,
          8.820566442099356,
          8.769839619344472,
          9.08388890600077,
          9.187361611173237,
          9.213559152902146,
          9.2126764499048,
          9.172146540087246,
          8.721140302536273,
          8.138884898097968,
          8.405671863237618,
          8.128883899419504,
          8.261388299452218,
          8.664791915479565,
          8.799621934167673,
          8.93335836907811,
          8.956837377896237,
          9.85327113703102,
          10.079442950485765,
          9.67522380899093,
          9.553834437533835,
          9.488600055637084,
          9.51074230178357,
          9.7568369506193,
          9.687620387223737,
          9.766942093104511,
          10.009814669018441,
          9.724440551926165,
          9.502870790564913,
          9.537379860646787,
          9.525404146671983,
          9.323932895279167,
          9.119539569990446,
          8.919669107923037,
          8.681565137164903,
          8.832783381826612,
          8.541062773663027,
          8.897634323197417,
          8.89607158383703,
          8.962131623018369,
          9.443849600767926,
          9.505419023505164,
          9.599374223224237,
          9.484374829328262,
          9.154570322973052,
          8.951498886276754,
          9.081411538237452,
          9.205565154735659,
          9.415254725497567,
          9.52831669889818,
          9.397728090961055,
          9.203783219806098,
          9.351029558486623,
          9.482347015618414,
          9.550886438469856,
          9.457456727435343,
          9.246973885495075,
          8.918069633810022,
          8.632380525627982,
          8.832688832999082,
          8.264055283509,
          8.627655707352684,
          9.010576104763294,
          9.470574210297311
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "DOC (mg/l) - Station 305"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "title": {
          "text": "DOC (mg/l)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Station 325 ===\n",
      "Linear Regression RMSE: 2.910769905701122\n",
      "Random Forest RMSE: 1.0530980061718223\n",
      "XGBoost RMSE: 0.8139665796807232\n",
      "LightGBM RMSE: 0.8469181798027792\n",
      "Neural Network RMSE: 1.3088656665016887\n",
      "\n",
      "Linear Regression R2: -13.274895017920912\n",
      "Random Forest R2: -0.809443026076218\n",
      "XGBoost R2: -0.10700649646879934\n",
      "LightGBM R2: -0.03542972119791754\n",
      "Neural Network R2: -1.8444077848201084\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "black"
         },
         "mode": "lines",
         "name": "True",
         "type": "scatter",
         "x": [
          "1986-11-30T00:00:00",
          "1986-12-31T00:00:00",
          "1987-01-31T00:00:00",
          "1987-02-28T00:00:00",
          "1987-03-31T00:00:00",
          "1987-04-30T00:00:00",
          "1987-05-31T00:00:00",
          "1987-06-30T00:00:00",
          "1987-07-31T00:00:00",
          "1987-08-31T00:00:00",
          "1987-09-30T00:00:00",
          "1987-10-31T00:00:00",
          "1987-11-30T00:00:00",
          "1987-12-31T00:00:00",
          "1988-01-31T00:00:00",
          "1988-02-29T00:00:00",
          "1988-03-31T00:00:00",
          "1988-04-30T00:00:00",
          "1988-05-31T00:00:00",
          "1988-06-30T00:00:00",
          "1988-07-31T00:00:00",
          "1988-08-31T00:00:00",
          "1988-09-30T00:00:00",
          "1988-10-31T00:00:00",
          "1988-11-30T00:00:00",
          "1988-12-31T00:00:00",
          "1989-01-31T00:00:00",
          "1989-02-28T00:00:00",
          "1989-03-31T00:00:00",
          "1989-04-30T00:00:00",
          "1989-05-31T00:00:00",
          "1989-06-30T00:00:00",
          "1989-07-31T00:00:00",
          "1989-08-31T00:00:00",
          "1989-09-30T00:00:00",
          "1989-10-31T00:00:00",
          "1989-11-30T00:00:00",
          "1989-12-31T00:00:00",
          "1990-01-31T00:00:00",
          "1990-02-28T00:00:00",
          "1990-03-31T00:00:00",
          "1990-04-30T00:00:00",
          "1990-05-31T00:00:00",
          "1990-06-30T00:00:00",
          "1990-07-31T00:00:00",
          "1990-08-31T00:00:00",
          "1990-09-30T00:00:00",
          "1990-10-31T00:00:00",
          "1990-11-30T00:00:00",
          "1990-12-31T00:00:00",
          "1991-01-31T00:00:00",
          "1991-02-28T00:00:00",
          "1991-03-31T00:00:00",
          "1991-04-30T00:00:00",
          "1991-05-31T00:00:00",
          "1991-06-30T00:00:00",
          "1991-07-31T00:00:00",
          "1991-08-31T00:00:00",
          "1991-09-30T00:00:00",
          "1991-10-31T00:00:00",
          "1991-11-30T00:00:00",
          "1991-12-31T00:00:00",
          "1992-01-31T00:00:00",
          "1992-02-29T00:00:00",
          "1992-03-31T00:00:00",
          "1992-04-30T00:00:00",
          "1992-05-31T00:00:00",
          "1992-06-30T00:00:00",
          "1992-07-31T00:00:00",
          "1992-08-31T00:00:00",
          "1992-09-30T00:00:00",
          "1992-10-31T00:00:00",
          "1992-11-30T00:00:00",
          "1992-12-31T00:00:00",
          "1993-01-31T00:00:00",
          "1993-02-28T00:00:00",
          "1993-03-31T00:00:00",
          "1993-04-30T00:00:00",
          "1993-05-31T00:00:00",
          "1993-06-30T00:00:00",
          "1993-07-31T00:00:00",
          "1993-08-31T00:00:00",
          "1993-09-30T00:00:00",
          "1993-10-31T00:00:00",
          "1993-11-30T00:00:00",
          "1993-12-31T00:00:00",
          "1994-01-31T00:00:00",
          "1994-02-28T00:00:00",
          "1994-03-31T00:00:00",
          "1994-04-30T00:00:00",
          "1994-05-31T00:00:00",
          "1994-06-30T00:00:00",
          "1994-07-31T00:00:00",
          "1994-08-31T00:00:00",
          "1994-09-30T00:00:00",
          "1994-10-31T00:00:00",
          "1994-11-30T00:00:00",
          "1994-12-31T00:00:00",
          "1995-01-31T00:00:00",
          "1995-02-28T00:00:00",
          "1995-03-31T00:00:00",
          "1995-04-30T00:00:00",
          "1995-05-31T00:00:00",
          "1995-06-30T00:00:00",
          "1995-07-31T00:00:00",
          "1995-08-31T00:00:00",
          "1995-09-30T00:00:00",
          "1995-10-31T00:00:00",
          "1995-11-30T00:00:00",
          "1995-12-31T00:00:00",
          "1996-01-31T00:00:00",
          "1996-02-29T00:00:00",
          "1996-03-31T00:00:00",
          "1996-04-30T00:00:00",
          "1996-05-31T00:00:00",
          "1996-06-30T00:00:00",
          "1996-07-31T00:00:00",
          "1996-08-31T00:00:00",
          "1996-09-30T00:00:00",
          "1996-10-31T00:00:00",
          "1996-11-30T00:00:00",
          "1996-12-31T00:00:00",
          "1997-01-31T00:00:00",
          "1997-02-28T00:00:00",
          "1997-03-31T00:00:00",
          "1997-04-30T00:00:00",
          "1997-05-31T00:00:00",
          "1997-06-30T00:00:00",
          "1997-07-31T00:00:00",
          "1997-08-31T00:00:00",
          "1997-09-30T00:00:00",
          "1997-10-31T00:00:00",
          "1997-11-30T00:00:00",
          "1997-12-31T00:00:00",
          "1998-01-31T00:00:00",
          "1998-02-28T00:00:00",
          "1998-03-31T00:00:00",
          "1998-04-30T00:00:00",
          "1998-05-31T00:00:00",
          "1998-06-30T00:00:00",
          "1998-07-31T00:00:00",
          "1998-08-31T00:00:00",
          "1998-09-30T00:00:00",
          "1998-10-31T00:00:00",
          "1998-11-30T00:00:00",
          "1998-12-31T00:00:00",
          "1999-01-31T00:00:00",
          "1999-02-28T00:00:00",
          "1999-03-31T00:00:00",
          "1999-04-30T00:00:00",
          "1999-05-31T00:00:00",
          "1999-06-30T00:00:00",
          "1999-07-31T00:00:00",
          "1999-08-31T00:00:00",
          "1999-09-30T00:00:00",
          "1999-10-31T00:00:00",
          "1999-11-30T00:00:00",
          "1999-12-31T00:00:00",
          "2000-01-31T00:00:00",
          "2000-02-29T00:00:00",
          "2000-03-31T00:00:00",
          "2000-04-30T00:00:00",
          "2000-05-31T00:00:00",
          "2000-06-30T00:00:00",
          "2000-07-31T00:00:00",
          "2000-08-31T00:00:00",
          "2000-09-30T00:00:00",
          "2000-10-31T00:00:00",
          "2000-11-30T00:00:00",
          "2000-12-31T00:00:00",
          "2001-01-31T00:00:00",
          "2001-02-28T00:00:00",
          "2001-03-31T00:00:00",
          "2001-04-30T00:00:00",
          "2001-05-31T00:00:00",
          "2001-06-30T00:00:00",
          "2001-07-31T00:00:00",
          "2001-08-31T00:00:00",
          "2001-09-30T00:00:00",
          "2001-10-31T00:00:00",
          "2001-11-30T00:00:00",
          "2001-12-31T00:00:00",
          "2002-01-31T00:00:00",
          "2002-02-28T00:00:00",
          "2002-03-31T00:00:00",
          "2002-04-30T00:00:00",
          "2002-05-31T00:00:00",
          "2002-06-30T00:00:00",
          "2002-07-31T00:00:00",
          "2002-08-31T00:00:00",
          "2002-09-30T00:00:00",
          "2002-10-31T00:00:00",
          "2002-11-30T00:00:00",
          "2002-12-31T00:00:00",
          "2003-01-31T00:00:00",
          "2003-02-28T00:00:00",
          "2003-03-31T00:00:00",
          "2003-04-30T00:00:00",
          "2003-05-31T00:00:00",
          "2003-06-30T00:00:00",
          "2003-07-31T00:00:00",
          "2003-08-31T00:00:00",
          "2003-09-30T00:00:00",
          "2003-10-31T00:00:00",
          "2003-11-30T00:00:00",
          "2003-12-31T00:00:00",
          "2004-01-31T00:00:00",
          "2004-02-29T00:00:00",
          "2004-03-31T00:00:00",
          "2004-04-30T00:00:00",
          "2004-05-31T00:00:00",
          "2004-06-30T00:00:00",
          "2004-07-31T00:00:00",
          "2004-08-31T00:00:00",
          "2004-09-30T00:00:00",
          "2004-10-31T00:00:00",
          "2004-11-30T00:00:00",
          "2004-12-31T00:00:00",
          "2005-01-31T00:00:00",
          "2005-02-28T00:00:00",
          "2005-03-31T00:00:00",
          "2005-04-30T00:00:00",
          "2005-05-31T00:00:00",
          "2005-06-30T00:00:00",
          "2005-07-31T00:00:00",
          "2005-08-31T00:00:00",
          "2005-09-30T00:00:00",
          "2005-10-31T00:00:00",
          "2005-11-30T00:00:00",
          "2005-12-31T00:00:00",
          "2006-01-31T00:00:00",
          "2006-02-28T00:00:00",
          "2006-03-31T00:00:00",
          "2006-04-30T00:00:00",
          "2006-05-31T00:00:00",
          "2006-06-30T00:00:00",
          "2006-07-31T00:00:00",
          "2006-08-31T00:00:00",
          "2006-09-30T00:00:00",
          "2006-10-31T00:00:00",
          "2006-11-30T00:00:00",
          "2006-12-31T00:00:00",
          "2007-01-31T00:00:00",
          "2007-02-28T00:00:00",
          "2007-03-31T00:00:00",
          "2007-04-30T00:00:00",
          "2007-05-31T00:00:00",
          "2007-06-30T00:00:00",
          "2007-07-31T00:00:00",
          "2007-08-31T00:00:00",
          "2007-09-30T00:00:00",
          "2007-10-31T00:00:00",
          "2007-11-30T00:00:00",
          "2007-12-31T00:00:00",
          "2008-01-31T00:00:00",
          "2008-02-29T00:00:00",
          "2008-03-31T00:00:00",
          "2008-04-30T00:00:00",
          "2008-05-31T00:00:00",
          "2008-06-30T00:00:00",
          "2008-07-31T00:00:00",
          "2008-08-31T00:00:00",
          "2008-09-30T00:00:00",
          "2008-10-31T00:00:00",
          "2008-11-30T00:00:00",
          "2008-12-31T00:00:00",
          "2009-01-31T00:00:00",
          "2009-02-28T00:00:00",
          "2009-03-31T00:00:00",
          "2009-04-30T00:00:00",
          "2009-05-31T00:00:00",
          "2009-06-30T00:00:00",
          "2009-07-31T00:00:00",
          "2009-08-31T00:00:00",
          "2009-09-30T00:00:00",
          "2009-10-31T00:00:00",
          "2009-11-30T00:00:00",
          "2009-12-31T00:00:00",
          "2010-01-31T00:00:00",
          "2010-02-28T00:00:00",
          "2010-03-31T00:00:00",
          "2010-04-30T00:00:00",
          "2010-05-31T00:00:00",
          "2010-06-30T00:00:00",
          "2010-07-31T00:00:00",
          "2010-08-31T00:00:00",
          "2010-09-30T00:00:00",
          "2010-10-31T00:00:00",
          "2010-11-30T00:00:00",
          "2010-12-31T00:00:00",
          "2011-01-31T00:00:00",
          "2011-02-28T00:00:00",
          "2011-03-31T00:00:00",
          "2011-04-30T00:00:00",
          "2011-05-31T00:00:00",
          "2011-06-30T00:00:00",
          "2011-07-31T00:00:00",
          "2011-08-31T00:00:00",
          "2011-09-30T00:00:00",
          "2011-10-31T00:00:00",
          "2011-11-30T00:00:00",
          "2011-12-31T00:00:00",
          "2012-01-31T00:00:00",
          "2012-02-29T00:00:00",
          "2012-03-31T00:00:00",
          "2012-04-30T00:00:00",
          "2012-05-31T00:00:00",
          "2012-06-30T00:00:00",
          "2012-07-31T00:00:00",
          "2012-08-31T00:00:00",
          "2012-09-30T00:00:00",
          "2012-10-31T00:00:00",
          "2012-11-30T00:00:00",
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          8.3,
          9.5,
          9.241666666666667,
          9.008333333333333,
          8.75,
          8.5,
          6.6,
          8,
          10,
          6.7,
          8.1,
          6.4,
          5.8,
          7.5,
          5.2,
          8.8,
          6.6,
          6.8,
          8.2,
          7.4,
          7.2,
          9.5,
          10,
          8.3,
          7,
          6.2,
          6.1,
          4.9,
          6.4,
          6.2,
          8,
          9.3,
          8.5,
          8.5,
          10,
          9.1,
          8.3,
          8,
          6.3,
          6.1,
          7.1,
          7.1,
          9,
          7.7,
          8,
          8,
          8.7,
          6.1,
          5.608196721311475,
          5.1,
          5.9,
          6.371910112359551,
          6.894382022471911,
          7.4,
          6.8,
          5,
          8,
          8.5,
          8.6,
          9,
          7.2,
          6.689010989010989,
          6.178021978021978,
          5.7,
          6.9,
          8.4,
          8,
          8.5,
          6.7,
          8.2,
          7.4,
          6.6,
          6.1,
          5.5,
          5.972881355932204,
          6.4,
          4.8,
          5.1,
          7.2,
          6.9,
          7.4,
          7.120491803278688,
          6.85,
          6.6,
          6,
          6.2,
          6.8,
          6.705084745762711,
          6.6,
          7.7,
          7.75,
          7.1,
          7.7,
          7.9,
          7.3,
          6.6,
          5.6,
          5.7,
          8.6,
          6,
          7.6,
          8.2,
          7.5,
          7.8,
          7.5,
          7.3,
          6.9,
          6.3,
          6.7,
          6.7,
          6.7,
          6.281111111111112,
          5.833333333333334,
          5.4,
          5.1,
          8.1,
          6.5,
          5.7,
          6.7,
          6.6,
          6.8,
          6.9,
          6.8,
          6.5,
          7.6,
          5.6,
          6.5,
          6.6,
          6.8,
          7.7,
          6.1,
          6.4,
          6.8,
          6.4,
          6.6,
          7.2,
          6.6,
          7.1,
          6.4,
          6.2,
          6,
          7.5,
          5.4,
          5.6,
          6.1,
          7.5,
          6.6,
          5.9,
          6.8,
          7.51311475409836,
          8.25,
          8,
          7.2,
          7.7,
          8.8,
          7.7,
          7.6,
          7.2,
          6.8,
          7.1,
          7.2,
          6.5,
          8.05,
          8.6,
          7.949999999999999,
          7.3,
          6.8,
          7.8,
          8.3,
          9.4,
          7.4,
          6.7,
          8,
          6.4,
          7.699999999999999,
          8.3,
          8.3,
          7.4,
          7.5,
          7.6,
          7.6,
          8.4,
          7.7,
          7.2,
          8.1,
          8.2,
          8,
          7.45,
          7.8,
          8.2,
          9.8,
          7.2,
          7.6,
          7.7,
          6.9,
          7.1,
          7,
          7.4,
          7.5,
          7.55,
          8.8,
          8.6,
          9.5,
          9.2,
          8.4,
          6.7,
          8.2,
          8.6,
          7.7,
          7.6,
          7.8,
          7.300000000000001,
          7,
          6.8,
          7.1,
          7.1,
          6.8,
          6.7,
          7,
          7.8,
          7.8,
          7.1,
          7.9,
          8,
          7.1,
          7.4,
          7.3,
          7.9,
          7,
          6.9,
          7.3,
          7.4,
          7.4,
          7.550000000000001,
          7.8,
          7.8,
          8.4,
          8.2,
          8.2,
          8.4,
          7.4,
          7.5,
          6.2,
          6.8,
          7.5,
          7.7,
          7.7,
          7.7,
          8.7,
          7.4,
          8.6,
          7.9,
          6.9,
          8.2,
          8,
          8.2,
          9.3,
          8.2,
          9.8,
          8.2,
          9.7,
          9.2,
          8.7,
          7.8,
          7.1,
          7.8,
          7.2,
          7.4,
          7.1,
          8.1,
          7.7,
          7.3,
          7.4,
          7.6,
          8.1,
          7.6,
          8.1,
          7.8,
          7.8,
          7.8,
          7.8,
          8.5,
          8.3,
          8.3,
          8.2,
          7.4,
          9,
          11.2,
          9.7,
          9.75,
          9.8,
          9.6,
          8.7,
          8.2,
          8.65,
          7.85,
          8.1,
          9.6,
          9.6,
          8.95,
          8.4,
          7.1,
          7.9,
          9.2,
          7.9,
          8.1,
          9.7,
          8.1,
          7.9,
          8.7,
          8.2,
          8.2,
          7.3,
          7.4,
          6,
          7.9,
          7.5,
          8.8,
          7.9,
          8.4,
          9.2,
          8.7,
          7.8,
          6.7,
          7.1,
          7.7,
          6.7,
          8.4,
          7.2,
          7.3,
          7.2,
          7.6,
          7,
          7.6,
          7.7,
          6.8,
          7,
          7.5,
          6.6,
          7.4,
          7.2,
          6.6,
          6.9,
          6.7,
          6.9,
          7.9,
          6.7,
          7.5,
          6.8,
          6.5,
          6.7,
          7.4,
          6.6,
          6.9,
          8.2,
          7.3,
          7.7,
          7.1,
          7,
          7,
          6.5,
          6.2,
          6.6,
          7.4,
          7.1,
          7.4,
          7,
          6.9,
          7.9,
          8.7,
          8.4,
          7.5,
          7.6,
          7.5,
          7.8,
          8.4,
          7.6,
          7.845901639344262,
          8.100000000000001,
          7.6,
          7.8,
          9,
          8.1,
          9.4,
          8.6,
          6.1,
          6.1,
          7.9,
          7.2,
          8.1,
          7.9,
          7.6,
          8,
          7,
          5.8,
          6.409836065573771,
          7,
          4.7,
          7.4,
          6.6,
          7.3,
          7.472131147540983,
          7.65,
          7.822131147540984,
          8,
          7.6,
          7.8,
          8.8,
          7.5,
          7,
          5.8,
          5.4,
          7.6,
          6.5,
          6.7,
          7.4,
          7.6,
          7.15,
          7.420491803278689,
          7.7,
          6.7,
          7,
          6.7,
          6.2,
          7.75,
          7.2,
          7.2,
          7.4,
          7.7,
          9.4,
          7.5,
          7.398360655737704,
          7.3,
          7.05,
          6.8,
          7.2,
          7.2,
          7.322950819672131,
          7.449999999999999,
          7.327049180327869,
          7.2,
          7.45,
          7.5,
          7.5,
          6.8,
          7.85,
          8.9,
          9.6
         ]
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines",
         "name": "Linear Regression",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          8.62189019541815,
          8.54235954793475,
          9.708049152919674,
          9.585485184101348,
          9.238487184177925,
          8.259975807292392,
          8.878964341074994,
          9.120641655000865,
          8.750871355836503,
          8.398795741290611,
          8.536594653012802,
          8.885289303710552,
          9.011552325962237,
          9.170397618433254,
          9.377920078941838,
          8.844977116644845,
          9.025189380553867,
          8.46378082831603,
          8.856259973362654,
          8.531038837844276,
          8.9947081574003,
          9.284150644914712,
          8.98924453941029,
          9.171274831268656,
          9.09926659149703,
          8.895081425039706,
          9.422925404286934,
          9.278156802642718,
          8.8467667109293,
          9.060655804253845,
          8.178937338582664,
          8.43777164697627,
          9.330546374093533,
          9.243931540472094,
          8.650716423587303,
          9.090784607437662,
          9.37367349126104,
          9.395002770803112,
          9.488328707115437,
          9.600511099337215,
          9.170729748872546,
          8.853511539375587,
          8.75916690445353,
          8.951648713486033,
          9.133795206948474,
          9.825907295306848,
          9.334164009546956,
          9.636052043043561,
          9.12885212456436,
          9.572365915338235,
          9.128111096951828,
          8.728192734644477,
          9.254021533274729,
          9.546056693672506,
          9.665160347004052,
          9.385212611678606,
          9.855847618923288,
          10.182505565758131,
          9.593932485672248,
          10.272960133622448,
          10.231048855126353,
          10.499052128924477,
          10.592675242343502,
          9.666941063260104,
          9.353821898606329,
          10.23236410949631,
          10.185234403975372,
          9.680870700754303,
          10.414184214361407,
          9.985145752607,
          9.799069300710109,
          9.681321260968243,
          9.719810883713878,
          10.011437272257409,
          10.260695336227222,
          10.232841902460475,
          10.948973566647346,
          11.122572715291515,
          10.218005630721704,
          11.075560198170889,
          10.86991150094328,
          10.255538874706211,
          10.183188150970436,
          10.038414122339113,
          10.51979045327853,
          11.016786006562816,
          10.591383124408155,
          10.780006092567366,
          10.412323317892715,
          10.035721659897838,
          9.952149676469832,
          9.9462280975363,
          11.429825410849942,
          10.475048436260565,
          10.130086439841563,
          10.579909096962913,
          10.596933103211716,
          10.875030741114896,
          10.479730484635411,
          10.037254804097719,
          10.047520828371976,
          10.898045648445118,
          11.322640154985582,
          11.951616740015655,
          10.76992111670008,
          11.083683624679365,
          10.936470374465578,
          10.548634036698104,
          11.960667805616328,
          11.487237233486516,
          11.623869369795916,
          11.397857384225775,
          11.033777123189637,
          10.59913702599362,
          10.739488947458842,
          10.997612769246725,
          10.36301249570956,
          10.839646440018685,
          10.84195991673357,
          11.108978367225657,
          10.975945004452214,
          11.29688942221941,
          11.327378985754962,
          11.004602533669825,
          11.119748306460485,
          11.525010317670391,
          10.561012139487149,
          10.838469898111649,
          10.937897141270192,
          11.86900852778595,
          11.030528108093801,
          11.762425701695546,
          11.693377192162677,
          13.599967974822432,
          12.943883987209777
         ]
        },
        {
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.5354214946724625,
          7.77534501933384,
          8.297146064417358,
          8.797739045018002,
          8.634800497415972,
          7.588112182233801,
          7.973911933227761,
          8.541045093800186,
          8.096711297479153,
          7.789545251579098,
          7.9833130959322425,
          8.28826421597744,
          8.367346512596955,
          8.518920004356369,
          8.526905632542007,
          8.152171445819645,
          8.406216991654292,
          7.856352446279466,
          8.22103704070582,
          7.895921233579541,
          8.159008982730779,
          8.643356655838035,
          8.365506380103604,
          8.426977950383638,
          8.19303859773286,
          8.060866224340657,
          8.618390538238247,
          8.491158736143714,
          8.064917591349257,
          8.191932226002175,
          7.405274012506096,
          7.761437801903156,
          8.528865721454803,
          8.550194184160416,
          7.69152883343964,
          8.080853699909866,
          8.367679690893947,
          7.797373451254554,
          8.608332499394338,
          8.468840667943036,
          8.348311337317602,
          8.135163581726596,
          8.001324841615586,
          8.260356761534949,
          8.408887703668908,
          8.809077264029856,
          8.567731773989888,
          8.72777397675922,
          8.248118724087883,
          8.491768231695008,
          7.995212679648658,
          7.845236833808778,
          8.476606995017553,
          8.73042423536346,
          7.747108877556099,
          8.314536682046855,
          8.781637214214628,
          9.437075418236914,
          8.772884842483672,
          9.197063388905029,
          9.182861528841174,
          9.441721643582339,
          9.603210973779737,
          8.665932200284274,
          8.484440747530309,
          9.20852331593182,
          9.266619624546642,
          8.82593754726117,
          9.350489318039099,
          9.053520896079279,
          8.663388570685855,
          8.633598337509447,
          8.623747488955564,
          9.026109529022698,
          9.216631912811714,
          9.18541587293321,
          9.62625085168869,
          9.936683403214646,
          8.726368056743329,
          10.036845666246958,
          9.685086979594454,
          9.245949362165275,
          9.217801623205084,
          9.017711523503086,
          9.322596670954464,
          9.72628582137747,
          9.414913259105505,
          9.713432312897725,
          9.405219679389004,
          8.997907753698376,
          8.93193589118523,
          8.885163491663393,
          9.946199169527056,
          9.373615838989867,
          8.748210630345302,
          9.338433845467064,
          9.285693946336341,
          9.666730573486314,
          8.690291930266032,
          8.713516410873487,
          8.844932555623505,
          9.524015674960117,
          9.938483909489205,
          10.3790875746506,
          9.667855420132804,
          9.996726538711115,
          9.768670470094055,
          9.38604755868621,
          10.363911796057643,
          10.218058791045843,
          10.364727246240847,
          10.179214410707225,
          9.84976412644874,
          9.428179991714345,
          9.373715322843283,
          9.726584058326793,
          9.125183440413174,
          9.62431530314623,
          9.523120505622382,
          9.683460404558193,
          9.57627412734486,
          9.912842866367038,
          10.030495102892493,
          9.672667871041828,
          9.930246733857736,
          10.259980024970318,
          9.266034372894048,
          9.526670690816609,
          9.640676022743476,
          10.402508571211113,
          9.456966969784993,
          10.223347384492438,
          9.84524654090845,
          11.627235927975935,
          11.098271368039628
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,0,255,0.2)",
         "line": {
          "color": "blue",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          9.708358896163837,
          9.309374076535658,
          11.11895224142199,
          10.373231323184694,
          9.84217387093988,
          8.931839432350982,
          9.784016748922227,
          9.700238216201544,
          9.405031414193854,
          9.008046231002124,
          9.089876210093362,
          9.482314391443664,
          9.655758139327519,
          9.82187523251014,
          10.228934525341668,
          9.537782787470045,
          9.644161769453442,
          9.071209210352595,
          9.491482906019488,
          9.166156442109012,
          9.830407332069822,
          9.92494463399139,
          9.612982698716976,
          9.915571712153675,
          10.005494585261198,
          9.729296625738755,
          10.227460270335621,
          10.065154869141722,
          9.628615830509343,
          9.929379382505514,
          8.952600664659231,
          9.114105492049383,
          10.132227026732263,
          9.937668896783771,
          9.609904013734965,
          10.100715514965458,
          10.379667291628135,
          10.99263209035167,
          10.368324914836537,
          10.732181530731394,
          9.99314816042749,
          9.571859497024578,
          9.517008967291472,
          9.642940665437118,
          9.85870271022804,
          10.84273732658384,
          10.100596245104025,
          10.544330109327902,
          10.009585525040839,
          10.652963598981462,
          10.261009514254999,
          9.611148635480175,
          10.031436071531905,
          10.361689151981553,
          11.583211816452007,
          10.455888541310356,
          10.930058023631949,
          10.927935713279348,
          10.414980128860824,
          11.348856878339866,
          11.279236181411532,
          11.556382614266616,
          11.582139510907266,
          10.667949926235934,
          10.223203049682349,
          11.256204903060798,
          11.103849183404101,
          10.535803854247437,
          11.477879110683716,
          10.916770609134721,
          10.934750030734362,
          10.729044184427039,
          10.815874278472192,
          10.99676501549212,
          11.30475875964273,
          11.28026793198774,
          12.271696281606003,
          12.308462027368384,
          11.709643204700079,
          12.11427473009482,
          12.054736022292106,
          11.265128387247147,
          11.148574678735788,
          11.059116721175139,
          11.716984235602595,
          12.307286191748162,
          11.767852989710805,
          11.846579872237006,
          11.419426956396427,
          11.0735355660973,
          10.972363461754433,
          11.007292703409208,
          12.913451652172828,
          11.576481033531262,
          11.511962249337824,
          11.821384348458762,
          11.908172260087092,
          12.083330908743479,
          12.26916903900479,
          11.360993197321951,
          11.250109101120447,
          12.272075621930119,
          12.706796400481958,
          13.524145905380712,
          11.871986813267357,
          12.170640710647614,
          12.104270278837102,
          11.711220514709998,
          13.557423815175014,
          12.756415675927189,
          12.883011493350985,
          12.616500357744325,
          12.217790119930534,
          11.770094060272896,
          12.1052625720744,
          12.268641480166657,
          11.600841551005946,
          12.05497757689114,
          12.160799327844758,
          12.53449632989312,
          12.375615881559568,
          12.680935978071782,
          12.62426286861743,
          12.336537196297822,
          12.309249879063234,
          12.790040610370465,
          11.85598990608025,
          12.15026910540669,
          12.235118259796907,
          13.335508484360785,
          12.604089246402609,
          13.301504018898655,
          13.541507843416904,
          15.572700021668929,
          14.789496606379926
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Random Forest",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.970175947226188,
          8.133116981123367,
          8.920086790110515,
          8.86312193545792,
          8.910256598204352,
          8.114381793745133,
          8.246457636523248,
          8.353055133925652,
          8.363346129352651,
          8.080369040407554,
          8.11839509977581,
          8.074934994718369,
          8.030241367609696,
          7.9269783749105285,
          8.088385912628564,
          7.96471718397236,
          8.154711227959364,
          7.973698679214622,
          8.098437824865666,
          8.050857859302594,
          8.404251416269528,
          8.553548461455826,
          8.255017044569644,
          8.162150112539358,
          7.646541204394334,
          7.7074844326886485,
          8.062652650448975,
          7.965450465697903,
          8.028801363172327,
          8.12866387902905,
          8.08319404522882,
          8.114659918501276,
          8.420505547327771,
          8.479194124371508,
          8.102568964203103,
          8.05281378435698,
          7.977304554288833,
          7.88454025254554,
          7.967345048471563,
          7.947545412965655,
          8.103341280487466,
          8.142455443982227,
          8.0420268359594,
          8.109557964593652,
          8.177712153165366,
          8.558290675516004,
          8.250273124151498,
          7.825126598234175,
          7.753134040299097,
          7.926653072699625,
          7.904953699729625,
          7.7351512019811,
          7.9795609344077,
          8.189826349484221,
          8.136859277256326,
          8.633783223258911,
          8.72428105338417,
          8.480268907191848,
          8.100203911953297,
          8.157828145878204,
          8.72630149535289,
          8.7091824626943,
          8.56415145130187,
          7.922963383044261,
          8.007277356597324,
          8.187367686142986,
          8.415199889241903,
          8.506024587702969,
          8.661116664546876,
          8.705307752802486,
          8.393750030969539,
          8.114746821349325,
          7.713010072462362,
          7.741470083869826,
          7.9050205287174276,
          7.9131778133570245,
          8.216895556368243,
          8.28329804369985,
          8.400612321867674,
          8.480294471165012,
          8.476597990426646,
          8.569451462222988,
          8.27504172039708,
          7.933188996763019,
          7.924545049525298,
          7.916217307154593,
          7.8641647586951295,
          8.029176227802898,
          8.07481868179733,
          8.13247314303512,
          8.190190360358848,
          8.249342871684233,
          8.565439183488758,
          8.562598693207164,
          8.338357593929715,
          8.07899316793452,
          7.798492017470494,
          7.737429828503895,
          7.86932417269179,
          7.830846953243062,
          7.914847325695302,
          8.074440913178082,
          8.375137261647664,
          8.461957557700023,
          8.405721980457338,
          8.335983988581752,
          8.108339987191991,
          7.990974553604939,
          8.053046044984168,
          7.9616186856802855,
          8.013796671253978,
          8.086852180290041,
          8.09261385324437,
          8.260742868881893,
          8.32133156630796,
          8.631055494346226,
          8.61659867583163,
          8.402220533733756,
          8.130521841447475,
          8.10931142592566,
          7.845973505079886,
          7.757210731074666,
          7.876691946553852,
          8.039299972840604,
          8.188856293901164,
          8.182218726923475,
          8.046831030041613,
          8.255511670303843,
          8.394797399613248,
          8.484065006378811,
          8.104315657816883,
          7.96539392741088,
          8.379620372688626,
          8.816909695774749,
          8.958648280879407
         ]
        },
        {
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.527018077056389,
          7.683767183171842,
          8.33664375392942,
          8.317600127752174,
          8.423214539937575,
          7.8763675999638085,
          7.842911358418431,
          8.080652983482747,
          8.073514554485865,
          7.800668851441079,
          7.7170230421242385,
          7.7364737151284935,
          7.478221867369233,
          7.509804966961025,
          7.763792377246835,
          7.460542263570346,
          7.777242899663627,
          7.732635271106223,
          7.814535083977214,
          7.728163987124751,
          8.066508555124564,
          8.203820277397702,
          7.935874098867023,
          7.790783959324364,
          7.271552307886824,
          7.298061291226585,
          7.639417336431989,
          7.644674582743099,
          7.718472565570446,
          7.805967817094831,
          7.7921336841666715,
          7.85949111507636,
          8.133653648340099,
          8.134317577353624,
          7.72425993250988,
          7.745127506249049,
          7.593898208096533,
          7.568162637373509,
          7.576110252981088,
          7.566730180933648,
          7.834079405022471,
          7.851945489509474,
          7.779691203610947,
          7.787437242288557,
          7.852025197538526,
          8.16675647988064,
          7.89065356195105,
          7.435922903271518,
          7.341586288037987,
          7.597065397115229,
          7.563205097871144,
          7.392779203597804,
          7.677809393153477,
          7.905107233001827,
          7.834245395027181,
          8.22048146613081,
          8.260146155096807,
          8.085176019545113,
          7.776181465355289,
          7.517666839994141,
          7.983621951878748,
          8.047937372016849,
          8.023737356391552,
          7.571162254643967,
          7.675842836733699,
          7.872055711153606,
          8.090201094305936,
          8.190531249747515,
          8.324066883251463,
          8.306965629416053,
          8.012872514999788,
          7.692121587833186,
          7.321490151159144,
          7.418590595027021,
          7.5646768765788615,
          7.536103181655302,
          7.915977542987897,
          7.932770275858406,
          8.0944918742514,
          8.146363689523714,
          8.169217258307116,
          8.212681344278543,
          7.895901199492744,
          7.6044969243886955,
          7.576777315586159,
          7.525415297054499,
          7.502640246704966,
          7.719153449197739,
          7.790291557827805,
          7.80494337858194,
          7.8404548294111285,
          7.947750537074051,
          8.223392164504107,
          8.194575516722239,
          7.937609324040899,
          7.728780583127209,
          7.4624783527618215,
          7.379610906986779,
          7.562357607571321,
          7.49129794555634,
          7.544368947455046,
          7.79360594046431,
          8.088387099306901,
          8.162922406458586,
          8.038616983334476,
          8.0099910046629,
          7.78036782478137,
          7.600010830087996,
          7.7034189806822715,
          7.558482234399727,
          7.701209169481627,
          7.7600533668564635,
          7.812732069134644,
          7.918164744880575,
          7.946982216414263,
          8.267998776794553,
          8.275860467052018,
          8.042895424409348,
          7.819833037890474,
          7.725181453150024,
          7.441821237711951,
          7.411522247856308,
          7.516789617338598,
          7.659103779744464,
          7.89199178050565,
          7.94186279725157,
          7.814837587327346,
          7.87635184092468,
          8.057189961087799,
          8.130127130811031,
          7.7846788708541625,
          7.302994115630078,
          7.616223674590764,
          8.188331781617888,
          8.444756199771254
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,0,0,0.2)",
         "line": {
          "color": "red",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          8.531203994014433,
          8.736734190876591,
          9.34599560964334,
          9.318187850861504,
          9.314846800011924,
          8.522983888733531,
          8.784702274127701,
          8.695405047095678,
          8.719920987516971,
          8.421540933201834,
          8.578074835244953,
          8.496155093294561,
          8.921659876089041,
          8.37412969820985,
          8.585850476532885,
          8.589181198699379,
          8.575046598670557,
          8.335528937433073,
          8.470894842516378,
          8.421635855093012,
          8.7005222116118,
          8.880541780682332,
          8.596693640514498,
          8.60469898449488,
          8.015313048304083,
          8.145412734863209,
          8.569204245724283,
          8.38940054133259,
          8.408732195103621,
          8.469978318543042,
          8.420883923865228,
          8.499064194898958,
          8.67298926122803,
          8.791533968924343,
          8.47741466959339,
          8.347342940490606,
          8.380232146835871,
          8.296721851494603,
          8.513158432916288,
          8.350148488739789,
          8.55112162319429,
          8.526703443566271,
          8.426504587748445,
          8.49035195032027,
          8.53780600543547,
          8.889729347242962,
          8.66247188407164,
          8.224874341660765,
          8.364035781180197,
          8.427234040644132,
          8.370985515013663,
          8.12821069995325,
          8.342968159072242,
          8.5655411303941,
          8.538435567394798,
          9.073559617261404,
          9.202054120989049,
          8.81447263600255,
          8.504773799002383,
          9.111203744480902,
          9.44516930061961,
          9.279273023521004,
          9.138789970155269,
          8.304660459422587,
          8.429528920242866,
          8.513183091111603,
          8.781227740016845,
          8.910220905695892,
          8.932497352122594,
          9.052005148427524,
          8.738397344115793,
          8.522074028814048,
          8.102570297297012,
          8.143543007991516,
          8.354128618270368,
          8.372984924610739,
          8.611849412118387,
          8.602409234854354,
          8.841609906882006,
          8.807395468426352,
          8.824110997518959,
          8.866753000127984,
          8.677217509189402,
          8.285189520388325,
          8.293119942223216,
          8.368660129329218,
          8.306260359559916,
          8.422622140564918,
          8.500259563092062,
          8.474231222501222,
          8.568397198590024,
          8.627104600685152,
          8.828509336799202,
          8.872943366082707,
          8.688342538244378,
          8.446000344052901,
          8.156324778359155,
          8.111347677278534,
          8.279243874874492,
          8.260387856614683,
          8.307501788302554,
          8.431129384216876,
          8.693550500846525,
          8.82138659511442,
          8.795955691292146,
          8.750169507367596,
          8.473500671275351,
          8.397927409952153,
          8.478579682267647,
          8.458212777713268,
          8.514179399352942,
          8.493300953126573,
          8.547915137879492,
          8.645760370754422,
          8.686701184537574,
          9.013560440003769,
          8.920035078011146,
          8.713422428160486,
          8.4621353375847,
          8.440526033250425,
          8.235327643988798,
          8.167267371702057,
          8.334154014668426,
          8.51456763343226,
          8.584857976913938,
          8.584419418926336,
          8.442200110932497,
          8.71045289617997,
          8.777365664604025,
          8.840234464921872,
          8.511252847290379,
          8.930799924303122,
          9.298604025557765,
          9.354777727280311,
          9.340321685389105
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines",
         "name": "XGBoost",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.480475435256958,
          7.506467304229736,
          7.472474875450135,
          7.535059423446655,
          7.669988875389099,
          7.681785173416138,
          7.775604286193848,
          7.836787371635437,
          7.826214919090271,
          7.668319544792175,
          7.583769006729126,
          7.565866255760193,
          7.5026559638977055,
          7.580262699127197,
          7.61124349117279,
          7.631822276115417,
          7.738980054855347,
          7.656036882400513,
          7.821987605094909,
          7.818881273269653,
          7.880794262886047,
          7.86156042098999,
          7.747827610969543,
          7.70810106754303,
          7.54486572265625,
          7.495430951118469,
          7.608161973953247,
          7.614802174568176,
          7.684083614349365,
          7.76194474697113,
          7.731427578926087,
          7.820618414878846,
          7.9422147178649904,
          7.7875587844848635,
          7.633152179718017,
          7.626865739822388,
          7.611078858375549,
          7.585684323310852,
          7.567767162322998,
          7.5660692405700685,
          7.617253456115723,
          7.786691889762879,
          7.790178499221802,
          7.814837198257447,
          7.819949669837952,
          7.97088164806366,
          7.699530673027039,
          7.585862298011779,
          7.508878626823425,
          7.573343424797058,
          7.566380653381348,
          7.572550783157348,
          7.645367436408996,
          7.841899876594543,
          7.7209962415695195,
          7.71094626903534,
          7.787603840827942,
          7.798426594734192,
          7.614619565010071,
          7.473442397117615,
          7.473224906921387,
          7.482385220527649,
          7.609578862190246,
          7.552701416015625,
          7.635352597236634,
          7.862449908256531,
          7.939967393875122,
          7.8819222116470335,
          7.97006820678711,
          7.883717737197876,
          7.750771040916443,
          7.675587797164917,
          7.566751022338867,
          7.563748440742493,
          7.617796287536621,
          7.601018323898315,
          7.854979395866394,
          7.818530740737915,
          7.931599445343018,
          7.893893022537231,
          8.004936380386352,
          7.884012379646301,
          7.684936656951904,
          7.641958518028259,
          7.615202865600586,
          7.662993311882019,
          7.553279032707215,
          7.630387306213379,
          7.692727518081665,
          7.754088478088379,
          7.821098313331604,
          7.896485562324524,
          8.018725967407226,
          7.890457224845886,
          7.680237112045288,
          7.711758885383606,
          7.623232707977295,
          7.599489326477051,
          7.5925959920883175,
          7.611353178024292,
          7.652192411422729,
          7.784086604118347,
          7.9814217042922975,
          8.083788590431213,
          7.845898761749267,
          7.822868504524231,
          7.7708482694625856,
          7.577174959182739,
          7.593455648422241,
          7.638514103889466,
          7.572833795547485,
          7.669348387718201,
          7.7231702661514285,
          7.804651203155518,
          7.967478194236755,
          7.982244935035705,
          7.9158597707748415,
          7.75606915473938,
          7.735076408386231,
          7.710063681602478,
          7.600265827178955,
          7.594279680252075,
          7.595086379051208,
          7.560414733886719,
          7.667753682136536,
          7.826335926055908,
          7.777089991569519,
          7.860121459960937,
          7.861065902709961,
          7.908227524757385,
          7.606064758300781,
          7.507854399681091,
          7.361002883911133,
          7.601065273284912,
          7.645006995201111
         ]
        },
        {
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.28275077342987,
          7.314425134658814,
          7.202919507026673,
          7.3033726811409,
          7.531048703193664,
          7.542886519432068,
          7.62605984210968,
          7.671083378791809,
          7.659123694896698,
          7.528957819938659,
          7.40501708984375,
          7.402864015102386,
          7.308506751060486,
          7.430739760398865,
          7.47353503704071,
          7.511225974559784,
          7.602418887615204,
          7.518355536460876,
          7.659642887115479,
          7.6617695689201355,
          7.7033926486969,
          7.693142390251159,
          7.60481104850769,
          7.577666294574738,
          7.39924190044403,
          7.329375886917115,
          7.460969305038452,
          7.48477532863617,
          7.559888505935669,
          7.616783463954926,
          7.586694896221161,
          7.66323618888855,
          7.763908791542053,
          7.634460020065307,
          7.504782450199127,
          7.494034838676453,
          7.459359931945801,
          7.447140681743622,
          7.429411351680756,
          7.398611867427826,
          7.486014807224274,
          7.648904013633728,
          7.633774375915527,
          7.659153914451599,
          7.657617223262787,
          7.779204320907593,
          7.576339542865753,
          7.453273952007294,
          7.325666379928589,
          7.4333631753921505,
          7.43544989824295,
          7.446829068660736,
          7.516134703159333,
          7.695230877399444,
          7.434680414199829,
          7.504131531715393,
          7.63306827545166,
          7.648580861091614,
          7.455742275714874,
          7.240553724765777,
          7.22448513507843,
          7.281747496128082,
          7.44492506980896,
          7.413284957408905,
          7.511372804641724,
          7.711144101619721,
          7.7754671454429625,
          7.7188940167427065,
          7.785835480690002,
          7.712727797031403,
          7.6176817417144775,
          7.558220100402832,
          7.4300735831260685,
          7.417523288726807,
          7.487360823154449,
          7.463706922531128,
          7.704931998252869,
          7.661285150051117,
          7.759284126758575,
          7.727255702018738,
          7.813475382328034,
          7.722133088111877,
          7.5570024967193605,
          7.519158494472504,
          7.483545649051666,
          7.543962216377258,
          7.409181368350983,
          7.489694488048554,
          7.572416126728058,
          7.61429625749588,
          7.664478826522827,
          7.733933353424073,
          7.821507287025452,
          7.727225804328919,
          7.5453051090240475,
          7.585431671142578,
          7.500411236286164,
          7.471119904518128,
          7.457429075241089,
          7.49166876077652,
          7.528764712810516,
          7.649424743652344,
          7.807141447067261,
          7.8731204748153685,
          7.68018057346344,
          7.6633204340934755,
          7.635302281379699,
          7.401737439632416,
          7.447592401504517,
          7.493804407119751,
          7.433157670497894,
          7.5340705752372745,
          7.6020297646522526,
          7.657905876636505,
          7.801321160793305,
          7.803255844116211,
          7.751767528057099,
          7.6122271656990055,
          7.610183084011078,
          7.593052279949188,
          7.463475525379181,
          7.455133366584778,
          7.453709053993225,
          7.409295821189881,
          7.540198957920074,
          7.681026530265808,
          7.630354034900665,
          7.70359638929367,
          7.695895302295685,
          7.733075273036957,
          7.442859578132629,
          7.304036724567413,
          7.119820499420166,
          7.3702515125274655,
          7.42525532245636
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(0,255,0,0.2)",
         "line": {
          "color": "green",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.637869787216187,
          7.6929965138435366,
          7.729620838165284,
          7.748852944374084,
          7.800269520282745,
          7.808655846118927,
          7.930277538299561,
          7.961907076835632,
          7.975846016407012,
          7.794490480422973,
          7.705476534366608,
          7.669271886348724,
          7.649626195430756,
          7.7223821640014645,
          7.741487526893616,
          7.742109179496765,
          7.842052710056305,
          7.791120779514313,
          7.953561449050904,
          7.969961452484131,
          8.030053210258483,
          8.002311635017396,
          7.864014756679535,
          7.8170081019401545,
          7.649290025234222,
          7.652494251728058,
          7.743425190448761,
          7.720442700386047,
          7.780851578712463,
          7.866866850852967,
          7.8530774235725405,
          7.974334812164306,
          8.10717694759369,
          7.9124943852424625,
          7.741631627082825,
          7.746607887744903,
          7.733318722248077,
          7.720948016643524,
          7.714595448970795,
          7.721003901958466,
          7.735313236713409,
          7.901368272304535,
          7.926945233345032,
          7.96039696931839,
          7.950109708309173,
          8.139766144752501,
          7.80960294008255,
          7.6909115552902225,
          7.647672533988953,
          7.716058707237243,
          7.702875816822052,
          7.698816764354706,
          7.7502427220344545,
          7.95620596408844,
          8.029235434532165,
          7.896880257129669,
          7.925402867794037,
          7.923089504241943,
          7.7368129849433895,
          7.66391259431839,
          7.672117936611175,
          7.705226349830627,
          7.771453487873077,
          7.69730178117752,
          7.736092412471772,
          7.984115207195282,
          8.102611112594605,
          8.047341179847717,
          8.134815955162049,
          8.036680674552917,
          7.8675242304801944,
          7.777685451507568,
          7.6731712341308596,
          7.707519006729126,
          7.742494511604309,
          7.715358173847198,
          7.973259508609772,
          7.9381954073905945,
          8.144661951065064,
          8.04203233718872,
          8.200194191932678,
          8.04714879989624,
          7.80087263584137,
          7.753259229660034,
          7.723386216163635,
          7.7681357860565186,
          7.68551379442215,
          7.753873646259308,
          7.799319648742676,
          7.85409984588623,
          7.949250900745392,
          8.057098317146302,
          8.217564177513122,
          8.055357432365417,
          7.802588987350464,
          7.819767546653748,
          7.731415724754333,
          7.71333487033844,
          7.732360565662384,
          7.724820756912232,
          7.753032171726227,
          7.914364349842072,
          8.163283133506775,
          8.302159571647644,
          8.010060620307922,
          7.95100177526474,
          7.882883048057556,
          7.7010670542716975,
          7.69820739030838,
          7.773853206634521,
          7.720736837387085,
          7.800638091564179,
          7.825761353969574,
          7.918441319465638,
          8.133415126800537,
          8.159302973747254,
          8.087692809104919,
          7.880816495418548,
          7.843317198753357,
          7.819355928897858,
          7.7110411643981935,
          7.718327569961548,
          7.731818807125092,
          7.719657599925995,
          7.782882845401764,
          7.940359199047089,
          7.938088309764862,
          8.02405252456665,
          8.0176029920578,
          8.05923318862915,
          7.7713991641998295,
          7.690609550476074,
          7.6423632264137265,
          7.820360994338989,
          7.836898934841156
         ]
        },
        {
         "line": {
          "color": "purple"
         },
         "mode": "lines",
         "name": "LightGBM",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.534676268851414,
          7.48251370630572,
          7.506978655610266,
          7.413420652800599,
          7.604823213152117,
          7.711817350446544,
          7.91519360321348,
          8.02337925627906,
          8.125455968013046,
          7.722762199683543,
          7.638623306683197,
          7.753924894814987,
          7.330281683524385,
          7.4000118240445385,
          7.389343230848936,
          7.263311546530262,
          7.638781374230652,
          7.579177481437383,
          7.660855826382217,
          7.704792195177064,
          8.112072867882853,
          8.228267831022077,
          7.903424744563009,
          7.597883068340423,
          7.504292129560347,
          7.148201358598974,
          7.314615354169314,
          7.462622411766245,
          7.581088659055204,
          7.651128111632045,
          7.694090550726495,
          7.847440677114267,
          8.164474640126247,
          8.00333235659937,
          7.713846076077634,
          7.46769601695579,
          7.753778048263546,
          7.366497548102841,
          7.495816166136715,
          7.457665133208551,
          7.490322667622682,
          7.600912387161276,
          7.785962335610008,
          7.636944795146894,
          7.764706683224163,
          8.000080477454603,
          7.642969171297268,
          7.255492207644392,
          7.188673535706085,
          7.50462603732296,
          7.563318588287362,
          7.357012211919916,
          7.838844882094365,
          7.649991761692187,
          7.854677467307146,
          8.016578705805738,
          7.988692262965149,
          7.918668929918399,
          7.557614576222237,
          7.14699981178997,
          7.1471300152294495,
          7.282553408267972,
          7.312992550302831,
          7.348631242486193,
          7.483240298271656,
          7.661898832094794,
          8.095246937458692,
          8.109196850376465,
          7.949991740229113,
          8.05089859717082,
          7.65513884157617,
          7.547806793554076,
          7.170776178123938,
          7.088972373862004,
          7.245997018763027,
          7.544118012011812,
          7.642860800375367,
          7.733016328386266,
          7.814598849158362,
          7.883365441226192,
          8.131067992708736,
          8.05217931969738,
          7.720527826164454,
          7.463581792588042,
          7.610610362448677,
          7.566719304989877,
          7.39809747567538,
          7.628009943453642,
          7.458382525414538,
          7.61220313301115,
          7.689268667607248,
          7.694764517405555,
          7.984541982360489,
          7.955041545066496,
          7.832192068168623,
          7.551386731896299,
          7.382136072001685,
          7.102360855020355,
          7.513589223496918,
          7.369567572427596,
          7.307247851081076,
          7.783695963136714,
          7.947592104768653,
          8.005877612800584,
          7.8878588694270935,
          7.7760172284561095,
          7.709908824869519,
          7.468867777290118,
          7.1738003499565925,
          7.509072645867989,
          7.299821367836087,
          7.356858186991453,
          7.669804289887287,
          7.551954045992676,
          7.713978356133066,
          8.028759079488358,
          7.989966011680335,
          7.8049646902705625,
          7.530264145464579,
          7.434011403710584,
          7.291014859365401,
          7.1464919240190685,
          7.1683075129382345,
          7.32059779451496,
          7.351806387940996,
          7.406908003320533,
          7.5420253715945105,
          7.781804741035398,
          7.829916942585824,
          7.977793410280048,
          7.669921235720956,
          7.040830469198774,
          7.115028086797971,
          7.0327573674313,
          7.192446752221369
         ]
        },
        {
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.028016255649717,
          6.9636033282835905,
          7.120565679918571,
          7.028438648865287,
          7.346140570208219,
          7.415875642358931,
          7.606569766188843,
          7.713039357535728,
          7.76204578854843,
          7.405000308796667,
          7.25619843592586,
          7.371137598989308,
          7.002590974849952,
          7.0755096523496475,
          7.079811069909154,
          6.863356865270786,
          7.264456019714815,
          7.085957019676298,
          7.3315223769019315,
          7.2176205337176675,
          7.764133132454507,
          7.8420918940039925,
          7.565269691531513,
          7.140757421968689,
          7.151841448460134,
          6.6747866429625144,
          6.9609750013174505,
          7.067510581585483,
          7.1401815242484945,
          7.117722606431331,
          7.1730375336708025,
          7.3590745157256805,
          7.750250292154966,
          7.595610732663548,
          7.307128862323789,
          7.077794969222273,
          7.325555818942875,
          6.948625749947462,
          6.9878003158926125,
          6.937377344435887,
          6.991104985555681,
          7.047721812332491,
          7.314854437312093,
          7.145209051667797,
          7.375587667403429,
          7.564243684598716,
          7.171946551646986,
          6.70733762775588,
          6.732961314879131,
          6.9644082940016805,
          6.958086735388476,
          6.837409272356626,
          7.4099275993827405,
          7.039650226682828,
          7.346553266436822,
          7.443237974993002,
          7.348365250822846,
          7.3156067296486285,
          7.007969383773505,
          6.499337671887004,
          6.664291259360545,
          6.62130411216108,
          6.703112073827066,
          6.720872963904869,
          6.933143707131389,
          7.1350508569798885,
          7.438821564309174,
          7.536419464812984,
          7.274623547197969,
          7.473742359525165,
          7.105849349726964,
          6.961009250293026,
          6.590701010606462,
          6.287263374027417,
          6.602533008047201,
          6.896570790944077,
          6.912912112761082,
          7.031136590519923,
          6.964172736673767,
          6.822202291067753,
          7.388802753232,
          7.119766681956971,
          7.0723032861272745,
          6.730359885523383,
          6.831536192245487,
          6.755454172532968,
          6.596566635160056,
          6.973199235439415,
          6.634613485430376,
          6.7012908275581085,
          6.967733860594383,
          6.81913600317564,
          7.27507320081484,
          7.005528972018086,
          7.0581752227480825,
          6.850550315445825,
          6.7069214875122345,
          6.204255255298821,
          6.625362575323288,
          6.651379981796698,
          6.6125941311437915,
          7.0109776841386395,
          7.140634744614433,
          6.893488364949841,
          6.477341534263622,
          6.701983968341117,
          6.926592592980701,
          6.67452874235612,
          6.377378208426332,
          6.719253063251403,
          6.422745053729008,
          6.457961628060116,
          6.797741947799909,
          6.588228129555271,
          6.845029986812214,
          7.086928092074873,
          6.957865377481431,
          6.728173416066103,
          6.654986076684112,
          6.667122763683249,
          6.3590792013114825,
          6.329486384581734,
          6.0742726747491655,
          6.204636321520729,
          6.338242038544435,
          6.364247211229316,
          6.587346346337942,
          6.804278140913231,
          6.868837143610394,
          6.918822564779756,
          6.6698100720843385,
          6.104262839360271,
          5.924584638320211,
          5.918734823418041,
          6.199333125232035
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(128,0,128,0.2)",
         "line": {
          "color": "purple",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.994146552093901,
          8.050983360903377,
          7.868371206320109,
          7.770665617214941,
          7.876946232322465,
          7.996378825310393,
          8.207084071792824,
          8.3844490791566,
          8.544901146641012,
          7.972686851756474,
          7.989086881478762,
          8.160922125238132,
          7.653638082687157,
          7.751100309073573,
          7.7313932914326156,
          7.681547186137859,
          8.00754800958402,
          7.91103726037315,
          7.987398637801242,
          8.067812857482368,
          8.420382586516375,
          8.607757209241507,
          8.274484488858137,
          8.105928392291908,
          7.882049724169033,
          7.47334460632369,
          7.725934671356862,
          7.849385949841462,
          7.916570989322259,
          8.111366695905657,
          8.127752716783764,
          8.27517963787737,
          8.45302841387293,
          8.382894808342199,
          8.107544902273327,
          7.734642733518851,
          8.130564539319508,
          7.793489201083353,
          8.119367948212997,
          7.857803215320828,
          7.881098917985753,
          7.90629700334096,
          8.182477409887237,
          7.992080805550286,
          8.062134685370683,
          8.356645177756997,
          8.097614446362604,
          7.7516377731090405,
          7.578184487170108,
          7.996226600290897,
          8.137951878077375,
          7.834169510147143,
          8.344802602030276,
          8.057025594813274,
          8.306146849727883,
          8.491018097279357,
          8.45196887427966,
          8.330513252059129,
          8.025535407440751,
          7.65535010625482,
          7.538344000971739,
          7.712307277053385,
          7.828215503733433,
          7.865533449846898,
          7.982517263325676,
          8.060805665989378,
          8.550659829660379,
          8.702850505534302,
          8.494463990197676,
          8.46178639354508,
          8.152592494977387,
          8.076247245030059,
          7.659504096538114,
          7.584786654257718,
          7.714806294025171,
          8.008128730824591,
          8.146054013714824,
          8.218992549266847,
          8.373127951861235,
          8.498669928720657,
          8.594494126339288,
          8.748331288225335,
          8.325719512031794,
          8.019444240783805,
          8.09955662103628,
          8.327121645557403,
          7.914002666509062,
          8.10188494532844,
          8.127334470317521,
          8.280388248511848,
          8.202650365923668,
          8.306229848584184,
          8.472051065501116,
          8.496415700420632,
          8.361421770904892,
          8.149182198536037,
          7.833557892114786,
          7.665306314879193,
          8.161512698430316,
          8.104159779561702,
          7.8561699695159435,
          8.27072852042177,
          8.544276657858434,
          8.645072485782814,
          8.56383315645829,
          8.36599371747966,
          8.392282041968013,
          8.069005344802711,
          7.724582887197398,
          8.062142776417613,
          7.977085206476035,
          7.985031715422859,
          8.34690396223317,
          8.22817955774301,
          8.31058885089622,
          8.657013919564902,
          8.795365293361453,
          8.530738069181787,
          8.261668259129543,
          8.275166263169742,
          7.928995208629613,
          7.723904348959201,
          7.799402876097024,
          8.014300772535123,
          8.12523495410394,
          8.087293738097916,
          8.252043307984819,
          8.430614760326138,
          8.488621640245295,
          8.681163379303028,
          8.392475737561583,
          7.723460715189623,
          8.078788717697421,
          7.747270280626852,
          7.970289844589357
         ]
        },
        {
         "line": {
          "color": "orange"
         },
         "mode": "lines",
         "name": "Neural Network",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.636288673849459,
          7.808410509573403,
          8.78227008485846,
          9.112874149213877,
          8.863653411546686,
          8.511746907934272,
          8.52499919770435,
          9.04643698701389,
          8.838732667759,
          8.419940191844873,
          8.108527288050462,
          8.25593260920198,
          8.106427660314209,
          8.042823545454816,
          8.006550342263564,
          8.040458515750009,
          7.937080043632552,
          8.093597383455261,
          8.460796313617752,
          8.40191106824585,
          8.841410371272444,
          9.000860145230115,
          8.65016029603437,
          8.288736729595014,
          7.820258622513056,
          7.662627750781353,
          8.01675965003855,
          7.879533067835665,
          8.077803220305686,
          8.13015531194012,
          8.038683085499962,
          8.202540949491642,
          8.96939434095508,
          8.75201774517721,
          7.997202037788396,
          7.567196840892425,
          7.937981470134953,
          7.941965521813645,
          7.691184288401275,
          8.327634004501226,
          8.251625091443646,
          8.383171404757991,
          8.536344827002186,
          8.471187969259528,
          8.625272287825304,
          9.015556876239344,
          8.670307172918363,
          7.9364717044879525,
          7.841990356588123,
          7.870819656331735,
          7.554946457979445,
          7.615062684763303,
          8.046652813629057,
          8.589150729959252,
          7.9334376707010605,
          8.488603779892067,
          8.860552124565201,
          9.165849165006403,
          8.630737658861888,
          8.677193552080228,
          8.60113556977576,
          8.792891864352915,
          8.695885000707218,
          8.098694437276986,
          8.026539787343761,
          8.543285386289993,
          9.051539207281131,
          8.983700075323354,
          9.292949290685357,
          9.065929204850017,
          8.729562315411837,
          8.127906492927204,
          7.828313413151663,
          7.935812763556824,
          8.150385050094377,
          8.001311177063535,
          8.491947841563764,
          8.70482095514173,
          9.007645460257397,
          9.409263811061017,
          9.195161103422233,
          9.121006081978402,
          8.444145361504773,
          8.147888541698833,
          8.049268743586861,
          8.090132388786984,
          8.17060172048722,
          8.334214391047107,
          8.472475227275508,
          8.522489932425021,
          8.509110854688663,
          8.713853486004384,
          9.04878543929043,
          9.15220690005596,
          8.592254201859127,
          8.060509753663556,
          8.174023366282924,
          7.950278933818335,
          8.018246517659799,
          7.916833903814231,
          8.047875143390488,
          8.270253587750394,
          9.15363653866934,
          9.529545431013485,
          9.247216811800378,
          9.136882155058277,
          8.859412882865909,
          8.348499776713192,
          8.469911815122586,
          8.560788771458023,
          8.62998740242508,
          8.663423105740478,
          8.516544295292055,
          8.728249648223278,
          9.17940249379293,
          9.393755959606471,
          9.294428581292378,
          8.820826784742733,
          8.484214011517636,
          8.383323861735828,
          8.183400709138711,
          8.288473656147376,
          8.446484071766049,
          8.656858839072582,
          8.859565288727072,
          9.022329058575947,
          8.757080728175326,
          8.871291569703956,
          8.914021049977798,
          9.158236989861205,
          8.489367434945134,
          8.595969089706843,
          8.608156869155643,
          9.309037171823121,
          9.620189699077487
         ]
        },
        {
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          7.116280759269791,
          7.512979841903892,
          8.332212757114998,
          8.725922538483696,
          8.494822418036787,
          8.224455650033162,
          8.109877268941405,
          8.7864683622307,
          8.528073168706195,
          8.106127806475627,
          7.7991156778800566,
          8.058318269974261,
          7.769484402301275,
          7.752609222876637,
          7.721551180416968,
          7.812446780281667,
          7.654016698837328,
          7.798225331322688,
          8.258648898168962,
          8.167362455233455,
          8.469542033045627,
          8.66080564474032,
          8.283916297516708,
          7.89824702096313,
          7.499042873227368,
          7.201986925824059,
          7.729462294111899,
          7.598274480809625,
          7.766170836590003,
          7.822907097436026,
          7.724554838358616,
          7.973560892391403,
          8.686842852702187,
          8.347512619924277,
          7.627178196589818,
          7.261930834755405,
          7.637199082262723,
          7.514422589464359,
          7.395997491164465,
          7.989664496894952,
          7.984273107856155,
          8.098545122419342,
          8.279200974075826,
          8.239533157807305,
          8.32070889120252,
          8.616477563735339,
          8.192086369552177,
          7.62990539361739,
          7.538078502931303,
          7.480263091717688,
          7.1671068819900015,
          7.291088422622081,
          7.763277266885392,
          8.303345674837875,
          7.3102849019480125,
          8.020234904685205,
          8.469347571351186,
          8.844389677770652,
          8.275599012575523,
          8.192634216697273,
          8.028897707878272,
          8.403803784477894,
          8.335910163122643,
          7.761716308999098,
          7.747210709217371,
          8.258300446358238,
          8.746377472658605,
          8.676282995429407,
          8.921154430237154,
          8.63391570732245,
          8.214670309408401,
          7.703160259402402,
          7.4781831831730505,
          7.476527332635083,
          7.827840396796262,
          7.686180115626383,
          8.180886695106462,
          8.341929606900994,
          8.509326855208437,
          9.052069259464638,
          8.782912509424813,
          8.75273056384361,
          8.058066767824666,
          7.7856983874745005,
          7.7230868959155705,
          7.6904533826327235,
          7.839340259552129,
          8.006047935299257,
          8.213362729353742,
          8.230506284580308,
          8.254387240662583,
          8.435698079920115,
          8.643208235744359,
          8.756774479519205,
          8.14552885996796,
          7.719445259322374,
          7.7953965906489575,
          7.499006360746327,
          7.585836653051345,
          7.584681992932014,
          7.686556346594802,
          7.894558942730029,
          8.814615403266988,
          9.07155002474439,
          8.82918534515185,
          8.773091867205231,
          8.48320681021229,
          7.996415148430655,
          8.106300432186211,
          8.116976862803096,
          8.302857642462184,
          8.335547037256793,
          8.183981942569915,
          8.446166399823715,
          8.782697554195224,
          9.02081827947196,
          8.94295242450247,
          8.364281688252134,
          8.065147781462331,
          7.974358799616124,
          7.815516045339971,
          7.82745482232102,
          8.073703864771426,
          8.337078679627982,
          8.592603435595604,
          8.745122386458805,
          8.406320748042752,
          8.595266450046212,
          8.634224802284619,
          8.796313352998192,
          8.019999427985535,
          7.984732827846881,
          7.937011020728913,
          8.827351486851784,
          9.049325766358361
         ]
        },
        {
         "fill": "tonexty",
         "fillcolor": "rgba(255,165,0,0.2)",
         "line": {
          "color": "orange",
          "dash": "dash"
         },
         "mode": "lines",
         "name": "95% CI",
         "type": "scatter",
         "x": [
          "2012-12-31T00:00:00",
          "2013-01-31T00:00:00",
          "2013-02-28T00:00:00",
          "2013-03-31T00:00:00",
          "2013-04-30T00:00:00",
          "2013-05-31T00:00:00",
          "2013-06-30T00:00:00",
          "2013-07-31T00:00:00",
          "2013-08-31T00:00:00",
          "2013-09-30T00:00:00",
          "2013-10-31T00:00:00",
          "2013-11-30T00:00:00",
          "2013-12-31T00:00:00",
          "2014-01-31T00:00:00",
          "2014-02-28T00:00:00",
          "2014-03-31T00:00:00",
          "2014-04-30T00:00:00",
          "2014-05-31T00:00:00",
          "2014-06-30T00:00:00",
          "2014-07-31T00:00:00",
          "2014-08-31T00:00:00",
          "2014-09-30T00:00:00",
          "2014-10-31T00:00:00",
          "2014-11-30T00:00:00",
          "2014-12-31T00:00:00",
          "2015-01-31T00:00:00",
          "2015-02-28T00:00:00",
          "2015-03-31T00:00:00",
          "2015-04-30T00:00:00",
          "2015-05-31T00:00:00",
          "2015-06-30T00:00:00",
          "2015-07-31T00:00:00",
          "2015-08-31T00:00:00",
          "2015-09-30T00:00:00",
          "2015-10-31T00:00:00",
          "2015-11-30T00:00:00",
          "2015-12-31T00:00:00",
          "2016-01-31T00:00:00",
          "2016-02-29T00:00:00",
          "2016-03-31T00:00:00",
          "2016-04-30T00:00:00",
          "2016-05-31T00:00:00",
          "2016-06-30T00:00:00",
          "2016-07-31T00:00:00",
          "2016-08-31T00:00:00",
          "2016-09-30T00:00:00",
          "2016-10-31T00:00:00",
          "2016-11-30T00:00:00",
          "2016-12-31T00:00:00",
          "2017-01-31T00:00:00",
          "2017-02-28T00:00:00",
          "2017-03-31T00:00:00",
          "2017-04-30T00:00:00",
          "2017-05-31T00:00:00",
          "2017-06-30T00:00:00",
          "2017-07-31T00:00:00",
          "2017-08-31T00:00:00",
          "2017-09-30T00:00:00",
          "2017-10-31T00:00:00",
          "2017-11-30T00:00:00",
          "2017-12-31T00:00:00",
          "2018-01-31T00:00:00",
          "2018-02-28T00:00:00",
          "2018-03-31T00:00:00",
          "2018-04-30T00:00:00",
          "2018-05-31T00:00:00",
          "2018-06-30T00:00:00",
          "2018-07-31T00:00:00",
          "2018-08-31T00:00:00",
          "2018-09-30T00:00:00",
          "2018-10-31T00:00:00",
          "2018-11-30T00:00:00",
          "2018-12-31T00:00:00",
          "2019-01-31T00:00:00",
          "2019-02-28T00:00:00",
          "2019-03-31T00:00:00",
          "2019-04-30T00:00:00",
          "2019-05-31T00:00:00",
          "2019-06-30T00:00:00",
          "2019-07-31T00:00:00",
          "2019-08-31T00:00:00",
          "2019-09-30T00:00:00",
          "2019-10-31T00:00:00",
          "2019-11-30T00:00:00",
          "2019-12-31T00:00:00",
          "2020-01-31T00:00:00",
          "2020-02-29T00:00:00",
          "2020-03-31T00:00:00",
          "2020-04-30T00:00:00",
          "2020-05-31T00:00:00",
          "2020-06-30T00:00:00",
          "2020-07-31T00:00:00",
          "2020-08-31T00:00:00",
          "2020-09-30T00:00:00",
          "2020-10-31T00:00:00",
          "2020-11-30T00:00:00",
          "2020-12-31T00:00:00",
          "2021-01-31T00:00:00",
          "2021-02-28T00:00:00",
          "2021-03-31T00:00:00",
          "2021-04-30T00:00:00",
          "2021-05-31T00:00:00",
          "2021-06-30T00:00:00",
          "2021-07-31T00:00:00",
          "2021-08-31T00:00:00",
          "2021-09-30T00:00:00",
          "2021-10-31T00:00:00",
          "2021-11-30T00:00:00",
          "2021-12-31T00:00:00",
          "2022-01-31T00:00:00",
          "2022-02-28T00:00:00",
          "2022-03-31T00:00:00",
          "2022-04-30T00:00:00",
          "2022-05-31T00:00:00",
          "2022-06-30T00:00:00",
          "2022-07-31T00:00:00",
          "2022-08-31T00:00:00",
          "2022-09-30T00:00:00",
          "2022-10-31T00:00:00",
          "2022-11-30T00:00:00",
          "2022-12-31T00:00:00",
          "2023-01-31T00:00:00",
          "2023-02-28T00:00:00",
          "2023-03-31T00:00:00",
          "2023-04-30T00:00:00",
          "2023-05-31T00:00:00",
          "2023-06-30T00:00:00",
          "2023-07-31T00:00:00",
          "2023-08-31T00:00:00",
          "2023-09-30T00:00:00",
          "2023-10-31T00:00:00",
          "2023-11-30T00:00:00",
          "2023-12-31T00:00:00",
          "2024-01-31T00:00:00",
          "2024-02-29T00:00:00"
         ],
         "y": [
          8.02969220423721,
          8.122219520296106,
          9.306778513063186,
          9.513721514332556,
          9.227333565152042,
          8.77715276918614,
          8.977902607993299,
          9.264502230360113,
          9.145111960427117,
          8.69224593856736,
          8.388968232907041,
          8.509769065259999,
          8.37893709437292,
          8.265696563064063,
          8.265806253293889,
          8.317703162001848,
          8.225682832161361,
          8.34158181453345,
          8.66245994408914,
          8.616605284904816,
          9.147432375429355,
          9.310275224676822,
          8.931254432375418,
          8.605178552216085,
          8.108186672028301,
          7.981486389623902,
          8.276901122740101,
          8.151532050777625,
          8.339707360922898,
          8.386820409877567,
          8.331537511871279,
          8.45714844789189,
          9.266733325669378,
          9.061454249469806,
          8.312455686443442,
          7.89806604720855,
          8.266123148104878,
          8.33675988831217,
          7.976593861589869,
          8.656269103173912,
          8.52147199429675,
          8.662240864104856,
          8.78620983053134,
          8.707792109234944,
          8.828507070706245,
          9.423318986377375,
          9.035967361223992,
          8.222882902525342,
          8.133108337494118,
          8.170079837463234,
          7.881534933629728,
          7.887023288273497,
          8.25564975743333,
          8.882208289789563,
          8.53908457466146,
          8.93452836545837,
          9.238888702434695,
          9.4588929747067,
          8.923778245914772,
          9.064856715823895,
          9.125932925593782,
          9.203577869428294,
          9.05296632308676,
          8.391123130808616,
          8.28682550405511,
          8.882294152558012,
          9.394167516692496,
          9.281102853186686,
          9.623241707055751,
          9.414965449579151,
          9.1017832366557,
          8.511900019807804,
          8.123928744038261,
          8.256122656914016,
          8.430944107426296,
          8.31741906916656,
          8.834533541579207,
          9.100828637827627,
          9.526875010593976,
          9.767049947386594,
          9.632093779796229,
          9.437652857886965,
          8.779437840575522,
          8.435332619071016,
          8.385488200277832,
          8.433785122166755,
          8.500119928932566,
          8.61299769037102,
          8.73141668394486,
          8.80107750493009,
          8.775491901638365,
          9.011830338693777,
          9.507302847670168,
          9.509065493233873,
          8.946477415566404,
          8.3758376020033,
          8.507676154906385,
          8.305974505704375,
          8.433196458216322,
          8.218483102603171,
          8.329430618758762,
          8.681149280337122,
          9.577791071394433,
          10.04071182864615,
          9.65161894588136,
          9.45549749564438,
          9.155359407374632,
          8.68540462026733,
          8.788471891611827,
          8.893080184813035,
          8.928266244416239,
          8.941294284952516,
          8.876703685341115,
          9.015349796128596,
          9.541562490857405,
          9.773879265491209,
          9.633941224386557,
          9.206888555450654,
          8.812802615551496,
          8.776080745980373,
          8.503300100020097,
          8.664221657880907,
          8.752669484681082,
          8.930912676009825,
          9.14093375478337,
          9.340656896673462,
          9.072605411802092,
          9.181749965757788,
          9.20369753194667,
          9.497993412388325,
          8.841493219969689,
          9.030525682671549,
          9.266154958804245,
          9.837771444252848,
          10.26493773435358
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "DOC (mg/l) - Station 325"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "title": {
          "text": "DOC (mg/l)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    print(f\"=== Station {station_id} ===\")\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    lr_result = lr_results[station_id]\n",
    "    rf_result = rf_results[station_id]\n",
    "    xgb_result = xgb_results[station_id]\n",
    "    lgbm_result = lgbm_results[station_id]\n",
    "    mlp_result = mlp_results[station_id]\n",
    "    \n",
    "    rmse_lr = lr_result[\"rmse\"]\n",
    "    rmse_rf = rf_result[\"rmse\"]\n",
    "    rmse_xgb = xgb_result[\"rmse\"]\n",
    "    rmse_lgbm = lgbm_result[\"rmse\"]\n",
    "    rmse_mlp = mlp_result[\"rmse\"]\n",
    "    \n",
    "    r2_lr = lr_result[\"r2\"]\n",
    "    r2_rf = rf_result[\"r2\"]\n",
    "    r2_xgb = xgb_result[\"r2\"]\n",
    "    r2_lgbm = lgbm_result[\"r2\"]\n",
    "    r2_mlp = mlp_result[\"r2\"]\n",
    "    \n",
    "    print(f\"Linear Regression RMSE: {rmse_lr}\")\n",
    "    print(f\"Random Forest RMSE: {rmse_rf}\")\n",
    "    print(f\"XGBoost RMSE: {rmse_xgb}\")\n",
    "    print(f\"LightGBM RMSE: {rmse_lgbm}\")\n",
    "    print(f\"Neural Network RMSE: {rmse_mlp}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear Regression R2: {r2_lr}\")\n",
    "    print(f\"Random Forest R2: {r2_rf}\")\n",
    "    print(f\"XGBoost R2: {r2_xgb}\")\n",
    "    print(f\"LightGBM R2: {r2_lgbm}\")\n",
    "    print(f\"Neural Network R2: {r2_mlp}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # TRUE\n",
    "    \n",
    "    # add both the training and testing data in a unique trace\n",
    "    y_true = pd.concat([y_tr, y_ts])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_true.index,\n",
    "            y=y_true[\"DOC (mg/l)\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"True\",\n",
    "            line=dict(color=\"black\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # LINEAR REGRESSION\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=lr_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Linear Regression\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lr_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"blue\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lr_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"blue\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(0,0,255,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=rf_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Random Forest\",\n",
    "            line=dict(color=\"red\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=rf_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"red\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=rf_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"red\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(255,0,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # XGBOOST\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=xgb_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"XGBoost\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=xgb_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"green\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=xgb_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"green\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(0,255,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # LGBM\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=lgbm_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"LightGBM\",\n",
    "            line=dict(color=\"purple\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lgbm_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"purple\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lgbm_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"purple\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(128,0,128,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # MLP\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=mlp_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Neural Network\",\n",
    "            line=dict(color=\"orange\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=mlp_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"orange\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=mlp_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"orange\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(255,165,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"DOC (mg/l) - Station {station_id}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"DOC (mg/l)\",\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-change-MEYtuKH4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
