{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"..\", \"..\", \"..\", \"data\", \"berlin\")\n",
    "clean_data_folder = os.path.join(data_folder, \"clean_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = pd.read_excel(os.path.join(clean_data_folder, \"surface.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df = pd.read_excel(os.path.join(clean_data_folder, \"ground.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns = [\"DateTime\", \"Station\"]\n",
    "bacteria_columns = [\n",
    "    \"E.Coli (MPN/100ml)\",\n",
    "    \"Enterococcus (MPN/100ml)\",\n",
    "    \"Coliform (MPN/100ml)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(df: pd.DataFrame, lags: int, rolling_window: int, poly_degree: int):\n",
    "    \n",
    "    initial_features = df.columns\n",
    "    # add polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_degree)\n",
    "    df_poly = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df_poly, columns=poly.get_feature_names_out(df.columns))\n",
    "    \n",
    "    # add lagged, rolling and expanding features for each variable in df\n",
    "    for col in initial_features.difference([\"Year\", \"Month\"]):\n",
    "        for lag in range(1, lags + 1):\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            \n",
    "        df[f\"{col}_rolling{rolling_window}\"] = df[col].rolling(rolling_window).mean()\n",
    "        \n",
    "    # fill NaN values with bfill\n",
    "    df.bfill(inplace=True)\n",
    "    \n",
    "    df.drop(columns=['1'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# Prepare the data for the models\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    df = surface_df[surface_df['Station'] == station_id]\n",
    "    \n",
    "    # add the year and month columns\n",
    "    df[\"Year\"] = df[\"DateTime\"].dt.year\n",
    "    df[\"Month\"] = df[\"DateTime\"].dt.month\n",
    "    \n",
    "    # Save the datetime column for later (drop diff returns error\n",
    "    # if I remove it before)\n",
    "    datetime_column = df.drop(columns=bacteria_columns).dropna()[\"DateTime\"]\n",
    "    \n",
    "    df = df.drop(columns=diff_columns + bacteria_columns).dropna()\n",
    "    \n",
    "    X = df.drop(columns=[\"DOC (mg/l)\"])\n",
    "    y = df[[\"DOC (mg/l)\"]]\n",
    "    \n",
    "    # X = extend_features(X, lags=1, rolling_window=3, poly_degree=2)\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = X.columns\n",
    "    \n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X, columns=cols)\n",
    "    \n",
    "    # Add the datetime column back\n",
    "    X[\"DateTime\"] = datetime_column.values\n",
    "    y[\"DateTime\"] = datetime_column.values\n",
    "    \n",
    "    \n",
    "    X = X.set_index(\"DateTime\")\n",
    "    y = y.set_index(\"DateTime\")\n",
    "    \n",
    "    X_tr, X_ts = X[:int(train_size * len(X))], X[int(train_size * len(X)):]\n",
    "    y_tr, y_ts = y[:int(train_size * len(y))], y[int(train_size * len(y)):]\n",
    "    \n",
    "    datasets[station_id] = (X_tr, X_ts, y_tr, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    model = sm.OLS(y_tr, sm.add_constant(X_tr))\n",
    "    results = model.fit()\n",
    "    \n",
    "    predictions = results.get_prediction(sm.add_constant(X_ts)).summary_frame(alpha=0.05)\n",
    "    \n",
    "    lr_results[station_id] = {\n",
    "        \"y_pred\": predictions['mean'],\n",
    "        \"y_pred_lower\": predictions['mean_ci_lower'],\n",
    "        \"y_pred_upper\": predictions['mean_ci_upper'],\n",
    "        \"model\": results,\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_ts, predictions[\"mean\"])),\n",
    "        \"r2\": r2_score(y_ts, predictions[\"mean\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_rf(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    # define the hyperparameters to search over\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 500, step=10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 32),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X_cv)):\n",
    "        cv_rmse[i] = fit_and_validate_rf(\n",
    "            X_cv, y_cv, train_index, test_index, params\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "    \n",
    "    if os.path.exists(f\"RandomForest-Station{station_id}.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - RandomForest\"\n",
    "        + \" + \"\n",
    "        + f\"Station {station_id}\",\n",
    "        storage=f\"sqlite:///RandomForest-Station{station_id}.sqlite3\",\n",
    "    )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///RandomForest-Station{station_id}.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - RandomForest\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    rf_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = rf_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = RandomForestRegressor(random_state=42, **params)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    rf_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "\n",
    "    if os.path.exists(f\"XGBoost-Station{station_id}.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - XGBoost\"\n",
    "        + \" + \"\n",
    "        + f\"Station{station_id}\",\n",
    "        storage=f\"sqlite:///XGBoost-Station{station_id}.sqlite3\",\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///XGBoost-Station{station_id}.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - XGBoost\"\n",
    "            + \" + \"\n",
    "            + f\"Station{station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "            \n",
    "    xgb_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = xgb_studies[station_id].best_params\n",
    "    \n",
    "    params[\"objective\"] = \"reg:squarederror\"\n",
    "    params[\"booster\"] = \"gblinear\"\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = xgb.XGBRegressor(**params, random_state=42)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    xgb_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lgbm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "\n",
    "    if params is not None:\n",
    "        model.set_params(**params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_estimators\": trial.suggest_int(\n",
    "            \"n_estimators\", 1, 20, step=1\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 16, step=1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 20, step=1),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\n",
    "            \"min_data_in_leaf\", 2, 50, step=1\n",
    "        ),\n",
    "        \"lambda_l1\": trial.suggest_float(\n",
    "            \"lambda_l1\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"lambda_l2\": trial.suggest_float(\n",
    "            \"lambda_l2\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"min_split_gain\": trial.suggest_float(\n",
    "            \"min_split_gain\", 0, 15, step=0.5\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"min_child_samples\": trial.suggest_int(\n",
    "            \"min_child_samples\", 20, 1000, log=True\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 500, step=10),\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lgbm_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "        \n",
    "        X_tr, _, y_tr, _ = datasets[station_id]\n",
    "    \n",
    "        if os.path.exists(f\"LGBM-Station{station_id}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - LGBM\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            storage=f\"sqlite:///LGBM-Station{station_id}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///LGBM-Station{station_id}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - LGBM\"\n",
    "                + \" + \"\n",
    "                + f\"Station {station_id}\",\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lgbm_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = lgbm_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        # Train the model with the best hyperparameters\n",
    "        model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=42,\n",
    "        linear_tree=True,\n",
    "        )\n",
    "        \n",
    "        model.set_params(**params)\n",
    "        \n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    lgbm_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_nn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        random_state=42,\n",
    "        hidden_layer_sizes=tuple(params[\"layers\"]),\n",
    "        max_iter=1000,\n",
    "    )\n",
    "\n",
    "    param = params.copy()\n",
    "    param.pop(\"layers\")\n",
    "    model.set_params(**param)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"layers\": [\n",
    "            trial.suggest_int(f\"n_units_{i}\", 50, 100, step=5)\n",
    "            for i in range(trial.suggest_int(\"n_layers\", 2, 2))\n",
    "        ],\n",
    "        \"activation\": trial.suggest_categorical(\n",
    "            \"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "        ),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"sgd\", \"adam\"]),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1),\n",
    "        \"learning_rate\": trial.suggest_categorical(\n",
    "            \"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "        ),\n",
    "        \"power_t\": trial.suggest_float(\"power_t\", 0.1, 1),\n",
    "        \"beta_1\": trial.suggest_float(\"beta_1\", 0.1, 1),\n",
    "        \"beta_2\": trial.suggest_float(\"beta_2\", 0.1, 1),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-8, 1),\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_nn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_studies = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "            \n",
    "    X_tr, _, y_tr, _ = datasets[station_id]\n",
    "\n",
    "    if os.path.exists(f\"MLP-Station{station_id}.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - MLP\"\n",
    "        + \" + \"\n",
    "        + f\"Station {station_id}\",\n",
    "        storage=f\"sqlite:///MLP-Station{station_id}.sqlite3\",\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///MLP-Station{station_id}.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - MLP\"\n",
    "            + \" + \"\n",
    "            + f\"Station {station_id}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=100, show_progress_bar=True)\n",
    "            \n",
    "    mlp_studies[station_id] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = {}\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    params = mlp_studies[station_id].best_params\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    n_size = len(X_tr)\n",
    "    predictions = np.zeros((len(X_ts), n_iterations))\n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        # Copy since we will be modifying the params\n",
    "        params_copy = params.copy()\n",
    "        \n",
    "        # Bootstrap sample (random state changes each iteration)\n",
    "        X_resampled, y_resampled = resample(X_tr, y_tr, n_samples=n_size, random_state=i)\n",
    "        \n",
    "        \n",
    "        hidden_layer_sizes = [\n",
    "            params_copy[f\"n_units_{k}\"] for k in range(params_copy[\"n_layers\"])\n",
    "        ]\n",
    "\n",
    "        for j in range(params_copy[\"n_layers\"]):\n",
    "            params_copy.pop(f\"n_units_{j}\")\n",
    "\n",
    "        params_copy.pop(\"n_layers\")\n",
    "            \n",
    "        model = MLPRegressor(\n",
    "            random_state=42,\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            max_iter=1000,\n",
    "        )\n",
    "    \n",
    "        model.set_params(**params_copy)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        model.fit(X_resampled, y_resampled.values.ravel())\n",
    "        y_pred = model.predict(X_ts)\n",
    "        predictions[:, i] = y_pred\n",
    "        \n",
    "        # Calculate and store the metric (e.g., RMSE)\n",
    "        metric = mean_squared_error(y_ts, y_pred, squared=False)\n",
    "        metrics.append(metric)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Convert to a numpy array for easier calculation\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    # Calculate the mean RMSE\n",
    "    mean_rmse = np.mean(metrics)\n",
    "    \n",
    "    # Calculate 95% confidence interval of the predictions\n",
    "    lower_bound = np.percentile(predictions, 2.5, axis=1)\n",
    "    upper_bound = np.percentile(predictions, 97.5, axis=1)\n",
    "    \n",
    "    # Calculate the mean predictions\n",
    "    mean_predictions = np.mean(predictions, axis=1)\n",
    "    \n",
    "    mlp_results[station_id] = {\n",
    "        \"y_pred\": mean_predictions,\n",
    "        \"y_pred_lower\": lower_bound,\n",
    "        \"y_pred_upper\": upper_bound,\n",
    "        \"model\": model,\n",
    "        \"rmse\": mean_rmse,\n",
    "        \"r2\": r2_score(y_ts, mean_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    print(f\"=== Station {station_id} ===\")\n",
    "    \n",
    "    X_tr, X_ts, y_tr, y_ts = datasets[station_id]\n",
    "    \n",
    "    lr_result = lr_results[station_id]\n",
    "    rf_result = rf_results[station_id]\n",
    "    xgb_result = xgb_results[station_id]\n",
    "    lgbm_result = lgbm_results[station_id]\n",
    "    mlp_result = mlp_results[station_id]\n",
    "    \n",
    "    rmse_lr = lr_result[\"rmse\"]\n",
    "    rmse_rf = rf_result[\"rmse\"]\n",
    "    rmse_xgb = xgb_result[\"rmse\"]\n",
    "    rmse_lgbm = lgbm_result[\"rmse\"]\n",
    "    rmse_mlp = mlp_result[\"rmse\"]\n",
    "    \n",
    "    r2_lr = lr_result[\"r2\"]\n",
    "    r2_rf = rf_result[\"r2\"]\n",
    "    r2_xgb = xgb_result[\"r2\"]\n",
    "    r2_lgbm = lgbm_result[\"r2\"]\n",
    "    r2_mlp = mlp_result[\"r2\"]\n",
    "    \n",
    "    print(f\"Linear Regression RMSE: {rmse_lr}\")\n",
    "    print(f\"Random Forest RMSE: {rmse_rf}\")\n",
    "    print(f\"XGBoost RMSE: {rmse_xgb}\")\n",
    "    print(f\"LightGBM RMSE: {rmse_lgbm}\")\n",
    "    print(f\"Neural Network RMSE: {rmse_mlp}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear Regression R2: {r2_lr}\")\n",
    "    print(f\"Random Forest R2: {r2_rf}\")\n",
    "    print(f\"XGBoost R2: {r2_xgb}\")\n",
    "    print(f\"LightGBM R2: {r2_lgbm}\")\n",
    "    print(f\"Neural Network R2: {r2_mlp}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # TRUE\n",
    "    \n",
    "    # add both the training and testing data in a unique trace\n",
    "    y_true = pd.concat([y_tr, y_ts])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_true.index,\n",
    "            y=y_true[\"DOC (mg/l)\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"True\",\n",
    "            line=dict(color=\"black\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # LINEAR REGRESSION\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=lr_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Linear Regression\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lr_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"blue\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lr_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"blue\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(0,0,255,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # RANDOM FOREST\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=rf_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Random Forest\",\n",
    "            line=dict(color=\"red\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=rf_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"red\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=rf_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"red\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(255,0,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # XGBOOST\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=xgb_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"XGBoost\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=xgb_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"green\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=xgb_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"green\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(0,255,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # LGBM\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=lgbm_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"LightGBM\",\n",
    "            line=dict(color=\"purple\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lgbm_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"purple\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=lgbm_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"purple\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(128,0,128,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # MLP\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_ts.index,\n",
    "            y=mlp_result[\"y_pred\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Neural Network\",\n",
    "            line=dict(color=\"orange\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add confidence intervals\n",
    "    fig.add_traces(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=mlp_result[\"y_pred_lower\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"orange\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=y_ts.index,\n",
    "                y=mlp_result[\"y_pred_upper\"],\n",
    "                mode=\"lines\",\n",
    "                line_color=\"orange\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "                name=\"95% CI\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(255,165,0,0.2)\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"DOC (mg/l) - Station {station_id}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"DOC (mg/l)\",\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-change-MEYtuKH4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
