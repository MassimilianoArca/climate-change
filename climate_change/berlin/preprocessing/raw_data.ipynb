{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Data\n",
    "\n",
    "Time series for the Havel River inflow to the city (Konradshöhe, Messstellennummer 305) and the downstream station (Schleuse Spandau, Messstellennummer 320), DOC and TOC.\n",
    "\n",
    "Two groundwater station are attached, only with quality, no DOC/TOC is measured here, but UV254 and other. The groundwater stations are not influenced by bank filtrate and represent near-natural conditions (for a city like Berlin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "from googletrans import Translator\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"..\", \"..\", \"data\", \"berlin\")\n",
    "\n",
    "raw_data_folder = os.path.join(data_folder, \"raw_data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"clean_data\")\n",
    "data_info_folder = os.path.join(data_folder, \"data_info\")\n",
    "\n",
    "ground_water_folder = os.path.join(raw_data_folder, \"ground water\")\n",
    "surface_water_folder = os.path.join(raw_data_folder, \"surface water\")\n",
    "meteorological_folder = os.path.join(raw_data_folder, \"meteorological\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_gw_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        ground_water_folder, \"time-series_ground-water_quality.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_gw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_gw_df.rename(\n",
    "    columns={\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Messwert\": \"Value\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        surface_water_folder, \"time-series_surface-water_quality.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        surface_water_folder, \"time-series_surface-water_flow.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df.rename(\n",
    "    columns={\n",
    "        \"Messstelle\": \"Station\",\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Wert\": \"Value\",\n",
    "        \"Bestimmungsgrenze\": \"LOQ\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "ts_sw_df.drop(\n",
    "    columns=[\n",
    "        \"Entnahmetiefe [m]\",\n",
    "        \"Vorzeichen\",\n",
    "        \"Messmethode\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df.rename(\n",
    "    columns={\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Tagesmittelwert\": \"Flow River\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological\n",
    "\n",
    "Daily Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        meteorological_folder,\n",
    "        \"produkt_klima_tag_19480101_20231231_00433.csv\",\n",
    "    ),\n",
    "    sep=\";\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.rename(\n",
    "    columns={\n",
    "        \"STATIONS_ID\": \"Station ID\",\n",
    "        \"MESS_DATUM\": \"DateTime\",\n",
    "        \"  FX\": \"Wind Speed Max (m/s)\",\n",
    "        \"  FM\": \"Wind Speed Mean (m/s)\",\n",
    "        \" RSK\": \"Cumulated Rainfall (mm)\",\n",
    "        \"RSKF\": \"Cumulated Rainfall Type\",\n",
    "        \" SDK\": \"Sunshine Duration (hours)\",\n",
    "        \"SHK_TAG\": \"Snow Height (cm)\",\n",
    "        \"  NM\": \"Cloud Coverage (1/8)\",\n",
    "        \" VPM\": \"Vapor Pressure (hPa)\",\n",
    "        \"  PM\": \"Pressure (hPa)\",\n",
    "        \" TMK\": \"Temperature Mean (°C)\",\n",
    "        \" UPM\": \"Humidity (%)\",\n",
    "        \" TXK\": \"Temperature Max at 2m (°C)\",\n",
    "        \" TNK\": \"Temperature Min at 2m (°C)\",\n",
    "        \" TGK\": \"Temperature Min at 5cm (°C)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Water Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ts_gw_df[\"Parameter\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_translated = [translator.translate(item, dest='en').text for item in parameters.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cumulated rainfall\n",
    "-Environmental temperature\n",
    "-Water temperature\n",
    "-Conductivity\n",
    "-Flow river\n",
    "Turbidity\n",
    "-Absorbance 254 nm\n",
    "-Ammonium\n",
    "Dissolved oxygen\n",
    "-Nitrate\n",
    "-pH\n",
    "Redox potential\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset per Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"Temperatur (Luft)\": \"Air Temperature (°C)\",\n",
    "    \"Temperatur (Wasser)\": \"Water Temperature (°C)\",\n",
    "    \"UV-Adsorption (254)\": \"UVA254 (1/m)\",\n",
    "    \"Leitfähigkeit 25°C vor Ort\": \"Conductivity (µS/cm)\",\n",
    "    \"Ammonium\": \"Ammonium (mg/l)\",\n",
    "    \"Nitrat\": \"Nitrate (mg/l)\",\n",
    "    \"pH-Wert (Feld)\": \"pH\",\n",
    "    \"Dichlormethan\": \"Dichloromethane (µg/l)\",\n",
    "    \"Trichlormethan\": \"Trichloromethane (µg/l)\",\n",
    "    \"Tetrachlormethan\": \"Tetrachloromethane (µg/l)\",\n",
    "    \"Bromoform\": \"Bromoform (µg/l)\",\n",
    "    \"Bromdichlormethan\": \"Bromodichloromethane (µg/l)\",\n",
    "    \"Dibromchlormethan\": \"Dibromochloromethane (µg/l)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df = ts_gw_df[ts_gw_df[\"Parameter\"].isin(variables.keys())]\n",
    "\n",
    "ground_df[\"Parameter\"] = ground_df[\"Parameter\"].map(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df[\"Station ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df[\"DateTime\"] = pd.to_datetime(ground_df[\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_stations_dict = {}\n",
    "for station in ground_df[\"Station ID\"].unique():\n",
    "    station_df = ground_df[ground_df[\"Station ID\"] == station]\n",
    "    station_df = station_df.pivot_table(\n",
    "        index=pd.Grouper(\"DateTime\"),\n",
    "        columns=\"Parameter\",\n",
    "        values=\"Value\",\n",
    "    )\n",
    "\n",
    "    gw_stations_dict[station] = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_info_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [ground_df[\"Station ID\"].unique(), variables.values()],\n",
    "        names=[\"Station ID\", \"Parameter\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5049 - Treptow-Köpenick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = gw_stations_dict[5049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isna().sum() / station_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency of the time series\n",
    "station_df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time series have a frequency of 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 5049\",\n",
    "        labels={\"Date\": \"Date\", column: column},\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 5049\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to nan the invalid values\n",
    "station_df.loc[\n",
    "    station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Dichloromethane (µg/l)\"] < 0, [\"Dichloromethane (µg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Tetrachloromethane (µg/l)\"] < 0,\n",
    "    [\"Tetrachloromethane (µg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Trichloromethane (µg/l)\"] < 0,\n",
    "    [\"Trichloromethane (µg/l)\"],\n",
    "] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    if station_df[column].dropna().shape[0] <= 1:\n",
    "        continue\n",
    "\n",
    "    start_date = (\n",
    "        station_df[column].dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "    end_date = (\n",
    "        station_df[column].dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "\n",
    "    df = station_df[start_date:end_date][column]\n",
    "\n",
    "    print(f\"Start date for {column}: {start_date}\")\n",
    "    print(f\"End date for {column}: {end_date}\")\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"Missing values for {column}: {missing_values}\")\n",
    "\n",
    "    frequency = df.index.to_series().diff().value_counts().index[0].days\n",
    "    print(f\"Frequency for {column}: {frequency}\")\n",
    "    print()\n",
    "\n",
    "    ground_info_df.loc[\"N Samples\", (5049, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    ground_info_df.loc[\n",
    "        \"% Missing Values\", (5049, column)\n",
    "    ] = missing_values\n",
    "    ground_info_df.loc[\"Frequency (days)\", (5049, column)] = frequency\n",
    "    ground_info_df.loc[\"Start Date\", (5049, column)] = start_date\n",
    "    ground_info_df.loc[\"End Date\", (5049, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isna().sum() / station_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(\n",
    "    columns=[\n",
    "        \"Dichloromethane (µg/l)\",\n",
    "        \"Tetrachloromethane (µg/l)\",\n",
    "        \"Trichloromethane (µg/l)\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# no outliers detected\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[\n",
    "        forecasting_final[\"anomaly\"] == \"No\"\n",
    "    ]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "    \n",
    "    station_df_copy.loc[df.index, column] = df\n",
    "    \n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the UVA254 date range\n",
    "start_date = station_df[\"UVA254 (1/m)\"].dropna().index.min()\n",
    "end_date = station_df[\"UVA254 (1/m)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_5049_df = station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da mettere nell'altro notebook\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    # compute linear regression on trend\n",
    "    X = np.arange(df.shape[0])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df.copy()\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # plot the line of the linear regression\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trend.index,\n",
    "            y=trend,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    slope = results.params[1]\n",
    "\n",
    "    print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "    p_value = results.pvalues[1]\n",
    "    print(f\"{column} - P-value: {p_value}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=line.index,\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7285 - Steglitz-Zehlendorf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = gw_stations_dict[7285]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isna().sum() / station_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency of the time series\n",
    "station_df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time series have a frequency of 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 7285\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 7285\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Bromodichloromethane (µg/l)\",\n",
    "    \"Bromoform (µg/l)\",\n",
    "    \"Dibromochloromethane (µg/l)\",\n",
    "    \"Dichloromethane (µg/l)\",\n",
    "    \"Tetrachloromethane (µg/l)\",\n",
    "    \"Trichloromethane (µg/l)\",\n",
    "    \"Nitrate (mg/l)\",\n",
    "]\n",
    "\n",
    "# set to nan the invalid values for the columns\n",
    "for column in cols:\n",
    "    station_df.loc[station_df[column] < 0, [column]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    if station_df[column].dropna().shape[0] < 2:\n",
    "        continue\n",
    "\n",
    "    start_date = (\n",
    "        station_df[column].dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "    end_date = (\n",
    "        station_df[column].dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "\n",
    "    df = station_df[start_date:end_date][column]\n",
    "\n",
    "    print(f\"Start date for {column}: {start_date}\")\n",
    "    print(f\"End date for {column}: {end_date}\")\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"Missing values for {column}: {missing_values}\")\n",
    "\n",
    "    frequency = df.index.to_series().diff().value_counts().index[0].days\n",
    "    print(f\"Frequency for {column}: {frequency}\")\n",
    "\n",
    "    ground_info_df.loc[\"N Samples\", (7285, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    ground_info_df.loc[\n",
    "        \"% Missing Values\", (7285, column)\n",
    "    ] = missing_values\n",
    "    ground_info_df.loc[\"Frequency (days)\", (7285, column)] = frequency\n",
    "    ground_info_df.loc[\"Start Date\", (7285, column)] = start_date\n",
    "    ground_info_df.loc[\"End Date\", (7285, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isna().sum() / station_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(columns=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[\n",
    "        forecasting_final[\"anomaly\"] == \"No\"\n",
    "    ]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the UVA254 date range\n",
    "start_date = station_df[\"UVA254 (1/m)\"].dropna().index.min()\n",
    "end_date = station_df[\"UVA254 (1/m)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_7285_df = station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da mettere nell'altro notebook\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    # compute linear regression on trend\n",
    "    X = np.arange(df.shape[0])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df.copy()\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # plot the line of the linear regression\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trend.index,\n",
    "            y=trend,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    slope = results.params[1]\n",
    "    print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "    p_value = results.pvalues[1]\n",
    "    print(f\"{column} - P-value: {p_value}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=line.index,\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Unique Ground Water Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build unique dataframe for all stations\n",
    "# set the number of the station as further variable\n",
    "gw_5049_df[\"Station\"] = 5049\n",
    "gw_7285_df[\"Station\"] = 7285\n",
    "\n",
    "gw_5049_df.index.name = \"DateTime\"\n",
    "gw_7285_df.index.name = \"DateTime\"\n",
    "\n",
    "gw_5049_df.reset_index(inplace=True)\n",
    "gw_7285_df.reset_index(inplace=True)\n",
    "\n",
    "gw_df = pd.concat([gw_5049_df, gw_7285_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UVA254 vs Ammonium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"red\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for station_id, station_df in gw_stations_dict.items():\n",
    "    df = station_df[[\"Ammonium (mg/l)\", \"UVA254 (1/m)\"]].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    X = df[\"Ammonium (mg/l)\"].copy()\n",
    "\n",
    "    # X = scaler.fit_transform(X.values.reshape(-1, 1))\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "    y = df[\"UVA254 (1/m)\"].copy()\n",
    "\n",
    "    # y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    slope = results.params[1]\n",
    "    p_value = results.pvalues[1]\n",
    "\n",
    "    print(f\"Station {station_id} - Slope: {slope}\")\n",
    "    print(f\"Station {station_id} - P-value: {p_value}\")\n",
    "\n",
    "    color = colors.pop()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[\"Ammonium (mg/l)\"],\n",
    "            y=y,\n",
    "            mode=\"markers\",\n",
    "            name=f\"Station {station_id}\",\n",
    "            marker=dict(size=8, opacity=0.7, color=color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[\"Ammonium (mg/l)\"],\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression Station {station_id}\",\n",
    "            line=dict(dash=\"dash\", color=color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Ammonium (mg/l)\",\n",
    "    yaxis_title=\"UVA254 (1/m)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "    title=\"Ground Water\",\n",
    "    # legend=dict(\n",
    "    #     yanchor=\"top\",\n",
    "    #     y=0.99,\n",
    "    #     xanchor=\"right\",\n",
    "    #     x=0.99\n",
    "    # )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Water Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ts_sw_df[\"Parameter\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_translated = [translator.translate(item, dest='en').text for item in parameters.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cumulated rainfall\n",
    "-Environmental temperature\n",
    "-Water temperature\n",
    "-Conductivity\n",
    "-Flow river\n",
    "Turbidity\n",
    "-Absorbance 254 nm\n",
    "-Ammonium\n",
    "-Dissolved oxygen\n",
    "-Nitrate\n",
    "-pH\n",
    "Redox potential\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset per Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters that are present for the moment are:\n",
    "variables = {\n",
    "    \"Lufttemperatur\": \"Air Temperature (°C)\",\n",
    "    \"Wassertemperatur\": \"Water Temperature (°C)\",\n",
    "    \"Spektraler Absorptionskoeffizient (SAK) 254nm\": \"UVA254 (1/m)\",\n",
    "    \"Leitfähigkeit\": \"Conductivity (µS/cm)\",\n",
    "    \"Ammonium-Stickstoff\": \"Ammonium (mg/l)\",\n",
    "    \"Nitrat-Stickstoff\": \"Nitrate (mg/l)\",\n",
    "    \"pH-Wert\": \"pH\",\n",
    "    \"DOC (Gelöster organischer Kohlenstoff)\": \"DOC (mg/l)\",\n",
    "    \"TOC (Organischer Kohlenstoff)\": \"TOC (mg/l)\",\n",
    "    \"Sauerstoff-Gehalt\": \"Dissolved Oxygen (mg/l)\",\n",
    "    \"Coliforme B.\": \"Coliform (MPN/100ml)\",\n",
    "    \"E.Coli\": \"E.Coli (MPN/100ml)\",\n",
    "    \"BSB1 (Biochem. Sauerstoffbedarf, 24h)\": \"BOD (mg/l)\",\n",
    "    \"Intestinale Enterokokken\": \"Enterococcus (MPN/100ml)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = ts_sw_df[ts_sw_df[\"Parameter\"].isin(variables.keys())]\n",
    "\n",
    "surface_df[\"Parameter\"] = surface_df[\"Parameter\"].map(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df[\"Station ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df[\"Station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df[\"DateTime\"] = pd.to_datetime(surface_df[\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_dict = {}\n",
    "for station in surface_df[\"Station ID\"].unique():\n",
    "    station_df = surface_df[surface_df[\"Station ID\"] == station]\n",
    "    station_df = station_df.pivot_table(\n",
    "        index=pd.Grouper(\"DateTime\"),\n",
    "        columns=\"Parameter\",\n",
    "        values=\"Value\",\n",
    "    )\n",
    "\n",
    "    stations_dict[station] = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Bacteria Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_columns = [\n",
    "    \"E.Coli (MPN/100ml)\",\n",
    "    \"Coliform (MPN/100ml)\",\n",
    "    \"Enterococcus (MPN/100ml)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already build the final dataset for the bacteria\n",
    "\n",
    "bacteria_dict = {}\n",
    "for station in surface_df[\"Station ID\"].unique():\n",
    "    station_df = surface_df[surface_df[\"Station ID\"] == station]\n",
    "    station_df = station_df.pivot_table(\n",
    "        index=pd.Grouper(\"DateTime\"),\n",
    "        columns=\"Parameter\",\n",
    "        values=[\"Value\", \"LOQ\"],\n",
    "    )\n",
    "\n",
    "    # get only the bacteria columns\n",
    "    station_df = station_df[\n",
    "        station_df.columns[station_df.columns.get_level_values(1).isin(bacteria_columns)]\n",
    "    ]\n",
    "    \n",
    "    station_df['Station'] = station\n",
    "    station_df.index = station_df.index.date\n",
    "    \n",
    "    bacteria_dict[station] = station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_df = pd.concat(bacteria_dict.values(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bacteria from the station_df datasets in station_dict\n",
    "\n",
    "for station in stations_dict.keys():\n",
    "    station_df = stations_dict[station]\n",
    "    station_df = station_df[\n",
    "        ~station_df.index.isin(bacteria_dict[station].index)\n",
    "    ]\n",
    "    stations_dict[station] = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Stations\n",
    "\n",
    "Coliform: /100 ml\n",
    "\n",
    "E.Coli: /100 ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common columns for all the stations\n",
    "common_columns = set(stations_dict[105].columns)\n",
    "for station_id, station_df in stations_dict.items():\n",
    "    common_columns = common_columns.intersection(station_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for station_id, station_df in stations_dict.items():\n",
    "        column_df = station_df[column].copy()\n",
    "\n",
    "        column_df.dropna(inplace=True)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=column_df.index,\n",
    "                y=column_df,\n",
    "                mode=\"lines\",\n",
    "                name=f\"Station {station_id}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_info_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [surface_df[\"Station ID\"].unique(), variables.values()],\n",
    "        names=[\"Station ID\", \"Parameter\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 105 - Dämeritzsee-Seemitte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = stations_dict[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "\n",
    "station_flow_df = flow_df[flow_df[\"Station ID\"] == 5827101]\n",
    "\n",
    "station_flow_df = station_flow_df[[\"DateTime\", \"Flow River\"]].set_index(\n",
    "    \"DateTime\"\n",
    ")\n",
    "\n",
    "station_flow_df.index = station_flow_df.index.date\n",
    "station_df.index = station_df.index.date\n",
    "\n",
    "# merge the flow data with the surface water data for the same date (just date, not time)\n",
    "station_df = station_df.merge(\n",
    "    station_flow_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "station_df.index = pd.to_datetime(station_df.index)\n",
    "\n",
    "station_df.rename(\n",
    "    columns={\"Flow River\": \"Flow River Rate (m³/s)\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency of the time series\n",
    "station_df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time series have a frequency of 14 days or 1 month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 105 - Range: {date_range[0].date()} - {date_range[1].date()}\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 105\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[station_df[\"DOC (mg/l)\"] <= 0, [\"DOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[station_df[\"TOC (mg/l)\"] <= 0, [\"TOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Flow River Rate (m³/s)\"] < 0, [\"Flow River Rate (m³/s)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Dissolved Oxygen (mg/l)\"] < 0,\n",
    "    [\"Dissolved Oxygen (mg/l)\"],\n",
    "] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"E.Coli (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"E.Coli (MPN/100ml)\"] > 3000),\n",
    "#     [\"E.Coli (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"Coliform (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"Coliform (MPN/100ml)\"] >= 5000),\n",
    "#     [\"Coliform (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "station_df.loc[\n",
    "    (station_df[\"BOD (mg/l)\"] < 0) | (station_df[\"BOD (mg/l)\"] > 100),\n",
    "    [\"BOD (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"pH\"] < 7, [\"pH\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 105\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    start_date = df.dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    surface_info_df.loc[\"N Samples\", (105, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    surface_info_df.loc[\n",
    "        \"% Missing Values\", (105, column)\n",
    "    ] = missing_values\n",
    "    surface_info_df.loc[\"Frequency (days)\", (105, column)] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "    surface_info_df.loc[\"Start Date\", (105, column)] = start_date\n",
    "    surface_info_df.loc[\"End Date\", (105, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.drop(columns=\"UVA254 (1/m)\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    # df.interpolate(method='time', inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"ds\"],\n",
    "            y=df[\"y\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction\",\n",
    "            line=dict(color=\"red\"),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Anomaly\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "            line=dict(color=\"green\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "            line=dict(color=\"orange\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        # legend=dict(\n",
    "        #         yanchor=\"top\",\n",
    "        #         y=0.99,\n",
    "        #         xanchor=\"left\",\n",
    "        #         x=0.01\n",
    "        #     )\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[\n",
    "        forecasting_final[\"anomaly\"] == \"No\"\n",
    "    ]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the DOC date range\n",
    "start_date = station_df[\"DOC (mg/l)\"].dropna().index.min()\n",
    "end_date = station_df[\"DOC (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_105_df = station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    # compute linear regression on trend\n",
    "    X = np.arange(df.shape[0])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df.copy()\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # plot the line of the linear regression\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trend.index,\n",
    "            y=trend,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    slope = results.params[1]\n",
    "\n",
    "    print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "    p_value = results.pvalues[1]\n",
    "    print(f\"{column} - P-value: {p_value}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=line.index,\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 305 - Oberhavel-Konradshöhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = stations_dict[305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "\n",
    "station_flow_df = flow_df[flow_df[\"Station ID\"] == 5815911]\n",
    "\n",
    "station_flow_df = station_flow_df[[\"DateTime\", \"Flow River\"]].set_index(\n",
    "    \"DateTime\"\n",
    ")\n",
    "\n",
    "station_flow_df.index = station_flow_df.index.date\n",
    "station_df.index = station_df.index.date\n",
    "\n",
    "# merge the flow data with the surface water data for the same date (just date, not time)\n",
    "station_df = station_df.merge(\n",
    "    station_flow_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "station_df.rename(\n",
    "    columns={\"Flow River\": \"Flow River Rate (m³/s)\"}, inplace=True\n",
    ")\n",
    "\n",
    "station_df.index = pd.to_datetime(station_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency of the time series\n",
    "station_df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time series have a frequency of 14 days or 1 month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    # compute date range for which the data is available\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 305 - Range: {date_range[0].date()} - {date_range[1].date()}\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 305\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[\n",
    "    (station_df[\"DOC (mg/l)\"] <= 0) | (station_df[\"DOC (mg/l)\"] >= 20),\n",
    "    [\"DOC (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"TOC (mg/l)\"] <= 0, [\"TOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Flow River Rate (m³/s)\"] < 0, [\"Flow River Rate (m³/s)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Dissolved Oxygen (mg/l)\"] < 0,\n",
    "    [\"Dissolved Oxygen (mg/l)\"],\n",
    "] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"E.Coli (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"E.Coli (MPN/100ml)\"] > 3000),\n",
    "#     [\"E.Coli (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"Coliform (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"Coliform (MPN/100ml)\"] >= 5000),\n",
    "#     [\"Coliform (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "station_df.loc[\n",
    "    (station_df[\"BOD (mg/l)\"] < 0) | (station_df[\"BOD (mg/l)\"] > 100),\n",
    "    [\"BOD (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]\n",
    "] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 305\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    start_date = df.dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    surface_info_df.loc[\"N Samples\", (305, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    surface_info_df.loc[\n",
    "        \"% Missing Values\", (305, column)\n",
    "    ] = missing_values\n",
    "    surface_info_df.loc[\"Frequency (days)\", (305, column)] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "    surface_info_df.loc[\"Start Date\", (305, column)] = start_date\n",
    "    surface_info_df.loc[\"End Date\", (305, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    # substitute the outliers with the trend + seasonal components of the STL decomposition\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[\n",
    "        forecasting_final[\"anomaly\"] == \"No\"\n",
    "    ]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the DOC date range\n",
    "start_date = station_df[\"DOC (mg/l)\"].dropna().index.min()\n",
    "end_date = station_df[\"DOC (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_305_df = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    # compute linear regression on trend\n",
    "    X = np.arange(df.shape[0])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # plot the line of the linear regression\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trend.index,\n",
    "            y=trend,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # get the slope of the regression\n",
    "    slope = results.params[1]\n",
    "\n",
    "    print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "    p_value = results.pvalues[1]\n",
    "    print(f\"{column} - P-value: {p_value}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=line.index,\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 325 - Havel-Pichelsdorfer Gemünd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = stations_dict[325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "\n",
    "station_flow_df = flow_df[flow_df[\"Station ID\"] == 5803200]\n",
    "\n",
    "station_flow_df = station_flow_df[[\"DateTime\", \"Flow River\"]].set_index(\n",
    "    \"DateTime\"\n",
    ")\n",
    "\n",
    "station_flow_df.index = station_flow_df.index.date\n",
    "station_df.index = station_df.index.date\n",
    "\n",
    "# merge the flow data with the surface water data for the same date (just date, not time)\n",
    "station_df = station_df.merge(\n",
    "    station_flow_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "station_df.rename(\n",
    "    columns={\"Flow River\": \"Flow River Rate (m³/s)\"}, inplace=True\n",
    ")\n",
    "\n",
    "station_df.index = pd.to_datetime(station_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.isna().sum() / station_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency of the time series\n",
    "station_df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the time series have a frequency of 14 days or 1 month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "for column in station_df.columns:\n",
    "    fig = px.line(\n",
    "        station_df,\n",
    "        x=station_df.index,\n",
    "        y=column,\n",
    "        title=f\"{column} at station 325\",\n",
    "        labels={\"DateTime\": \"DateTime\", column: column},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 325\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[\n",
    "    (station_df[\"DOC (mg/l)\"] > 15) | (station_df[\"DOC (mg/l)\"] < 4.5),\n",
    "    [\"DOC (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"TOC (mg/l)\"] <= 0, [\"TOC (mg/l)\"]] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Flow River Rate (m³/s)\"] < 0, [\"Flow River Rate (m³/s)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Dissolved Oxygen (mg/l)\"] < 0,\n",
    "    [\"Dissolved Oxygen (mg/l)\"],\n",
    "] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"E.Coli (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"E.Coli (MPN/100ml)\"] > 3000),\n",
    "#     [\"E.Coli (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "# station_df.loc[\n",
    "#     (station_df[\"Coliform (MPN/100ml)\"] < 0)\n",
    "#     | (station_df[\"Coliform (MPN/100ml)\"] >= 5000),\n",
    "#     [\"Coliform (MPN/100ml)\"],\n",
    "# ] = np.nan\n",
    "station_df.loc[\n",
    "    (station_df[\"BOD (mg/l)\"] < 0) | (station_df[\"BOD (mg/l)\"] > 100),\n",
    "    [\"BOD (mg/l)\"],\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Ammonium (mg/l)\"] < 0, [\"Ammonium (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[\n",
    "    station_df[\"Nitrate (mg/l)\"] < 0, [\"Nitrate (mg/l)\"]\n",
    "] = np.nan\n",
    "station_df.loc[station_df[\"pH\"] < 7, [\"pH\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "    column_df = station_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at station 105\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    start_date = df.dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    surface_info_df.loc[\"N Samples\", (325, column)] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    surface_info_df.loc[\n",
    "        \"% Missing Values\", (325, column)\n",
    "    ] = missing_values\n",
    "    surface_info_df.loc[\"Frequency (days)\", (325, column)] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "    surface_info_df.loc[\"Start Date\", (325, column)] = start_date\n",
    "    surface_info_df.loc[\"End Date\", (325, column)] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, compute the % of missing values\n",
    "for column in station_df.columns:\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    df = station_df[date_range[0] : date_range[1]][column]\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"{column}: {missing_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition and Prophet\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is used to remove outliers\n",
    "\n",
    "# create copy such that the processed columns do not affect the original dataframe until the end\n",
    "station_df_copy = station_df.copy()\n",
    "\n",
    "station_df_copy = station_df_copy.resample(\"M\").median()\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers\n",
    "    forecasting_final = forecasting_final[\n",
    "        forecasting_final[\"anomaly\"] == \"No\"\n",
    "    ]\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df.rename(columns={\"y\": column}, inplace=True)\n",
    "\n",
    "    # redo the resampling since the outliers have been removed and\n",
    "    # some months may have been removed\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    station_df_copy.loc[df.index, column] = df[column]\n",
    "\n",
    "\n",
    "station_df = station_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in station_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the dataset based on the DOC date range\n",
    "start_date = station_df[\"DOC (mg/l)\"].dropna().index.min()\n",
    "end_date = station_df[\"DOC (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_325_df = station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    date_range = station_df[column].dropna().index\n",
    "    date_range = date_range.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    # compute linear regression on trend\n",
    "    X = np.arange(df.shape[0])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # plot the line of the linear regression\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trend.index,\n",
    "            y=trend,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # get the slope of the regression\n",
    "    slope = results.params[1]\n",
    "\n",
    "    print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "    p_value = results.pvalues[1]\n",
    "    print(f\"{column} - P-value: {p_value}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=line.index,\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Unique Surface Water Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build unique dataframe for all stations\n",
    "# set the number of the station as further variable\n",
    "sw_105_df[\"Station\"] = 105\n",
    "sw_305_df[\"Station\"] = 305\n",
    "sw_325_df[\"Station\"] = 325\n",
    "\n",
    "sw_105_df.index.name = \"DateTime\"\n",
    "sw_305_df.index.name = \"DateTime\"\n",
    "sw_325_df.index.name = \"DateTime\"\n",
    "\n",
    "sw_105_df.reset_index(inplace=True)\n",
    "sw_305_df.reset_index(inplace=True)\n",
    "sw_325_df.reset_index(inplace=True)\n",
    "\n",
    "# merge the dataframes\n",
    "sw_df = pd.concat([sw_105_df, sw_305_df, sw_325_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC vs TOC per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in stations_dict.keys():\n",
    "    station_df = stations_dict[station_id]\n",
    "\n",
    "    # plot the doc and toc in a scatter plot to see if there is a correlation\n",
    "    fig = px.scatter(\n",
    "        station_df,\n",
    "        x=\"DOC (mg/l)\",\n",
    "        y=\"TOC (mg/l)\",\n",
    "        trendline=\"ols\",\n",
    "        trendline_color_override=\"red\",\n",
    "        trendline_scope=\"overall\",\n",
    "    )\n",
    "\n",
    "    results = px.get_trendline_results(fig)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df[\"DOC (mg/l)\"],\n",
    "            y=station_df[\"TOC (mg/l)\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Data\",\n",
    "            marker=dict(size=8, color=\"blue\", opacity=0.7),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add line on bisector\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=[0, 20],\n",
    "    #         y=[0, 20],\n",
    "    #         mode='lines',\n",
    "    #         name='Bisector',\n",
    "    #         line=dict(\n",
    "    #             color='red',\n",
    "    #             width=2,\n",
    "    #             dash='dash'\n",
    "    #         )\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # get the slope and intercept of the trendline\n",
    "    slope = results.iloc[0][\"px_fit_results\"].params[1]\n",
    "    intercept = results.iloc[0][\"px_fit_results\"].params[0]\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=0.9,\n",
    "        y=0.1,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=f\"y = {slope:.2f}x + {intercept:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=18, color=\"red\"),\n",
    "    )\n",
    "\n",
    "    x = np.linspace(2, 14, 100)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=slope * x + intercept,\n",
    "            mode=\"lines\",\n",
    "            name=\"Overall Trendline\",\n",
    "            line=dict(color=\"red\", width=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add the equation to the legend\n",
    "    fig.update_traces(\n",
    "        name=f\"Linear Regression\",\n",
    "        selector=dict(name=\"Overall Trendline\"),\n",
    "    )\n",
    "\n",
    "    if station_id == 105:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"DOC (mg/l)\",\n",
    "            yaxis_title=\"TOC (mg/l)\",\n",
    "            font=dict(\n",
    "                size=18,\n",
    "            ),\n",
    "            title=f\"DOC vs TOC at station {station_id}\",\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"DOC (mg/l)\",\n",
    "            yaxis_title=\"TOC (mg/l)\",\n",
    "            font=dict(\n",
    "                size=18,\n",
    "            ),\n",
    "            title=f\"DOC vs TOC at station {station_id}\",\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "        )\n",
    "\n",
    "    fig.show(width=20, height=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC vs Ammonium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"red\", \"green\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for station_id, station_df in stations_dict.items():\n",
    "    station_df.index = pd.to_datetime(station_df.index)\n",
    "\n",
    "    station_df = station_df.resample(\"M\").median()\n",
    "\n",
    "    station_df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    df = station_df[[\"Ammonium (mg/l)\", \"DOC (mg/l)\"]].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # compute linear regression and plot the line\n",
    "    X = df[\"Ammonium (mg/l)\"].copy()\n",
    "    X = sm.add_constant(X)\n",
    "    y = df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    slope = results.params[1]\n",
    "    p_value = results.pvalues[1]\n",
    "\n",
    "    print(f\"Station {station_id} - Slope: {slope}\")\n",
    "    print(f\"Station {station_id} - P-value: {p_value}\")\n",
    "\n",
    "    color = colors.pop()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Ammonium (mg/l)\"],\n",
    "            y=df[\"DOC (mg/l)\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Station {station_id}\",\n",
    "            marker=dict(size=8, opacity=0.7, color=color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Ammonium (mg/l)\"],\n",
    "            y=line,\n",
    "            mode=\"lines\",\n",
    "            name=f\"Linear Regression Station {station_id}\",\n",
    "            line=dict(dash=\"dash\", color=color),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Ammonium (mg/l)\",\n",
    "    yaxis_title=\"DOC (mg/l)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "    title=\"Surface Water\",\n",
    "    # legend=dict(\n",
    "    #     yanchor=\"top\",\n",
    "    #     y=0.99,\n",
    "    #     xanchor=\"right\",\n",
    "    #     x=0.99\n",
    "    # )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns = [\"QN_3\", \"QN_4\", \"eor\", \"Cumulated Rainfall Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df[\"DateTime\"] = pd.to_datetime(\n",
    "    meteo_df[\"DateTime\"], format=\"%Y%m%d\"\n",
    ")\n",
    "\n",
    "meteo_df.set_index(\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.drop(columns=[\"Station ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.loc[\n",
    "    meteo_df[\"Cumulated Rainfall (mm)\"] < 0, [\"Cumulated Rainfall (mm)\"]\n",
    "] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to nan every value that is equal to -999 or -999.0 in the dataframe\n",
    "meteo_df.replace(-999, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_info_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=pd.Index([\"Parameter\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=meteo_df.index,\n",
    "            y=meteo_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=column,\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        title=column,\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the data\n",
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    fig = go.Figure()\n",
    "    column_df = meteo_df[column]\n",
    "\n",
    "    for year in column_df.index.year.unique():\n",
    "        fig.add_trace(\n",
    "            go.Box(y=column_df[column_df.index.year == year], name=year)\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"{column} at airport\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=column,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    start_date = (\n",
    "        meteo_df[column].dropna().index.min().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "    end_date = (\n",
    "        meteo_df[column].dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "    )\n",
    "\n",
    "    df = meteo_df[start_date:end_date][column]\n",
    "\n",
    "    print(f\"Start date for {column}: {start_date}\")\n",
    "    print(f\"End date for {column}: {end_date}\")\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"Missing values for {column}: {missing_values}\")\n",
    "\n",
    "    frequency = df.index.to_series().diff().value_counts().index[0].days\n",
    "    print(f\"Frequency for {column}: {frequency}\")\n",
    "\n",
    "    meteo_info_df.loc[\"N Samples\", column] = (\n",
    "        meteo_df[column].dropna().shape[0]\n",
    "    )\n",
    "    meteo_info_df.loc[\"% Missing Values\", column] = missing_values\n",
    "    meteo_info_df.loc[\"Frequency (days)\", column] = frequency\n",
    "    meteo_info_df.loc[\"Start Date\", column] = start_date\n",
    "    meteo_info_df.loc[\"End Date\", column] = end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers and Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the outliers through the STL decomposition\n",
    "\n",
    "for column in meteo_df.columns.difference(diff_columns):\n",
    "    df = meteo_df[column].copy()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df.resample(\"M\").median()\n",
    "\n",
    "    df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    date_range = meteo_df[column].dropna().index\n",
    "    date_range = meteo_df.min(), date_range.max()\n",
    "\n",
    "    # make sure that the dataframe starts and finishes in the same month\n",
    "    start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "    # Slice the dataframe to start from the found index\n",
    "    df = df.loc[start_index:]\n",
    "\n",
    "    # === STL decomposition ===\n",
    "\n",
    "    stl = STL(df, period=12)\n",
    "\n",
    "    result = stl.fit()\n",
    "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "    denoised_df = trend + seasonal\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "\n",
    "    threshold = 3 * std_resid\n",
    "\n",
    "    outliers_index = resid[\n",
    "        (resid > mean_resid + threshold)\n",
    "        | (resid < mean_resid - threshold)\n",
    "    ].index\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=denoised_df,\n",
    "            mode=\"lines\",\n",
    "            name=\"Trend + Seasonal (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== STL =====\")\n",
    "    print()\n",
    "\n",
    "    # compute rmse between the original and the denoised\n",
    "    MAE = mean_absolute_error(df, denoised_df)\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(df, denoised_df)\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(df, denoised_df)\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(int(mean_squared_error(df, denoised_df)))\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(df, denoised_df)\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    # compute std of original and std of residuals\n",
    "    std_df = df.std()\n",
    "\n",
    "    print()\n",
    "    print(f\"Data std: {std_df}, Resid std: {std_resid}\")\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=df.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=resid,\n",
    "            mode=\"lines\",\n",
    "            name=\"Residuals\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=outliers_index,\n",
    "            y=resid.loc[outliers_index],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (STL)\",\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # plot the threshold\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid + threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=[mean_resid - threshold] * df.shape[0],\n",
    "            mode=\"lines\",\n",
    "            name=\"Threshold\",\n",
    "            line=dict(dash=\"dash\", color=\"black\"),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with your original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    factor = 1.5\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(\"===== Prophet =====\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Absolute Error (MAE): \" + str(np.round(MAE, 2)))\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    MEDAE = median_absolute_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Median Absolute Error (MedAE): \" + str(np.round(MEDAE, 2)))\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    MSE = mean_squared_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\"Mean Squared Error (MSE): \" + str(np.round(MSE, 2)))\n",
    "\n",
    "    # Root Mean Squarred Error (RMSE)\n",
    "    RMSE = np.sqrt(\n",
    "        int(\n",
    "            mean_squared_error(\n",
    "                forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Root Mean Squared Error (RMSE): \" + str(np.round(RMSE, 2)))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = mean_absolute_percentage_error(\n",
    "        forecasting_final[\"yhat\"], forecasting_final[\"y\"]\n",
    "    )\n",
    "    print(\n",
    "        \"Mean Absolute Percentage Error (MAPE): \"\n",
    "        + str(np.round(MAPE, 2))\n",
    "        + \" %\"\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"yhat\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Prediction (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"error\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Error\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecasting_final[\"ds\"],\n",
    "            y=forecasting_final[\"uncertainty\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Uncertainty\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anomaly[\"ds\"],\n",
    "            y=anomaly[\"error\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Outliers (Prophet)\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to remove outliers\n",
    "meteo_df.drop(columns=diff_columns, inplace=True)\n",
    "\n",
    "meteo_df = meteo_df.resample(\"M\").median()\n",
    "\n",
    "meteo_df.interpolate(method=\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "\n",
    "for column in meteo_df.columns:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=meteo_df.index,\n",
    "            y=meteo_df[column],\n",
    "            mode=\"lines\",\n",
    "            name=\"Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=column,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=column,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Meteo Data to Surface and Ground Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to compare the air temperature between the airport and the stations first\n",
    "\n",
    "# plot the air temperature for the airport and the stations\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df.index,\n",
    "        y=meteo_df[\"Temperature Mean (°C)\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Airport\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df[\"DateTime\"],\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Temperature (°C)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "    title=\"Temperature\",\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pearson correlation\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "    station_df = station_df[\n",
    "        (station_df[\"DateTime\"] >= start_date)\n",
    "        & (station_df[\"DateTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # compute pearson correlation\n",
    "    corr, _ = pearsonr(\n",
    "        airport_df[\"Temperature Mean (°C)\"],\n",
    "        station_df[\"Air Temperature (°C)\"],\n",
    "    )\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df.index,\n",
    "            y=airport_df[\"Temperature Mean (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Airport\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"],\n",
    "            y=sw_df[sw_df[\"Station\"] == station_id][\n",
    "                \"Air Temperature (°C)\"\n",
    "            ],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add the correlation to the plot\n",
    "    fig.add_annotation(\n",
    "        x=0.01,\n",
    "        y=0.95,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=f\"Pearson Correlation: {corr:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Air Temperature (°C)\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the air temperature between the airport and the stations\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "    station_df = station_df[\n",
    "        (station_df[\"DateTime\"] >= start_date)\n",
    "        & (station_df[\"DateTime\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df[\"Temperature Mean (°C)\"],\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Data\",\n",
    "            marker=dict(size=8, color=\"blue\", opacity=0.7),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add line on bisector\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[-10, 40],\n",
    "            y=[-10, 40],\n",
    "            mode=\"lines\",\n",
    "            name=\"Bisector\",\n",
    "            line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Airport\",\n",
    "        yaxis_title=f\"Station {station_id}\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        title=\"Air Temperature\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the sw_df first to have unique indices\n",
    "sw_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation is high between the airport and the stations,\n",
    "# so we can add the airport data variables to the stations\n",
    "\n",
    "# add the rainfall data to the stations\n",
    "sw_df[\"Cumulated Rainfall (mm)\"] = np.nan\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    start_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = sw_df[sw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    # Identify the indices in sw_df that match the station_id and are within the date range\n",
    "    indices = sw_df[\n",
    "        (sw_df[\"Station\"] == station_id)\n",
    "        & (sw_df[\"DateTime\"] >= start_date)\n",
    "        & (sw_df[\"DateTime\"] <= end_date)\n",
    "    ].index\n",
    "\n",
    "    # Directly update sw_df for the matching indices\n",
    "    sw_df.loc[indices, \"Cumulated Rainfall (mm)\"] = airport_df[\n",
    "        \"Cumulated Rainfall (mm)\"\n",
    "    ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df[\"Cumulated Rainfall (mm)\"].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.drop(columns=[\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the date range for every station and variable\n",
    "\n",
    "for station_id in sw_df[\"Station\"].unique():\n",
    "    station_df = sw_df[sw_df[\"Station\"] == station_id]\n",
    "\n",
    "    for column in station_df.columns.difference(\n",
    "        [\"DateTime\", \"Station\"]\n",
    "    ):\n",
    "        start_date = (\n",
    "            station_df[[\"DateTime\", column]].dropna()[\"DateTime\"].min()\n",
    "        )\n",
    "        end_date = (\n",
    "            station_df[[\"DateTime\", column]].dropna()[\"DateTime\"].max()\n",
    "        )\n",
    "\n",
    "        print(f\"Station {station_id} - {column}\")\n",
    "        print(f\"Start date: {start_date}\")\n",
    "        print(f\"End date: {end_date}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ammonium last value\n",
    "sw_df[\"Ammonium (mg/l)\"].ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to compare the air temperature between the airport and the stations first\n",
    "\n",
    "# plot the air temperature for the airport and the stations\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=meteo_df.index,\n",
    "        y=meteo_df[\"Temperature Mean (°C)\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Airport\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "for station_id in gw_stations_dict.keys():\n",
    "    station_df = gw_stations_dict[station_id]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Temperature (°C)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "    title=\"Temperature\",\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pearson correlation\n",
    "\n",
    "for station_id in gw_stations_dict.keys():\n",
    "    station_df = gw_stations_dict[station_id]\n",
    "\n",
    "    station_df = station_df.resample(\"M\").median()\n",
    "\n",
    "    # get dates for the station where the temperature is not nan\n",
    "    dates = station_df[\"Air Temperature (°C)\"].dropna().index\n",
    "\n",
    "    station_df = station_df.loc[dates]\n",
    "\n",
    "    airport_df = meteo_df.loc[dates].copy()\n",
    "\n",
    "    # compute pearson correlation\n",
    "    corr, _ = pearsonr(\n",
    "        airport_df[\"Temperature Mean (°C)\"],\n",
    "        station_df[\"Air Temperature (°C)\"],\n",
    "    )\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df.index,\n",
    "            y=airport_df[\"Temperature Mean (°C)\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Airport\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Station {station_id}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=station_df.index,\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"lines\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add the correlation to the plot\n",
    "    fig.add_annotation(\n",
    "        x=0.01,\n",
    "        y=0.95,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=f\"Pearson Correlation: {corr:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Air Temperature (°C)\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the air temperature between the airport and the stations\n",
    "for station_id in gw_stations_dict.keys():\n",
    "    station_df = gw_stations_dict[station_id]\n",
    "\n",
    "    station_df = station_df.resample(\"M\").median()\n",
    "\n",
    "    # get dates for the station where the temperature is not nan\n",
    "    dates = station_df[\"Air Temperature (°C)\"].dropna().index\n",
    "\n",
    "    station_df = station_df.loc[dates]\n",
    "\n",
    "    airport_df = meteo_df.loc[dates].copy()\n",
    "\n",
    "    X = airport_df[\"Temperature Mean (°C)\"].copy()\n",
    "\n",
    "    # X = scaler.fit_transform(X.values.reshape(-1, 1))\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "    y = station_df[\"Air Temperature (°C)\"].copy()\n",
    "\n",
    "    # y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    line = pd.Series(results.predict(X), index=df.index)\n",
    "\n",
    "    slope = results.params[1]\n",
    "    p_value = results.pvalues[1]\n",
    "\n",
    "    print(f\"Station {station_id} - Slope: {slope}\")\n",
    "    print(f\"Station {station_id} - P-value: {p_value}\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=airport_df[\"Temperature Mean (°C)\"],\n",
    "            y=station_df[\"Air Temperature (°C)\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Data\",\n",
    "            marker=dict(size=8, color=\"blue\", opacity=0.7),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add line on bisector\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[-10, 40],\n",
    "            y=[-10, 40],\n",
    "            mode=\"lines\",\n",
    "            name=\"Bisector\",\n",
    "            line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Airport\",\n",
    "        yaxis_title=f\"Station {station_id}\",\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        ),\n",
    "        title=\"Air Temperature\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the sw_df first to have unique indices\n",
    "gw_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation is high between the airport and the stations,\n",
    "# so we can add the airport data variables to the stations\n",
    "\n",
    "# add the rainfall data to the stations\n",
    "gw_df[\"Cumulated Rainfall (mm)\"] = np.nan\n",
    "\n",
    "for station_id in gw_df[\"Station\"].unique():\n",
    "    start_date = gw_df[gw_df[\"Station\"] == station_id][\"DateTime\"].min()\n",
    "    end_date = gw_df[gw_df[\"Station\"] == station_id][\"DateTime\"].max()\n",
    "\n",
    "    # take the common date range with the airport\n",
    "    start_date = max(start_date, meteo_df.index.min())\n",
    "    end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "    airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "    # take the common date range with the station\n",
    "    # Identify the indices in sw_df that match the station_id and are within the date range\n",
    "    indices = gw_df[\n",
    "        (gw_df[\"Station\"] == station_id)\n",
    "        & (gw_df[\"DateTime\"] >= start_date)\n",
    "        & (gw_df[\"DateTime\"] <= end_date)\n",
    "    ].index\n",
    "\n",
    "    # Directly update sw_df for the matching indices\n",
    "    gw_df.loc[indices, \"Cumulated Rainfall (mm)\"] = airport_df[\n",
    "        \"Cumulated Rainfall (mm)\"\n",
    "    ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_df.drop(columns=\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the columns\n",
    "surface_info_df.sort_index(axis=1, inplace=True)\n",
    "ground_info_df.sort_index(axis=1, inplace=True)\n",
    "bacteria_info_df.sort_index(axis=1, inplace=True)\n",
    "meteo_info_df.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "surface_info_df.to_excel(\n",
    "    os.path.join(data_info_folder, \"surface_water_info.xlsx\")\n",
    ")\n",
    "ground_info_df.to_excel(\n",
    "    os.path.join(data_info_folder, \"ground_water_info.xlsx\")\n",
    ")\n",
    "bacteria_info_df.to_excel(\n",
    "    os.path.join(data_info_folder, \"bacteria_info.xlsx\")\n",
    ")\n",
    "meteo_info_df.to_excel(\n",
    "    os.path.join(data_info_folder, \"meteo_info.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df.to_excel(\n",
    "    os.path.join(clean_data_folder, \"surface.xlsx\"), index=False\n",
    ")\n",
    "gw_df.to_excel(\n",
    "    os.path.join(clean_data_folder, \"ground.xlsx\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UVA254 Raw Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_uva254_df = ts_gw_df[\n",
    "    ts_gw_df[\"Parameter\"] == \"UV-Adsorption (254)\"\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_uva254_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_uva254_df[\"DateTime\"] = pd.to_datetime(\n",
    "    ts_uva254_df[\"DateTime\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_uva254_df[\"Year\"] = ts_uva254_df[\"DateTime\"].dt.year\n",
    "ts_uva254_df[\"Month\"] = ts_uva254_df[\"DateTime\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ts_uva254_df[\"Station ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    ts_uva254_df,\n",
    "    x=\"Date\",\n",
    "    y=\"Value\",\n",
    "    color=\"Station ID\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"UV-Adsorption (254)\",\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "    },\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Value\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station 7285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_7285_df = ts_uva254_df[\n",
    "    ts_uva254_df[\"Station ID\"] == 7285\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_7285_df[\"Season\"] = station_7285_df[\"Month\"].apply(\n",
    "    lambda x: \"Winter\"\n",
    "    if x in [12, 1, 2]\n",
    "    else \"Spring\"\n",
    "    if x in [3, 4, 5]\n",
    "    else \"Summer\"\n",
    "    if x in [6, 7, 8]\n",
    "    else \"Autumn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot station 7285 with seasons as hue\n",
    "fig = px.line(\n",
    "    station_7285_df,\n",
    "    x=\"DateTime\",\n",
    "    y=\"Value\",\n",
    "    color=\"Season\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"UV-Adsorption (254) at station 7285\",\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "    },\n",
    "    xaxis_title=\"DateTime\",\n",
    "    yaxis_title=\"Value\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_station_7285_df = (\n",
    "    station_7285_df.groupby([\"Year\"])\n",
    "    .agg({\"Value\": [\"mean\", \"count\"]})\n",
    "    .reset_index()\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_station_7285_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_station_7285_df = (\n",
    "    station_7285_df.groupby([\"Season\"])\n",
    "    .agg({\"Value\": [\"mean\", \"count\"]})\n",
    "    .reset_index()\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_station_7285_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maggiorparte delle misurazioni in autunno e primavera, semestrali circa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = station_7285_df[[\"DateTime\", \"Value\"]].copy()\n",
    "\n",
    "result_7285 = smt.seasonal_decompose(\n",
    "    ts.set_index(\"Date\"), model=\"additive\", period=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=result_7285.trend.index,\n",
    "        y=result_7285.trend,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"MA period=2\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ts[\"Date\"],\n",
    "        y=ts[\"Value\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Original\",\n",
    "        line=dict(color=\"red\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station 5130"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-change-MEYtuKH4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
