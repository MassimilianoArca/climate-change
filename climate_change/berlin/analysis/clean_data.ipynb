{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests, kpss\n",
    "\n",
    "import pymannkendall as mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\"..\", \"..\", \"data\", \"berlin\")\n",
    "plot_folder = os.path.join(data_folder, \"plots\")\n",
    "surface_plot_folder = os.path.join(plot_folder, \"surface\")\n",
    "surface_cross_corr_folder = os.path.join(surface_plot_folder, \"cross_corr\")\n",
    "surface_corr_folder = os.path.join(surface_plot_folder, \"corr\")\n",
    "\n",
    "ground_plot_folder = os.path.join(plot_folder, \"ground\")\n",
    "ground_cross_corr_folder = os.path.join(ground_plot_folder, \"cross_corr\")\n",
    "ground_corr_folder = os.path.join(ground_plot_folder, \"corr\")\n",
    "\n",
    "clean_data_folder = os.path.join(data_folder, \"clean_data\")\n",
    "projections_folder = os.path.join(data_folder, \"projections\")\n",
    "\n",
    "my_projections_folder = os.path.join(projections_folder, \"found_by_me\")\n",
    "cat_projections_folder = os.path.join(projections_folder, \"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"surface.xlsx\")\n",
    ")\n",
    "ground_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"ground.xlsx\")\n",
    ")\n",
    "\n",
    "flow_river_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"flow_river.xlsx\")\n",
    ")\n",
    "\n",
    "hist_flow_river_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"flow_river_hist.xlsx\")\n",
    ")\n",
    "\n",
    "air_temp_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"air_temp.xlsx\")\n",
    ")\n",
    "\n",
    "hist_air_temp_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"air_temp_hist.xlsx\")\n",
    ")\n",
    "\n",
    "precip_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"precip.xlsx\")\n",
    ")\n",
    "\n",
    "hist_precip_projections = pd.read_excel(\n",
    "    os.path.join(my_projections_folder, \"precip_hist.xlsx\")\n",
    ")\n",
    "\n",
    "cat_flow_river_projections = pd.read_excel(\n",
    "    os.path.join(cat_projections_folder, \"flow_river.xlsx\")\n",
    ")\n",
    "\n",
    "cat_air_temp_projections = pd.read_excel(\n",
    "    os.path.join(cat_projections_folder, \"air_temp.xlsx\")\n",
    ")\n",
    "\n",
    "cat_precip_projections = pd.read_excel(\n",
    "    os.path.join(cat_projections_folder, \"precip.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis post Berlin Wall Fall\n",
    "surface_df = surface_df[surface_df[\"DateTime\"] >= \"1992-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conductivity vs Flow River Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Conductivity (µS/cm)', 'Flow River Rate (m³/s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id][['DateTime'] + columns].copy()\n",
    "    station_df.dropna(inplace=True)\n",
    "    \n",
    "    # Granger Causality Test\n",
    "    max_lag = 6\n",
    "    \n",
    "    for column in columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        station_df[column] = scaler.fit_transform(station_df[[column]])\n",
    "        \n",
    "    \n",
    "    \n",
    "    cond = station_df['Conductivity (µS/cm)']\n",
    "    flow = station_df['Flow River Rate (m³/s)']\n",
    "    \n",
    "    print()\n",
    "    print('Pearson Correlation')\n",
    "    print(pearsonr(cond, flow))\n",
    "    \n",
    "    # first check for stationarity\n",
    "    \n",
    "    # Augmented Dickey-Fuller test\n",
    "    adf_cond = adfuller(cond)\n",
    "    adf_flow = adfuller(flow)\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print(\"Augmented Dickey-Fuller Test\")\n",
    "    if adf_cond[1] > 0.05 or adf_flow[1] > 0.05:\n",
    "        print(\"Non-stationary data\")\n",
    "        print(\"Differencing data\")\n",
    "        cond = np.diff(cond, n=1)\n",
    "        flow = np.diff(flow, n=1)\n",
    "    \n",
    "    print(\"Granger Causality Test - Conductivity -> Flow\")\n",
    "    df = pd.DataFrame({\n",
    "        'cond': cond,\n",
    "        'flow': flow\n",
    "    })\n",
    "    \n",
    "    grangercausalitytests(df, max_lag, verbose=True)\n",
    "    \n",
    "    print()\n",
    "    print(\"Granger Causality Test - Flow -> Conductivity\")\n",
    "    df = pd.DataFrame({\n",
    "        'flow': flow,\n",
    "        'cond': cond\n",
    "    })\n",
    "    \n",
    "    grangercausalitytests(df, max_lag, verbose=True)\n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=station_df['DateTime'],\n",
    "                y=station_df[column],\n",
    "                mode='lines',\n",
    "                name=column\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    fig.update_layout(\n",
    "        title=f\"Surface Station {station_id}\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Value\"\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns = [\"DateTime\", \"Station\"]\n",
    "bacteria_columns = [\n",
    "    \"E.Coli (MPN/100ml)\",\n",
    "    \"Enterococcus (MPN/100ml)\",\n",
    "    \"Coliform (MPN/100ml)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Tests on Stationarity and Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to store the adf and mann-kendall test results for each station\n",
    "\n",
    "statistics_df = pd.DataFrame(\n",
    "    index=surface_df.columns.difference(diff_columns + bacteria_columns),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), ['ADF p-value', 'ADF result', 'MK p-value', 'MK result', 'Slope', 'Slope p-value', 'Trend Percentage']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in surface_df[\"Station\"].unique():\n",
    "    station_df = surface_df[surface_df[\"Station\"] == station_id].copy()\n",
    "    \n",
    "    # analysis post Berlin Wall Fall\n",
    "    station_df = station_df[station_df[\"DateTime\"] >= \"1992-01-01\"]\n",
    "\n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        df = station_df[[\"DateTime\", column]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        date_range = df.index\n",
    "        date_range = date_range.min(), date_range.max()\n",
    "\n",
    "        # make sure that the dataframe starts and finishes in the same month\n",
    "        start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "        # Slice the dataframe to start from the found index\n",
    "        df = df.loc[start_index:]\n",
    "\n",
    "        # ===== Prophet =====\n",
    "\n",
    "        df.index.name = \"ds\"\n",
    "\n",
    "        df = df.reset_index()\n",
    "\n",
    "        df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "        # using prophet\n",
    "\n",
    "        model = Prophet()\n",
    "        model.fit(df)\n",
    "        # Make predictions for both columns\n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        # Merging forecasted data with your original data\n",
    "        forecasting_final = pd.merge(\n",
    "            forecast,\n",
    "            df,\n",
    "            how=\"inner\",\n",
    "            on=\"ds\",\n",
    "        )\n",
    "\n",
    "        # compute linear regression on trend\n",
    "        X = np.arange(df.shape[0])\n",
    "        X = sm.add_constant(X)\n",
    "        y = df[\"y\"].copy()\n",
    "\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "\n",
    "        # plot the line of the linear regression\n",
    "        line = pd.Series(results.predict(X), index=df['ds'])\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['ds'],\n",
    "                y=df[\"y\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Original\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecasting_final[\"ds\"],\n",
    "                y=forecasting_final[\"trend\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Trend\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # perfrom Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(df[\"y\"], autolag=\"AIC\")\n",
    "        # perform KPSS test\n",
    "        kpss_result = kpss(df[\"y\"])\n",
    "        \n",
    "        # perfrom Mann-Kendall test        \n",
    "        mk_result = mk.original_test(df[\"y\"] - forecasting_final['yearly'])\n",
    "        \n",
    "        print()\n",
    "        print(f\"{column} - Augmented Dickey-Fuller Test\")\n",
    "        print(f\"ADF P-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"Lag used: {adf_result[2]}\")\n",
    "        if adf_result[1] > 0.05:\n",
    "            print(\"Unit root present, data is non-stationary\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"{column} - KPSS Test\")\n",
    "        print(f\"KPSS P-value: {kpss_result[1]:.4f}\")\n",
    "        if kpss_result[1] < 0.05:\n",
    "            print(\"Unit root present, data is non-stationary\")\n",
    "        print()\n",
    "        \n",
    "        if (adf_result[1] > 0.05 and kpss_result[1] < 0.05) or (adf_result[1] < 0.05 and kpss_result[1] > 0.05):\n",
    "            print(\"=== Consistency between tests! ===\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"{column} - Mann-Kendall Test\")\n",
    "        print(f\"Monotonic Trend: {mk_result.trend}\")\n",
    "        print(f\"p-value: {mk_result.p:.4f}\")\n",
    "        print()\n",
    "        slope = results.params.iloc[1]\n",
    "        print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "        p_value = results.pvalues.iloc[1]\n",
    "        print(f\"{column} - P-value: {p_value}\")\n",
    "        \n",
    "        statistics_df.loc[column, (station_id, 'ADF p-value')] = adf_result[1]\n",
    "        statistics_df.loc[column, (station_id, 'ADF result')] = 'Stationary' if adf_result[1] < 0.05 else 'Non-Stationary'\n",
    "        \n",
    "        statistics_df.loc[column, (station_id, 'MK p-value')] = mk_result.p\n",
    "        statistics_df.loc[column, (station_id, 'MK result')] = mk_result.trend\n",
    "        \n",
    "        # store the slope\n",
    "        statistics_df.loc[column, (station_id, 'Slope')] = slope\n",
    "        statistics_df.loc[column, (station_id, 'Slope p-value')] = p_value\n",
    "        \n",
    "        # calculate the percentage of the trend\n",
    "        trend_percentage = slope / df['y'].mean() * 100\n",
    "        \n",
    "        statistics_df.loc[column, (station_id, 'Trend Percentage')] = trend_percentage\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line.index,\n",
    "                y=line,\n",
    "                mode=\"lines\",\n",
    "                name=f\"Linear Regression\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        start_date = df['ds'].min()\n",
    "        end_date = df['ds'].max()\n",
    "\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=column,\n",
    "            font=dict(\n",
    "                size=18,\n",
    "            ),\n",
    "            title=f\"{station_id} - {column} - {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')} - Trend Percentage: {trend_percentage:.2f}% - Slope: {slope:.4f}\",\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the percentage change for each subsequent year\n",
    "for station_id in surface_df[\"Station\"].unique():\n",
    "    station_df = surface_df[surface_df[\"Station\"] == station_id].copy()\n",
    "    \n",
    "    # analysis post Berlin Wall Fall\n",
    "    station_df = station_df[station_df[\"DateTime\"] >= \"1992-01-01\"]\n",
    "\n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        df = station_df[[\"DateTime\", column]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "    \n",
    "        # compute the percentage change with respect to the first complete year\n",
    "        complete_years = df.groupby(df.index.year).filter(lambda x: len(x) == 12)\n",
    "\n",
    "        # Get the first complete year (with measurements for all 12 months)\n",
    "        first_year = complete_years.index.year.min()\n",
    "        \n",
    "        last_year = complete_years.index.year.max()\n",
    "        \n",
    "        first_year_df = complete_years[complete_years.index.year == first_year]\n",
    "        \n",
    "        last_year_df = complete_years[complete_years.index.year == last_year]\n",
    "        \n",
    "        percentage_change = (last_year_df[column].mean() - first_year_df[column].mean()) / first_year_df[column].mean() * 100\n",
    "        \n",
    "        print(f\"{station_id} - {column} - Percentage Change: {percentage_change:.2f}% - First Year: {first_year} - Last Year: {last_year}\")\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_excel(os.path.join(surface_plot_folder, 'trends', \"statistics.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    for column in station_df.columns.difference(['DateTime', 'Station']):\n",
    "        \n",
    "        print(f\"{station_id} - {column}\")\n",
    "        plot_acf(station_df[column], lags=20)\n",
    "        \n",
    "        \n",
    "        plot_pacf(station_df[column], lags=20)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causality and Cointegration\n",
    "\n",
    "\"If two or more time-series are cointegrated, then there must be Granger causality between them - either one-way or in both directions. However, the converse is not true.\"\n",
    "\n",
    "\"So, if your data are cointegrated but you don't find any evidence of causality, you have a conflict in your results. (This might occur if your sample size is too small to satisfy the asymptotics that the cointegration and causality tests rely on.) If you have cointegration and find one-way causality, everything is fine. (You may still be wrong about there being no causality in the other direction.) If your data are not cointegrated, then you have no cross-check on your causality results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Johansen test\n",
    "def johansen_test(data, det_order, k_ar_diff=1):\n",
    "    \"\"\"\n",
    "    Performs the Johansen cointegration test and prints the results.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): The time series data for the cointegration test.\n",
    "    det_order (int): The order of the deterministic terms.\n",
    "                     -1: No constant or trend.\n",
    "                      0: Constant term only.\n",
    "                      1: Constant and trend terms.\n",
    "    k_ar_diff (int): The number of lags to include in the VAR model.\n",
    "    \"\"\"\n",
    "    result = coint_johansen(data, det_order, k_ar_diff)\n",
    "    \n",
    "    print(f'Johansen Test Results (det_order={det_order})')\n",
    "    print('Trace Statistics:', result.trace_stat)\n",
    "    print('Critical Values (Trace):', result.trace_stat_crit_vals)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_dfs = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns + bacteria_columns).dropna()\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print()\n",
    "    \n",
    "    df = check_stationarity(station_df)\n",
    "    \n",
    "    coint_df = pd.DataFrame(columns=df.columns, index=df.columns)\n",
    "    \n",
    "    # perfrom Engle-Granger test\n",
    "    for column in df.columns:\n",
    "        for column2 in df.columns:\n",
    "            if column == column2:\n",
    "                continue\n",
    "            result = coint(df[column], df[column2])\n",
    "            \n",
    "            if result[1] < 0.05:\n",
    "                coint_df.loc[column, column2] = True\n",
    "            else:\n",
    "                coint_df.loc[column, column2] = False\n",
    "                \n",
    "    coint_dfs[station_id] = coint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_df\n",
    "\n",
    "# the row is cointrated with the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns + bacteria_columns).dropna()\n",
    "    \n",
    "    station_df = check_stationarity(station_df)\n",
    "    \n",
    "    aic, bic, fpe, hqic = [], [], [], []\n",
    "    model = VAR(station_df)\n",
    "    if station_id == 105:\n",
    "        p = range(0, 7)\n",
    "    else:\n",
    "        p = range(0, 17)\n",
    "    for i in p:\n",
    "        result = model.fit(i)\n",
    "        aic.append(result.aic)\n",
    "        bic.append(result.bic)\n",
    "        fpe.append(result.fpe)\n",
    "        hqic.append(result.hqic)\n",
    "    lags_metrics_df = pd.DataFrame({'AIC': aic, \n",
    "                                    'BIC': bic, \n",
    "                                    'HQIC': hqic,\n",
    "                                    'FPE': fpe}, \n",
    "                                index=p)    \n",
    "    fig, ax = plt.subplots(1, 4, figsize=(15, 3), sharex=True)\n",
    "    lags_metrics_df.plot(subplots=True, ax=ax, marker='o')\n",
    "    \n",
    "    # set the title\n",
    "    plt.suptitle(f\"Station {station_id}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max lag = 1 for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate case\n",
    "\n",
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()\n",
    "\n",
    "def grangers_causation_matrix_multivariate(data, maxlag=12, test='ssr_chi2test', verbose=False):\n",
    "    \"\"\"\n",
    "    Check Granger Causality in a multivariate setting.\n",
    "    \"\"\"\n",
    "    variables = data.columns\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "        \n",
    "    # Ensure data is stationary\n",
    "    data = check_stationarity(data)\n",
    "    \n",
    "    # Select the optimal lag length for the VAR model\n",
    "    \n",
    "    # Fit the VAR model\n",
    "    model = VAR(data)\n",
    "    model_fitted = model.fit(maxlags=maxlag)\n",
    "    \n",
    "    for r in variables:\n",
    "        for c in variables:\n",
    "            if c != r:\n",
    "                test_result = model_fitted.test_causality(r, c, kind='f')\n",
    "                p_value = round(test_result.pvalue, 4)\n",
    "                df.loc[r, c] = p_value\n",
    "                if verbose:\n",
    "                    print(f'Y = {r}, X = {c}, P-Value = {p_value}')\n",
    "            else:\n",
    "                df.loc[r, c] = 1\n",
    "    \n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    \n",
    "    return df, model_fitted.summary(), model_fitted.irf(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    causality_matrix, summary, irf = grangers_causation_matrix_multivariate(station_df.drop(columns=diff_columns + bacteria_columns).dropna(), maxlag=1)\n",
    "    \n",
    "    causality_matrices[station_id] = causality_matrix\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print(summary)\n",
    "    \n",
    "    ax = irf.plot(orth=True)\n",
    "    ax.set_size_inches(40, 20)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the cointration matrix is consistent with the Granger Causality matrix\n",
    "# if the cointration matrix is True, then the Granger Causality matrix should have a p-value < 0.05\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    coint_df = coint_dfs[station_id]\n",
    "    causality_matrix = causality_matrices[station_id]\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print()\n",
    "    \n",
    "    count = 0\n",
    "    cons_count = 0\n",
    "    \n",
    "    for column in coint_df.columns:\n",
    "        for column2 in coint_df.columns:\n",
    "            \n",
    "            if column == column2:\n",
    "                continue\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if coint_df.loc[column, column2]:\n",
    "                if causality_matrix.loc[column + '_y', column2 + '_x'] < 0.05:\n",
    "                    print(f\"{column} -> {column2} is consistent\")\n",
    "                    cons_count += 1\n",
    "                else:\n",
    "                    print(f\"{column} -> {column2} is not consistent\")\n",
    "            else:\n",
    "                if causality_matrix.loc[column + '_y', column2 + '_x'] < 0.05:\n",
    "                    print(f\"{column} -> {column2} is not consistent\")\n",
    "                else:\n",
    "                    print(f\"{column} -> {column2} is consistent\")\n",
    "                    cons_count += 1\n",
    "                    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(f\"Consistency: {cons_count}/{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, causality_matrix in causality_matrices.items():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=causality_matrix.values,\n",
    "            x=causality_matrix.columns,\n",
    "            y=causality_matrix.index,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    annotations = []\n",
    "    for i in range(causality_matrix.shape[0]):\n",
    "        for j in range(causality_matrix.shape[1]):\n",
    "            \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            value = causality_matrix.iloc[i, j]\n",
    "            if value < 0.05:\n",
    "                annotations.append(\n",
    "                    dict(\n",
    "                        x=causality_matrix.columns[j],\n",
    "                        y=causality_matrix.index[i],\n",
    "                        text=f\"{value:.2f}\",\n",
    "                        showarrow=False,\n",
    "                        font=dict(color=\"white\"),\n",
    "                        xref=\"x\",\n",
    "                        yref=\"y\"\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Granger Causality p-value Matrix - Station {station_id}\",\n",
    "        xaxis_title=\"X\",\n",
    "        yaxis=dict(\n",
    "            title=\"Y\",\n",
    "            autorange='reversed'  # Reverse the order of the y-axis\n",
    "        ),\n",
    "        annotations= annotations + [\n",
    "                dict(\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    x=-0.2,\n",
    "                    y=-0.6,\n",
    "                    showarrow=False,\n",
    "                    text='Note: if a given p-value is < significance level (0.05), then, <br> the corresponding X series (column) causes the Y (row).',\n",
    "                    font=dict(\n",
    "                        size=12\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate case\n",
    "\n",
    "def select_best_lag(data, maxlag):\n",
    "    \"\"\"\n",
    "    Select the best lag length using information criteria.\n",
    "    \"\"\"\n",
    "    model = VAR(data)\n",
    "    lag_order = model.select_order()\n",
    "    return lag_order.aic\n",
    "\n",
    "\n",
    "# Check for stationarity using both adfuller and kpss tests\n",
    "def check_stationarity(series):\n",
    "    adf_result = adfuller(series)\n",
    "    kpss_result = kpss(series)\n",
    "    return adf_result[1] < 0.05 and kpss_result[1] < 0.05\n",
    "\n",
    "def grangers_causation_matrix(data, variables, maxlag=12, test='ssr_chi2test', verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    \n",
    "    df = df.astype(object)\n",
    "    \n",
    "    lag_df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            if c != r:\n",
    "                granger_df = data[[c, r]]\n",
    "                if data[c].isna().any() or data[r].isna().any():\n",
    "                    granger_df = granger_df[[r, c]].dropna()\n",
    "                \n",
    "                try:\n",
    "                    adfuller(granger_df[c])\n",
    "                    adfuller(granger_df[r])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "                \n",
    "                if not check_stationarity(granger_df[c]):\n",
    "                    print(f\"{c} is non-stationary\")\n",
    "                    print(\"Differencing data\")\n",
    "                    granger_df[c] = granger_df[c].diff().dropna()\n",
    "                \n",
    "                if not check_stationarity(granger_df[r]):\n",
    "                    print(f\"{r} is non-stationary\")\n",
    "                    print(\"Differencing data\")\n",
    "                    granger_df[r] = granger_df[r].diff().dropna()\n",
    "                    \n",
    "                granger_df.dropna(inplace=True)\n",
    "                \n",
    "                # Select the best lag\n",
    "                selected_lag = select_best_lag(granger_df, maxlag)\n",
    "                \n",
    "                test_result = grangercausalitytests(granger_df, maxlag=selected_lag, verbose=False)\n",
    "                \n",
    "                # p_values = [round(test_result[i+1][0][test][1], 4) for i in range(selected_lag)]\n",
    "                \n",
    "                # the lag is the index of the list + 1\n",
    "                # store all the p-values to make a plot later\n",
    "                p_values = [round(test_result[i+1][0][test][1], 4) for i in range(selected_lag)]\n",
    "                statistics = [test_result[i+1][0][test][0] for i in range(selected_lag)]\n",
    "                \n",
    "                # need to store as list to avoid the error\n",
    "                \n",
    "                # store both the p values and the statistics in the same cell of df\n",
    "                df.loc[r, c] = [p_values, statistics]\n",
    "                \n",
    "            else:\n",
    "                df.loc[r, c] = 1\n",
    "                \n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    \n",
    "    return df, lag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices = {}\n",
    "lag_matrices = {}\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    causality_matrix, lag_matrix = grangers_causation_matrix(station_df, variables=station_df.columns.difference(diff_columns + bacteria_columns), maxlag=12)\n",
    "    \n",
    "    causality_matrices[station_id] = causality_matrix\n",
    "    lag_matrices[station_id] = lag_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices[station_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, causality_matrix in causality_matrices.items():\n",
    "    \n",
    "    for i in range(causality_matrix.shape[0]):\n",
    "        for j in range(causality_matrix.shape[1]):\n",
    "            if i != j:\n",
    "                test_result = causality_matrix.iloc[i, j]\n",
    "                \n",
    "                # check if the test result is a list\n",
    "                if not isinstance(test_result, list):\n",
    "                    continue\n",
    "                \n",
    "                p_values = test_result[0]\n",
    "                statistics = test_result[1]\n",
    "                \n",
    "                lags = np.arange(1, len(p_values) + 1)\n",
    "                \n",
    "                fig = go.Figure()\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=lags,\n",
    "                        y=p_values,\n",
    "                        mode='lines',\n",
    "                        name='P-Value'\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # fig.add_trace(\n",
    "                #     go.Scatter(\n",
    "                #         x=lags,\n",
    "                #         y=statistics,\n",
    "                #         mode='lines',\n",
    "                #         name='F-Statistic'\n",
    "                #     )\n",
    "                # )\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    title=f\"Station {station_id} - Granger Causality Test - {causality_matrix.columns[j]} -> {causality_matrix.index[i]}\",\n",
    "                    xaxis_title=\"Lag\",\n",
    "                    yaxis_title=\"Value\"\n",
    "                )\n",
    "                \n",
    "                fig.show()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, lag_matrix in lag_matrices.items():\n",
    "    \n",
    "    px.imshow(\n",
    "        lag_matrix,\n",
    "        labels=dict(x=\"X\", y=\"Y\", color=\"Lag\"),\n",
    "        title=f\"Lag Matrix - Station {station_id}\",\n",
    "        text_auto=True,\n",
    "        color_continuous_scale='Viridis',\n",
    "        width=800,\n",
    "        height=800\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'rgb(200, 2, 110)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    station_df = station_df.drop(columns=bacteria_columns)\n",
    "    \n",
    "    # Analysis post Berlin Wall Fall\n",
    "    station_df = station_df[station_df.index >= \"1992-01-01\"]\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'ds': station_df.index,\n",
    "            'y': station_df[column]\n",
    "        })\n",
    "        \n",
    "        model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df)\n",
    "        # Make predictions for both columns\n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=station_df.index,\n",
    "                y=station_df[column],\n",
    "                mode='lines',\n",
    "                name='Historical',\n",
    "                line=dict(\n",
    "                    color='black',\n",
    "                    width=0.6\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecast['ds'],\n",
    "                y=forecast['trend'],\n",
    "                mode='lines',\n",
    "                name='Trend',\n",
    "                line=dict(color=color)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Time\",\n",
    "            yaxis_title=f\"{column}\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(os.path.join(surface_plot_folder, 'trends', f\"station_{station_id}\")):\n",
    "            os.makedirs(os.path.join(surface_plot_folder, 'trends', f\"station_{station_id}\")) \n",
    "            \n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "        \n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                surface_plot_folder,\n",
    "                'trends',\n",
    "                f\"station_{station_id}\",\n",
    "                f\"{column_}.png\"\n",
    "            ),\n",
    "            scale=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform trend analysis for each month\n",
    "\n",
    "# Build interaction plot\n",
    "\n",
    "months = surface_df['DateTime'].dt.month.unique()\n",
    "months.sort()\n",
    "\n",
    "# Get the names of the months\n",
    "months_name = [pd.to_datetime(f\"{month}-01-2021\").strftime(\"%B\") for month in months]\n",
    "\n",
    "# Create dataframe to store the adf and mann-kendall test results for each station for each month\n",
    "statistics_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([surface_df.columns.difference(diff_columns + bacteria_columns), months_name]),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), ['ADF p-value', 'ADF result', 'MK p-value', 'MK result', 'Slope', 'Slope p-value', 'Trend Percentage']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "# Calculate global min and max for each variable\n",
    "global_min_max = {}\n",
    "for column in surface_df.columns.difference(diff_columns + bacteria_columns):\n",
    "    global_min_max[column] = (surface_df[column].min(), surface_df[column].max())\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    station_df = station_df.drop(columns=bacteria_columns)\n",
    "    \n",
    "    # Analysis post Berlin Wall Fall\n",
    "    station_df = station_df[station_df.index >= \"1992-01-01\"]\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            4,\n",
    "            3,\n",
    "            subplot_titles=months_name,\n",
    "            vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "            horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        )\n",
    "        \n",
    "        for i, month in enumerate(months):\n",
    "            \n",
    "            month_df = station_df[station_df.index.month == month]\n",
    "                       \n",
    "            # Perform Augmented Dickey-Fuller test\n",
    "            try:\n",
    "                adf_result = adfuller(month_df[column].dropna(), autolag=\"AIC\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            # Perform Mann-Kendall test\n",
    "            mk_result = mk.original_test(month_df[column])\n",
    "            \n",
    "            # Store the results in the statistics dataframe\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'ADF p-value')] = adf_result[1]\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'ADF result')] = 'Stationary' if adf_result[1] < 0.05 else 'Non-Stationary'\n",
    "            \n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'MK p-value')] = mk_result.p\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'MK result')] = mk_result.trend\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': month_df.index,\n",
    "                'y': month_df[column]\n",
    "            })\n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df)\n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # compute linear regression on trend\n",
    "            X = np.arange(df.shape[0])\n",
    "            X = sm.add_constant(X)\n",
    "            y = df[\"y\"].copy()\n",
    "\n",
    "            model = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            slope = results.params.iloc[1]\n",
    "\n",
    "            p_value = results.pvalues.iloc[1]\n",
    "            \n",
    "            # store the slope\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'Slope')] = slope\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'Slope p-value')] = p_value\n",
    "            \n",
    "            # calculate the percentage of the trend\n",
    "            first_value = forecast['trend'].iloc[0]\n",
    "            last_value = forecast['trend'].iloc[-1]\n",
    "            \n",
    "            trend_percentage = (last_value - first_value) / first_value * 100\n",
    "            \n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'Trend Percentage')] = trend_percentage\n",
    "                \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=month_df.index,\n",
    "                    y=month_df[column],\n",
    "                    mode='lines',\n",
    "                    name='Historical',\n",
    "                    line=dict(\n",
    "                        color='black',\n",
    "                        width=0.6\n",
    "                    ),\n",
    "                    showlegend=True if column not in legend_added else False\n",
    "                ),\n",
    "                row=(i // 3) + 1,\n",
    "                col=(i % 3) + 1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=forecast['ds'],\n",
    "                    y=forecast['trend'],\n",
    "                    mode='lines',\n",
    "                    name='Trend',\n",
    "                    line=dict(color='blue'),\n",
    "                    showlegend=True if column not in legend_added else False\n",
    "                ),\n",
    "                row=(i // 3) + 1,\n",
    "                col=(i % 3) + 1\n",
    "            )\n",
    "            \n",
    "            legend_added.add(column)\n",
    "            \n",
    "        # Set the same y-axis range for all subplots\n",
    "        fig.update_yaxes(range=global_min_max[column])\n",
    "        \n",
    "        fig.add_annotation(dict(\n",
    "            x=0.5,\n",
    "            y=-0.08,\n",
    "            showarrow=False,\n",
    "            text=\"Time\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            font=dict(size=20)\n",
    "        ))\n",
    "\n",
    "        fig.add_annotation(dict(\n",
    "            x=-0.08,\n",
    "            y=0.5,\n",
    "            showarrow=False,\n",
    "            text=column,\n",
    "            textangle=-90,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            font=dict(size=20)\n",
    "        )) \n",
    "            \n",
    "        fig.update_layout(\n",
    "            height=1000,\n",
    "            width=1200,\n",
    "            title={\n",
    "                'text': f\"Station {station_id}\",\n",
    "                'y': 0.98,  # Vertical position\n",
    "                'x': 0.5,  # Horizontal position\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'\n",
    "            },\n",
    "            font=dict(size=18),\n",
    "            margin=dict(t=80, l=80, b=100, r=40)  # Adjust margins\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(os.path.join(surface_plot_folder, 'trends', 'monthwise', f\"station_{station_id}\")):\n",
    "            os.makedirs(os.path.join(surface_plot_folder, 'trends', 'monthwise', f\"station_{station_id}\")) \n",
    "            \n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "        \n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                surface_plot_folder,\n",
    "                'trends',\n",
    "                'monthwise',\n",
    "                f\"station_{station_id}\",\n",
    "                f\"{column_}.png\"\n",
    "            ),\n",
    "            scale=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_excel(os.path.join(surface_plot_folder, 'trends', 'monthwise', \"statistics.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Correlation (lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_corr(ts1, ts2, max_lag):\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    cross_corr = [ts1.corr(ts2.shift(lag)) for lag in lags]\n",
    "    return lags, cross_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a time series is stationary, if not, difference it\n",
    "\n",
    "correlation_results_df = pd.DataFrame(\n",
    "    index=surface_df.columns.difference(diff_columns + bacteria_columns),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), surface_df.columns.difference(diff_columns + bacteria_columns)])\n",
    ")\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns + bacteria_columns).dropna()\n",
    "    \n",
    "    station_df = check_stationarity(station_df)\n",
    "    \n",
    "    # normalize the data\n",
    "    for column in station_df.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        station_df[column] = scaler.fit_transform(station_df[[column]])\n",
    "        \n",
    "    \n",
    "    max_lag = 12\n",
    "    \n",
    "    for column in station_df.columns:\n",
    "        for column2 in station_df.columns:\n",
    "            if column == column2:\n",
    "                continue\n",
    "            \n",
    "            lags, cross_corr = compute_cross_corr(station_df[column], station_df[column2], max_lag)\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(lags),\n",
    "                    y=cross_corr,\n",
    "                    mode='lines+markers',\n",
    "                    name='Cross Correlation'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Station {station_id} - {column} vs {column2}\",\n",
    "                xaxis_title=\"Lag (Months)\",\n",
    "                yaxis_title=\"Cross Correlation\"\n",
    "            )\n",
    "            \n",
    "            if not os.path.exists(os.path.join(surface_cross_corr_folder, f\"station_{station_id}\")):\n",
    "                os.makedirs(os.path.join(surface_cross_corr_folder, f\"station_{station_id}\"))\n",
    "                \n",
    "                \n",
    "            column_ = column.replace(\"/\", \"_\")\n",
    "            column2_ = column2.replace(\"/\", \"_\")\n",
    "                \n",
    "            fig.write_image(\n",
    "                os.path.join(\n",
    "                    surface_cross_corr_folder,\n",
    "                    f\"station_{station_id}\",\n",
    "                    f\"{column_}_vs_{column2_}.png\"\n",
    "                ),\n",
    "                scale=3\n",
    "            )\n",
    "            \n",
    "            corr, pvalue = pearsonr(station_df[column], station_df[column2])\n",
    "            \n",
    "            # put the correlation just if the p-value is less than 0.05 and in the upper triangle\n",
    "            if pvalue < 0.05:\n",
    "                correlation_results_df.loc[column, (station_id, column2)] = corr\n",
    "            else:\n",
    "                correlation_results_df.loc[column, (station_id, column2)] = np.nan\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_df.to_excel(os.path.join(surface_cross_corr_folder, \"correlation_results.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year by Year Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform year by year correlation\n",
    "correlation_results = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([surface_df.columns.difference(diff_columns + bacteria_columns), sorted(surface_df['DateTime'].dt.year.unique())]),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), surface_df.columns.difference(diff_columns + bacteria_columns)])\n",
    ")\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=bacteria_columns).dropna()\n",
    "    \n",
    "    # normalize the data\n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        scaler = MinMaxScaler()\n",
    "        station_df[column] = scaler.fit_transform(station_df[[column]])\n",
    "    \n",
    "    correlation_results[station_id] = {}\n",
    "    \n",
    "    for year in station_df['DateTime'].dt.year.unique():\n",
    "        \n",
    "        year_df = station_df[station_df['DateTime'].dt.year == year]\n",
    "        \n",
    "        year_df.drop(columns=diff_columns, inplace=True)\n",
    "        \n",
    "        for column in year_df.columns:\n",
    "            for column2 in year_df.columns:\n",
    "                if column == column2:\n",
    "                    continue\n",
    "                \n",
    "                result = pearsonr(year_df[column], year_df[column2])\n",
    "                \n",
    "                correlation_results.loc[(column, year), (station_id, column2)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation results\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "        \n",
    "        station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "        \n",
    "        station_df = station_df.drop(columns=bacteria_columns).dropna()\n",
    "        \n",
    "        station_df.set_index('DateTime', inplace=True)\n",
    "        \n",
    "        for column in station_df.columns.difference(diff_columns):\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            for column2 in station_df.columns.difference(diff_columns):\n",
    "                \n",
    "                if column == column2:\n",
    "                    continue\n",
    "                \n",
    "                years = correlation_results.loc[column, (station_id, column2)].index\n",
    "                \n",
    "                correlation = correlation_results.loc[(column, years), (station_id, column2)].dropna()            \n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=correlation.index.get_level_values(1),\n",
    "                        y=correlation.apply(lambda x: x[0]),\n",
    "                        mode='lines+markers',\n",
    "                        name=column2\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            fig.update_layout(\n",
    "                title=f\"Station {station_id} - {column} vs Other Parameters\",\n",
    "                xaxis_title=\"Year\",\n",
    "                yaxis_title=\"Pearson Correlation Coefficient\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(os.path.join(surface_corr_folder, f\"station_{station_id}\")):\n",
    "                os.makedirs(os.path.join(surface_corr_folder, f\"station_{station_id}\"))\n",
    "                \n",
    "            column_ = column.replace(\"/\", \"_\")\n",
    "            \n",
    "            fig.write_image(\n",
    "                os.path.join(\n",
    "                    surface_corr_folder,\n",
    "                    f\"station_{station_id}\",\n",
    "                    f\"{column_}.png\"\n",
    "                ),\n",
    "                scale=3,\n",
    "                width=1200,\n",
    "            )\n",
    "            \n",
    "            if station_id == 325:\n",
    "                fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results.to_excel(os.path.join(surface_corr_folder, \"correlation_results.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression for Average Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month by Month Variation for each Year\n",
    "\n",
    "A time series of each year is built, the trend is computed with Prophet and a linear regression on the trend is computed in order to get the slope, which is the average month variation for the given year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform trend analysis for each year\n",
    "\n",
    "# build interaction plot\n",
    "\n",
    "years = surface_df['DateTime'].dt.year.unique()\n",
    "years.sort()\n",
    "\n",
    "# convert the years to int\n",
    "years = [int(year) for year in years]\n",
    "\n",
    "# create dataframe to store the adf and mann-kendall test results for each station for each year\n",
    "slopes_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([surface_df.columns.difference(diff_columns + bacteria_columns), years]),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), ['Slope', 'P-Value', 'Error', 'Trend Percentage']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    station_df = station_df.drop(columns=bacteria_columns)\n",
    "    \n",
    "    # station_df.dropna(inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        # fig = make_subplots(\n",
    "        #     4,\n",
    "        #     3,\n",
    "        #     subplot_titles=years,\n",
    "        #     vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "        #     horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        # )\n",
    "        \n",
    "        for i, year in enumerate(years):\n",
    "            \n",
    "            year_df = station_df[station_df.index.year == year]\n",
    "            \n",
    "            # if the dataframe contains only NaN values, skip the year\n",
    "            if year_df[column].isna().all():\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': year_df.index,\n",
    "                'y': year_df[column]\n",
    "            })\n",
    "             \n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            \n",
    "            model.fit(df)    \n",
    "            \n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # compute the linear regression on the trend\n",
    "            X = np.arange(forecast.shape[0])\n",
    "            X = sm.add_constant(X)\n",
    "            y = forecast['trend'].copy()\n",
    "            \n",
    "            model = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            slope = results.params.iloc[1]\n",
    "            p_value = results.pvalues.iloc[1]\n",
    "            st_error = results.bse.iloc[1]\n",
    "            \n",
    "            # store the results in the dataframe\n",
    "            slopes_df.loc[(column, year), (station_id, 'Slope')] = round(slope, 4)\n",
    "            slopes_df.loc[(column, year), (station_id, 'P-Value')] = round(p_value, 4)\n",
    "            slopes_df.loc[(column, year), (station_id, 'Error')] = round(st_error, 4)\n",
    "            \n",
    "            # compute the percentage of the trend\n",
    "            trend_percentage = (slope / forecast['trend'].mean()) * 100\n",
    "            slopes_df.loc[(column, year), (station_id, 'Trend Percentage')] = round(trend_percentage, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df.to_excel(os.path.join(surface_plot_folder, 'trends', \"slopes.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year by Year Variation for each Month\n",
    "\n",
    "A time series of each month is built, the trend is computed with Prophet and a linear regression on the trend is computed in order to get the slope, which is the average year variation for the given month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform trend analysis for each month\n",
    "\n",
    "# build interaction plot\n",
    "\n",
    "months = surface_df['DateTime'].dt.month.unique()\n",
    "months.sort()\n",
    "\n",
    "# get the names of the months\n",
    "months_name = [pd.to_datetime(f\"{month}-01-2021\").strftime(\"%B\") for month in months]\n",
    "\n",
    "# create dataframe to store the adf and mann-kendall test results for each station for each month\n",
    "slopes_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([surface_df.columns.difference(diff_columns + bacteria_columns), months_name]),\n",
    "    columns=pd.MultiIndex.from_product([surface_df['Station'].unique().tolist(), ['Slope', 'P-Value', 'Error', 'Trend Percentage']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    station_df = station_df.drop(columns=bacteria_columns)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        # fig = make_subplots(\n",
    "        #     4,\n",
    "        #     3,\n",
    "        #     subplot_titles=months_name,\n",
    "        #     vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "        #     horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        # )\n",
    "        \n",
    "        for i, month in enumerate(months):\n",
    "            \n",
    "            month_df = station_df[station_df.index.month == month]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': month_df.index,\n",
    "                'y': month_df[column]\n",
    "            })\n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df)\n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # compute the linear regression on the trend\n",
    "            X = np.arange(forecast.shape[0])\n",
    "            X = sm.add_constant(X)\n",
    "            y = forecast['trend'].copy()\n",
    "            \n",
    "            model = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            slope = results.params.iloc[1]\n",
    "            p_value = results.pvalues.iloc[1]\n",
    "            st_error = results.bse.iloc[1]\n",
    "            \n",
    "            # store the results in the dataframe\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Slope')] = slope\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'P-Value')] = p_value\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Error')] = st_error\n",
    "            \n",
    "            # compute the percentage of the trend\n",
    "            trend_percentage = (slope / forecast['trend'].mean()) * 100\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Trend Percentage')] = trend_percentage\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df.to_excel(os.path.join(surface_plot_folder, 'trends', 'monthwise', \"slopes.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# per il momento lascia qua, potrebbe tornare utile se si decidesse di calcolare una linear regression tra ogni changepoint\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns + bacteria_columns):\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'ds': station_df.index,\n",
    "                'y': station_df[column]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df)\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # add the changepoints detected by Prophet\n",
    "        changepoints = model.changepoints\n",
    "    \n",
    "        signif_changepoints = changepoints[\n",
    "            np.abs(np.nanmean(model.params['delta'], axis=0)) >= 0.1\n",
    "        ]\n",
    "        \n",
    "        changepoints_values = forecast.loc[forecast['ds'].isin(signif_changepoints), 'trend']\n",
    "        \n",
    "        trend = forecast['trend']\n",
    "        \n",
    "        # compute linear regression on trend\n",
    "        X = np.arange(trend.shape[0])\n",
    "        X = sm.add_constant(X)\n",
    "        y = trend.copy()\n",
    "        \n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "        \n",
    "        # plot the line of the linear regression\n",
    "        line = pd.Series(results.predict(X), index=forecast['ds'])\n",
    "        \n",
    "        # get the slope with the p-value and the standard error\n",
    "        slope = results.params.iloc[1]\n",
    "        p_value = results.pvalues.iloc[1]\n",
    "        std_err = results.bse.iloc[1]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=station_df.index,\n",
    "                y=station_df[column],\n",
    "                mode='lines',\n",
    "                name='Original',\n",
    "                line=dict(color='black')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecast['ds'],\n",
    "                y=trend,\n",
    "                mode='lines',\n",
    "                name='Trend',\n",
    "                line=dict(color=color)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line.index,\n",
    "                y=line,\n",
    "                mode='lines',\n",
    "                name='Linear Regression',\n",
    "                line=dict(color='red', dash='dash')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=signif_changepoints,\n",
    "                y=changepoints_values,\n",
    "                mode='markers',\n",
    "                name='Changepoints',\n",
    "                marker=dict(\n",
    "                    color='green',\n",
    "                    size=10\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Slope: {slope:.3f}, P-Value: {p_value:.3f}, Std. Error: {std_err:.3f}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\"\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Tests on Stationarity and Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to store the adf and mann-kendall test results for each station\n",
    "\n",
    "statistics_df = pd.DataFrame(\n",
    "    index=ground_df.columns.difference(diff_columns),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ['ADF p-value', 'ADF result', 'MK p-value', 'MK result']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in ground_df[\"Station\"].unique():\n",
    "    station_df = ground_df[ground_df[\"Station\"] == station_id].copy()\n",
    "\n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        df = station_df[[\"DateTime\", column]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        date_range = df.index\n",
    "        date_range = date_range.min(), date_range.max()\n",
    "\n",
    "        # make sure that the dataframe starts and finishes in the same month\n",
    "        start_index = df[df.index.month == date_range[1].month].index[0]\n",
    "\n",
    "        # Slice the dataframe to start from the found index\n",
    "        df = df.loc[start_index:]\n",
    "\n",
    "        # ===== Prophet =====\n",
    "\n",
    "        df.index.name = \"ds\"\n",
    "\n",
    "        df = df.reset_index()\n",
    "\n",
    "        df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "        # using prophet\n",
    "\n",
    "        model = Prophet()\n",
    "        model.fit(df)\n",
    "        # Make predictions for both columns\n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        # Merging forecasted data with your original data\n",
    "        forecasting_final = pd.merge(\n",
    "            forecast,\n",
    "            df,\n",
    "            how=\"inner\",\n",
    "            on=\"ds\",\n",
    "        )\n",
    "\n",
    "        # compute linear regression on trend\n",
    "        X = np.arange(df.shape[0])\n",
    "        X = sm.add_constant(X)\n",
    "        y = df[\"y\"].copy()\n",
    "\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "\n",
    "        # plot the line of the linear regression\n",
    "        line = pd.Series(results.predict(X), index=df['ds'])\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['ds'],\n",
    "                y=df[\"y\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Original\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecasting_final[\"ds\"],\n",
    "                y=forecasting_final[\"trend\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Trend\",\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # perfrom Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(df[\"y\"], autolag=\"AIC\")\n",
    "        # perform KPSS test\n",
    "        kpss_result = kpss(df[\"y\"])\n",
    "        \n",
    "        # perfrom Mann-Kendall test        \n",
    "        mk_result = mk.original_test(df[\"y\"] - forecasting_final['yearly'])\n",
    "        \n",
    "        print()\n",
    "        print(f\"{column} - Augmented Dickey-Fuller Test\")\n",
    "        print(f\"ADF P-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"Lag used: {adf_result[2]}\")\n",
    "        if adf_result[1] > 0.05:\n",
    "            print(\"Unit root present, data is non-stationary\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"{column} - KPSS Test\")\n",
    "        print(f\"KPSS P-value: {kpss_result[1]:.4f}\")\n",
    "        if kpss_result[1] < 0.05:\n",
    "            print(\"Unit root present, data is non-stationary\")\n",
    "        print()\n",
    "        \n",
    "        if (adf_result[1] > 0.05 and kpss_result[1] < 0.05) or (adf_result[1] < 0.05 and kpss_result[1] > 0.05):\n",
    "            print(\"=== Consistency between tests! ===\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"{column} - Mann-Kendall Test\")\n",
    "        print(f\"Monotonic Trend: {mk_result.trend}\")\n",
    "        print(f\"p-value: {mk_result.p:.4f}\")\n",
    "        print()\n",
    "        slope = results.params.iloc[1]\n",
    "        print(f\"{column} - Slope: {slope}\")\n",
    "\n",
    "        p_value = results.pvalues.iloc[1]\n",
    "        print(f\"{column} - P-value: {p_value}\")\n",
    "        \n",
    "        statistics_df.loc[column, (station_id, 'ADF p-value')] = adf_result[1]\n",
    "        statistics_df.loc[column, (station_id, 'ADF result')] = 'Stationary' if adf_result[1] < 0.05 else 'Non-Stationary'\n",
    "        \n",
    "        statistics_df.loc[column, (station_id, 'MK p-value')] = mk_result.p\n",
    "        statistics_df.loc[column, (station_id, 'MK result')] = mk_result.trend\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line.index,\n",
    "                y=line,\n",
    "                mode=\"lines\",\n",
    "                name=f\"Linear Regression\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        start_date = df['ds'].min()\n",
    "        end_date = df['ds'].max()\n",
    "\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=column,\n",
    "            font=dict(\n",
    "                size=18,\n",
    "            ),\n",
    "            title=f\"{station_id} - {column} - {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}\",\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_excel(os.path.join(ground_plot_folder, 'trends', \"statistics.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    for column in station_df.columns.difference(['DateTime', 'Station']):\n",
    "        \n",
    "        print(f\"{station_id} - {column}\")\n",
    "        plot_acf(station_df[column], lags=20)\n",
    "        \n",
    "        \n",
    "        plot_pacf(station_df[column], lags=20)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causality and Cointegration\n",
    "\n",
    "\"If two or more time-series are cointegrated, then there must be Granger causality between them - either one-way or in both directions. However, the converse is not true.\"\n",
    "\n",
    "\"So, if your data are cointegrated but you don't find any evidence of causality, you have a conflict in your results. (This might occur if your sample size is too small to satisfy the asymptotics that the cointegration and causality tests rely on.) If you have cointegration and find one-way causality, everything is fine. (You may still be wrong about there being no causality in the other direction.) If your data are not cointegrated, then you have no cross-check on your causality results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Johansen test\n",
    "def johansen_test(data, det_order, k_ar_diff=1):\n",
    "    \"\"\"\n",
    "    Performs the Johansen cointegration test and prints the results.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): The time series data for the cointegration test.\n",
    "    det_order (int): The order of the deterministic terms.\n",
    "                     -1: No constant or trend.\n",
    "                      0: Constant term only.\n",
    "                      1: Constant and trend terms.\n",
    "    k_ar_diff (int): The number of lags to include in the VAR model.\n",
    "    \"\"\"\n",
    "    result = coint_johansen(data, det_order, k_ar_diff)\n",
    "    \n",
    "    print(f'Johansen Test Results (det_order={det_order})')\n",
    "    print('Trace Statistics:', result.trace_stat)\n",
    "    print('Critical Values (Trace):', result.trace_stat_crit_vals)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_dfs = {}\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns).dropna()\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print()\n",
    "    \n",
    "    df = check_stationarity(station_df)\n",
    "    \n",
    "    coint_df = pd.DataFrame(columns=df.columns, index=df.columns)\n",
    "    \n",
    "    # perfrom Engle-Granger test\n",
    "    for column in df.columns:\n",
    "        for column2 in df.columns:\n",
    "            if column == column2:\n",
    "                continue\n",
    "            result = coint(df[column], df[column2])\n",
    "            \n",
    "            if result[1] < 0.05:\n",
    "                coint_df.loc[column, column2] = True\n",
    "            else:\n",
    "                coint_df.loc[column, column2] = False\n",
    "                \n",
    "    coint_dfs[station_id] = coint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_df\n",
    "\n",
    "# the row is cointrated with the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns).dropna()\n",
    "    \n",
    "    station_df = check_stationarity(station_df)\n",
    "    \n",
    "    aic, bic, fpe, hqic = [], [], [], []\n",
    "    model = VAR(station_df)\n",
    "    if station_id == 105:\n",
    "        p = range(0, 7)\n",
    "    else:\n",
    "        p = range(0, 17)\n",
    "    for i in p:\n",
    "        result = model.fit(i)\n",
    "        aic.append(result.aic)\n",
    "        bic.append(result.bic)\n",
    "        fpe.append(result.fpe)\n",
    "        hqic.append(result.hqic)\n",
    "    lags_metrics_df = pd.DataFrame({'AIC': aic, \n",
    "                                    'BIC': bic, \n",
    "                                    'HQIC': hqic,\n",
    "                                    'FPE': fpe}, \n",
    "                                index=p)    \n",
    "    fig, ax = plt.subplots(1, 4, figsize=(15, 3), sharex=True)\n",
    "    lags_metrics_df.plot(subplots=True, ax=ax, marker='o')\n",
    "    \n",
    "    # set the title\n",
    "    plt.suptitle(f\"Station {station_id}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max lag = 1 for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate case\n",
    "\n",
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()\n",
    "\n",
    "def grangers_causation_matrix_multivariate(data, maxlag=12, test='ssr_chi2test', verbose=False):\n",
    "    \"\"\"\n",
    "    Check Granger Causality in a multivariate setting.\n",
    "    \"\"\"\n",
    "    variables = data.columns\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "        \n",
    "    # Ensure data is stationary\n",
    "    data = check_stationarity(data)\n",
    "    \n",
    "    # Select the optimal lag length for the VAR model\n",
    "    \n",
    "    # Fit the VAR model\n",
    "    model = VAR(data)\n",
    "    model_fitted = model.fit(maxlags=maxlag)\n",
    "    \n",
    "    for r in variables:\n",
    "        for c in variables:\n",
    "            if c != r:\n",
    "                test_result = model_fitted.test_causality(r, c, kind='f')\n",
    "                p_value = round(test_result.pvalue, 4)\n",
    "                df.loc[r, c] = p_value\n",
    "                if verbose:\n",
    "                    print(f'Y = {r}, X = {c}, P-Value = {p_value}')\n",
    "            else:\n",
    "                df.loc[r, c] = 1\n",
    "    \n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    \n",
    "    return df, model_fitted.summary(), model_fitted.irf(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices = {}\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    causality_matrix, summary, irf = grangers_causation_matrix_multivariate(station_df.drop(columns=diff_columns).dropna(), maxlag=1)\n",
    "    \n",
    "    causality_matrices[station_id] = causality_matrix\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print(summary)\n",
    "    \n",
    "    ax = irf.plot(orth=True)\n",
    "    ax.set_size_inches(40, 20)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the cointration matrix is consistent with the Granger Causality matrix\n",
    "# if the cointration matrix is True, then the Granger Causality matrix should have a p-value < 0.05\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    \n",
    "    coint_df = coint_dfs[station_id]\n",
    "    causality_matrix = causality_matrices[station_id]\n",
    "    \n",
    "    print(f\"Station {station_id}\")\n",
    "    print()\n",
    "    \n",
    "    count = 0\n",
    "    cons_count = 0\n",
    "    \n",
    "    for column in coint_df.columns:\n",
    "        for column2 in coint_df.columns:\n",
    "            \n",
    "            if column == column2:\n",
    "                continue\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if coint_df.loc[column, column2]:\n",
    "                if causality_matrix.loc[column + '_y', column2 + '_x'] < 0.05:\n",
    "                    print(f\"{column} -> {column2} is consistent\")\n",
    "                    cons_count += 1\n",
    "                else:\n",
    "                    print(f\"{column} -> {column2} is not consistent\")\n",
    "            else:\n",
    "                if causality_matrix.loc[column + '_y', column2 + '_x'] < 0.05:\n",
    "                    print(f\"{column} -> {column2} is not consistent\")\n",
    "                else:\n",
    "                    print(f\"{column} -> {column2} is consistent\")\n",
    "                    cons_count += 1\n",
    "                    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(f\"Consistency: {cons_count}/{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, causality_matrix in causality_matrices.items():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=causality_matrix.values,\n",
    "            x=causality_matrix.columns,\n",
    "            y=causality_matrix.index,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    annotations = []\n",
    "    for i in range(causality_matrix.shape[0]):\n",
    "        for j in range(causality_matrix.shape[1]):\n",
    "            \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            value = causality_matrix.iloc[i, j]\n",
    "            if value < 0.05:\n",
    "                annotations.append(\n",
    "                    dict(\n",
    "                        x=causality_matrix.columns[j],\n",
    "                        y=causality_matrix.index[i],\n",
    "                        text=f\"{value:.2f}\",\n",
    "                        showarrow=False,\n",
    "                        font=dict(color=\"white\"),\n",
    "                        xref=\"x\",\n",
    "                        yref=\"y\"\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Granger Causality p-value Matrix - Station {station_id}\",\n",
    "        xaxis_title=\"X\",\n",
    "        yaxis=dict(\n",
    "            title=\"Y\",\n",
    "            autorange='reversed'  # Reverse the order of the y-axis\n",
    "        ),\n",
    "        annotations= annotations + [\n",
    "                dict(\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    x=-0.2,\n",
    "                    y=-0.6,\n",
    "                    showarrow=False,\n",
    "                    text='Note: if a given p-value is < significance level (0.05), then, <br> the corresponding X series (column) causes the Y (row).',\n",
    "                    font=dict(\n",
    "                        size=12\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate case\n",
    "\n",
    "def select_best_lag(data, maxlag):\n",
    "    \"\"\"\n",
    "    Select the best lag length using information criteria.\n",
    "    \"\"\"\n",
    "    model = VAR(data)\n",
    "    lag_order = model.select_order()\n",
    "    return lag_order.aic\n",
    "\n",
    "\n",
    "# Check for stationarity using both adfuller and kpss tests\n",
    "def check_stationarity(series):\n",
    "    adf_result = adfuller(series)\n",
    "    kpss_result = kpss(series)\n",
    "    return adf_result[1] < 0.05 and kpss_result[1] < 0.05\n",
    "\n",
    "def grangers_causation_matrix(data, variables, maxlag=12, test='ssr_chi2test', verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    \n",
    "    df = df.astype(object)\n",
    "    \n",
    "    lag_df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            if c != r:\n",
    "                granger_df = data[[c, r]]\n",
    "                if data[c].isna().any() or data[r].isna().any():\n",
    "                    granger_df = granger_df[[r, c]].dropna()\n",
    "                \n",
    "                try:\n",
    "                    adfuller(granger_df[c])\n",
    "                    adfuller(granger_df[r])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "                \n",
    "                if not check_stationarity(granger_df[c]):\n",
    "                    print(f\"{c} is non-stationary\")\n",
    "                    print(\"Differencing data\")\n",
    "                    granger_df[c] = granger_df[c].diff().dropna()\n",
    "                \n",
    "                if not check_stationarity(granger_df[r]):\n",
    "                    print(f\"{r} is non-stationary\")\n",
    "                    print(\"Differencing data\")\n",
    "                    granger_df[r] = granger_df[r].diff().dropna()\n",
    "                    \n",
    "                granger_df.dropna(inplace=True)\n",
    "                \n",
    "                # Select the best lag\n",
    "                selected_lag = select_best_lag(granger_df, maxlag)\n",
    "                \n",
    "                test_result = grangercausalitytests(granger_df, maxlag=selected_lag, verbose=False)\n",
    "                \n",
    "                # p_values = [round(test_result[i+1][0][test][1], 4) for i in range(selected_lag)]\n",
    "                \n",
    "                # the lag is the index of the list + 1\n",
    "                # store all the p-values to make a plot later\n",
    "                p_values = [round(test_result[i+1][0][test][1], 4) for i in range(selected_lag)]\n",
    "                statistics = [test_result[i+1][0][test][0] for i in range(selected_lag)]\n",
    "                \n",
    "                # need to store as list to avoid the error\n",
    "                \n",
    "                # store both the p values and the statistics in the same cell of df\n",
    "                df.loc[r, c] = [p_values, statistics]\n",
    "                \n",
    "            else:\n",
    "                df.loc[r, c] = 1\n",
    "                \n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    \n",
    "    return df, lag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices = {}\n",
    "lag_matrices = {}\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    causality_matrix, lag_matrix = grangers_causation_matrix(station_df, variables=station_df.columns.difference(diff_columns), maxlag=12)\n",
    "    \n",
    "    causality_matrices[station_id] = causality_matrix\n",
    "    lag_matrices[station_id] = lag_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causality_matrices[station_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, causality_matrix in causality_matrices.items():\n",
    "    \n",
    "    for i in range(causality_matrix.shape[0]):\n",
    "        for j in range(causality_matrix.shape[1]):\n",
    "            if i != j:\n",
    "                test_result = causality_matrix.iloc[i, j]\n",
    "                \n",
    "                # check if the test result is a list\n",
    "                if not isinstance(test_result, list):\n",
    "                    continue\n",
    "                \n",
    "                p_values = test_result[0]\n",
    "                statistics = test_result[1]\n",
    "                \n",
    "                lags = np.arange(1, len(p_values) + 1)\n",
    "                \n",
    "                fig = go.Figure()\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=lags,\n",
    "                        y=p_values,\n",
    "                        mode='lines',\n",
    "                        name='P-Value'\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # fig.add_trace(\n",
    "                #     go.Scatter(\n",
    "                #         x=lags,\n",
    "                #         y=statistics,\n",
    "                #         mode='lines',\n",
    "                #         name='F-Statistic'\n",
    "                #     )\n",
    "                # )\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    title=f\"Station {station_id} - Granger Causality Test - {causality_matrix.columns[j]} -> {causality_matrix.index[i]}\",\n",
    "                    xaxis_title=\"Lag\",\n",
    "                    yaxis_title=\"Value\"\n",
    "                )\n",
    "                \n",
    "                fig.show()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, lag_matrix in lag_matrices.items():\n",
    "    \n",
    "    px.imshow(\n",
    "        lag_matrix,\n",
    "        labels=dict(x=\"X\", y=\"Y\", color=\"Lag\"),\n",
    "        title=f\"Lag Matrix - Station {station_id}\",\n",
    "        text_auto=True,\n",
    "        color_continuous_scale='Viridis',\n",
    "        width=800,\n",
    "        height=800\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'rgb(200, 2, 110)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'ds': station_df.index,\n",
    "            'y': station_df[column]\n",
    "        })\n",
    "        \n",
    "        model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df)\n",
    "        # Make predictions for both columns\n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=station_df.index,\n",
    "                y=station_df[column],\n",
    "                mode='lines',\n",
    "                name='Historical',\n",
    "                line=dict(\n",
    "                    color='black',\n",
    "                    width=0.6\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecast['ds'],\n",
    "                y=forecast['trend'],\n",
    "                mode='lines',\n",
    "                name='Trend',\n",
    "                line=dict(color=color)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Time\",\n",
    "            yaxis_title=f\"{column}\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(os.path.join(ground_plot_folder, 'trends', f\"station_{station_id}\")):\n",
    "            os.makedirs(os.path.join(ground_plot_folder, 'trends', f\"station_{station_id}\")) \n",
    "            \n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "        \n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                ground_plot_folder,\n",
    "                'trends',\n",
    "                f\"station_{station_id}\",\n",
    "                f\"{column_}.png\"\n",
    "            ),\n",
    "            scale=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform trend analysis for each month\n",
    "\n",
    "# build interaction plot\n",
    "\n",
    "months = ground_df['DateTime'].dt.month.unique()\n",
    "months.sort()\n",
    "\n",
    "# get the names of the months\n",
    "months_name = [pd.to_datetime(f\"{month}-01-2021\").strftime(\"%B\") for month in months]\n",
    "\n",
    "# create dataframe to store the adf and mann-kendall test results for each station for each month\n",
    "statistics_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([ground_df.columns.difference(diff_columns), months_name]),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ['ADF p-value', 'ADF result', 'MK p-value', 'MK result']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "for station_id in surface_df['Station'].unique():\n",
    "    \n",
    "    station_df = surface_df[surface_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            4,\n",
    "            3,\n",
    "            subplot_titles=months_name,\n",
    "            vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "            horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        )\n",
    "        \n",
    "        for i, month in enumerate(months):\n",
    "            \n",
    "            month_df = station_df[station_df.index.month == month]\n",
    "                       \n",
    "            # pefrom Augmented Dickey-Fuller test\n",
    "            try:\n",
    "                adf_result = adfuller(month_df[column].dropna(), autolag=\"AIC\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            # perfrom Mann-Kendall test\n",
    "            mk_result = mk.original_test(month_df[column])\n",
    "            \n",
    "            # store the results in the statistics dataframe\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'ADF p-value')] = adf_result[1]\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'ADF result')] = 'Stationary' if adf_result[1] < 0.05 else 'Non-Stationary'\n",
    "            \n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'MK p-value')] = mk_result.p\n",
    "            statistics_df.loc[(column, months_name[i]), (station_id, 'MK result')] = mk_result.trend\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': month_df.index,\n",
    "                'y': month_df[column]\n",
    "            })\n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df)\n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=month_df.index,\n",
    "                    y=month_df[column],\n",
    "                    mode='lines',\n",
    "                    name='Historical',\n",
    "                    line=dict(\n",
    "                        color='black',\n",
    "                        width=0.6\n",
    "                    ),\n",
    "                    showlegend=True if column not in legend_added else False\n",
    "                ),\n",
    "                row=(i // 3) + 1,\n",
    "                col=(i % 3) + 1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=forecast['ds'],\n",
    "                    y=forecast['trend'],\n",
    "                    mode='lines',\n",
    "                    name='Trend',\n",
    "                    line=dict(color=color),\n",
    "                    showlegend=True if column not in legend_added else False\n",
    "                ),\n",
    "                row=(i // 3) + 1,\n",
    "                col=(i % 3) + 1\n",
    "            )\n",
    "            \n",
    "            legend_added.add(column)\n",
    "            \n",
    "        fig.add_annotation(dict(\n",
    "            x=0.5,\n",
    "            y=-0.08,\n",
    "            showarrow=False,\n",
    "            text=\"Time\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            font=dict(size=20)\n",
    "        ))\n",
    "\n",
    "        fig.add_annotation(dict(\n",
    "            x=-0.08,\n",
    "            y=0.5,\n",
    "            showarrow=False,\n",
    "            text=column,\n",
    "            textangle=-90,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            font=dict(size=20)\n",
    "        )) \n",
    "            \n",
    "        fig.update_layout(\n",
    "            height=1000,\n",
    "            width=1200,\n",
    "            title={\n",
    "                'text': f\"Station {station_id}\",\n",
    "                'y': 0.98,  # Vertical position\n",
    "                'x': 0.5,  # Horizontal position\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'\n",
    "            },\n",
    "            font=dict(size=18),\n",
    "            margin=dict(t=80, l=80, b=100, r=40)  # Adjust margins\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(os.path.join(ground_plot_folder, 'trends', 'monthwise', f\"station_{station_id}\")):\n",
    "            os.makedirs(os.path.join(ground_plot_folder, 'trends', 'monthwise', f\"station_{station_id}\")) \n",
    "            \n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "        \n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                ground_plot_folder,\n",
    "                'trends',\n",
    "                'monthwise',\n",
    "                f\"station_{station_id}\",\n",
    "                f\"{column_}.png\"\n",
    "            ),\n",
    "            scale=3\n",
    "        )        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.to_excel(os.path.join(ground_plot_folder, 'trends', 'monthwise', \"statistics.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Correlation (lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(df):\n",
    "    \"\"\"\n",
    "    Check and make the time series stationary if required.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        adf = adfuller(df[column])\n",
    "        kp = kpss(df[column])\n",
    "        if adf[1] > 0.05 and kp[1] < 0.05:\n",
    "            print(f'{column} is non-stationary. Differencing the data.')\n",
    "            df[column] = df[column].diff().dropna()\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_corr(ts1, ts2, max_lag):\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    cross_corr = [ts1.corr(ts2.shift(lag)) for lag in lags]\n",
    "    return lags, cross_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a time series is stationary, if not, difference it\n",
    "\n",
    "correlation_results_df = pd.DataFrame(\n",
    "    index=ground_df.columns.difference(diff_columns),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ground_df.columns.difference(diff_columns)])\n",
    ")\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df = station_df.drop(columns=diff_columns).dropna()\n",
    "    \n",
    "    station_df = check_stationarity(station_df)\n",
    "    \n",
    "    # normalize the data\n",
    "    for column in station_df.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        station_df[column] = scaler.fit_transform(station_df[[column]])\n",
    "        \n",
    "    \n",
    "    max_lag = 12\n",
    "    \n",
    "    for column in station_df.columns:\n",
    "        for column2 in station_df.columns:\n",
    "            if column == column2:\n",
    "                continue\n",
    "            \n",
    "            lags, cross_corr = compute_cross_corr(station_df[column], station_df[column2], max_lag)\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(lags),\n",
    "                    y=cross_corr,\n",
    "                    mode='lines+markers',\n",
    "                    name='Cross Correlation'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Station {station_id} - {column} vs {column2}\",\n",
    "                xaxis_title=\"Lag (Months)\",\n",
    "                yaxis_title=\"Cross Correlation\"\n",
    "            )\n",
    "            \n",
    "            if not os.path.exists(os.path.join(ground_cross_corr_folder, f\"station_{station_id}\")):\n",
    "                os.makedirs(os.path.join(ground_cross_corr_folder, f\"station_{station_id}\"))\n",
    "                \n",
    "                \n",
    "            column_ = column.replace(\"/\", \"_\")\n",
    "            column2_ = column2.replace(\"/\", \"_\")\n",
    "                \n",
    "            fig.write_image(\n",
    "                os.path.join(\n",
    "                    ground_cross_corr_folder,\n",
    "                    f\"station_{station_id}\",\n",
    "                    f\"{column_}_vs_{column2_}.png\"\n",
    "                ),\n",
    "                scale=3\n",
    "            )\n",
    "            \n",
    "            corr, pvalue = pearsonr(station_df[column], station_df[column2])\n",
    "            \n",
    "            # put the correlation just if the p-value is less than 0.05 and in the upper triangle\n",
    "            if pvalue < 0.05:\n",
    "                correlation_results_df.loc[column, (station_id, column2)] = corr\n",
    "            else:\n",
    "                correlation_results_df.loc[column, (station_id, column2)] = np.nan\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_df.to_excel(os.path.join(ground_cross_corr_folder, \"correlation_results.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year by Year Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform year by year correlation\n",
    "correlation_results = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([ground_df.columns.difference(diff_columns), sorted(ground_df['DateTime'].dt.year.unique())]),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ground_df.columns.difference(diff_columns)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id in ground_df['Station'].unique():\n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    # normalize the data\n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        scaler = MinMaxScaler()\n",
    "        station_df[column] = scaler.fit_transform(station_df[[column]])\n",
    "    \n",
    "    correlation_results[station_id] = {}\n",
    "    \n",
    "    for year in station_df['DateTime'].dt.year.unique():\n",
    "        \n",
    "        year_df = station_df[station_df['DateTime'].dt.year == year]\n",
    "        \n",
    "        year_df.drop(columns=diff_columns, inplace=True)\n",
    "        \n",
    "        for column in year_df.columns:\n",
    "            for column2 in year_df.columns:\n",
    "                if column == column2:\n",
    "                    continue\n",
    "                \n",
    "                result = pearsonr(year_df[column], year_df[column2])\n",
    "                \n",
    "                correlation_results.loc[(column, year), (station_id, column2)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation results\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "        \n",
    "        station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "        \n",
    "        station_df.set_index('DateTime', inplace=True)\n",
    "        \n",
    "        for column in station_df.columns.difference(diff_columns):\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            \n",
    "            for column2 in station_df.columns.difference(diff_columns):\n",
    "                \n",
    "                if column == column2:\n",
    "                    continue\n",
    "                \n",
    "                years = correlation_results.loc[column, (station_id, column2)].index\n",
    "                \n",
    "                correlation = correlation_results.loc[(column, years), (station_id, column2)].dropna()            \n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=correlation.index.get_level_values(1),\n",
    "                        y=correlation.apply(lambda x: x[0]),\n",
    "                        mode='lines+markers',\n",
    "                        name=column2\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            fig.update_layout(\n",
    "                title=f\"Station {station_id} - {column} vs Other Parameters\",\n",
    "                xaxis_title=\"Year\",\n",
    "                yaxis_title=\"Pearson Correlation Coefficient\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(os.path.join(ground_corr_folder, f\"station_{station_id}\")):\n",
    "                os.makedirs(os.path.join(ground_corr_folder, f\"station_{station_id}\"))\n",
    "                \n",
    "            column_ = column.replace(\"/\", \"_\")\n",
    "            \n",
    "            fig.write_image(\n",
    "                os.path.join(\n",
    "                    ground_corr_folder,\n",
    "                    f\"station_{station_id}\",\n",
    "                    f\"{column_}.png\"\n",
    "                ),\n",
    "                scale=3,\n",
    "                width=1200,\n",
    "            )\n",
    "            \n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results.to_excel(os.path.join(ground_corr_folder, \"correlation_results.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression for Average Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month by Month Variation for each Year\n",
    "\n",
    "A time series of each year is built, the trend is computed with Prophet and a linear regression on the trend is computed in order to get the slope, which is the average month variation for the given year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform trend analysis for each year\n",
    "\n",
    "# build interaction plot\n",
    "\n",
    "years = ground_df['DateTime'].dt.year.unique()\n",
    "years.sort()\n",
    "\n",
    "# convert the years to int\n",
    "years = [int(year) for year in years]\n",
    "\n",
    "# create dataframe to store the adf and mann-kendall test results for each station for each year\n",
    "slopes_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([ground_df.columns.difference(diff_columns), years]),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ['Slope', 'P-Value', 'Error', 'Trend Percentage']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    \n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    station_df.dropna(inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        # fig = make_subplots(\n",
    "        #     4,\n",
    "        #     3,\n",
    "        #     subplot_titles=years,\n",
    "        #     vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "        #     horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        # )\n",
    "        \n",
    "        for i, year in enumerate(years):\n",
    "            \n",
    "            if year not in station_df.index.year:\n",
    "                continue\n",
    "            \n",
    "            year_df = station_df[station_df.index.year == year]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': year_df.index,\n",
    "                'y': year_df[column]\n",
    "            })\n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df)\n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # compute the linear regression on the trend\n",
    "            X = np.arange(forecast.shape[0])\n",
    "            X = sm.add_constant(X)\n",
    "            y = forecast['trend'].copy()\n",
    "            \n",
    "            model = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            slope = results.params.iloc[1]\n",
    "            p_value = results.pvalues.iloc[1]\n",
    "            st_error = results.bse.iloc[1]\n",
    "            \n",
    "            # store the results in the dataframe\n",
    "            slopes_df.loc[(column, year), (station_id, 'Slope')] = round(slope, 4)\n",
    "            slopes_df.loc[(column, year), (station_id, 'P-Value')] = round(p_value, 4)\n",
    "            slopes_df.loc[(column, year), (station_id, 'Error')] = round(st_error, 4)\n",
    "            \n",
    "            # compute the percentage of the trend\n",
    "            trend_percentage = (slope / forecast['trend'].mean()) * 100\n",
    "            slopes_df.loc[(column, year), (station_id, 'Trend Percentage')] = round(trend_percentage, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df.to_excel(os.path.join(ground_plot_folder, 'trends', \"slopes.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year by Year Variation for each Month\n",
    "\n",
    "A time series of each month is built, the trend is computed with Prophet and a linear regression on the trend is computed in order to get the slope, which is the average year variation for the given month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform trend analysis for each month\n",
    "\n",
    "# build interaction plot\n",
    "\n",
    "months = ground_df['DateTime'].dt.month.unique()\n",
    "months.sort()\n",
    "\n",
    "# get the names of the months\n",
    "months_name = [pd.to_datetime(f\"{month}-01-2021\").strftime(\"%B\") for month in months]\n",
    "\n",
    "# create dataframe to store the adf and mann-kendall test results for each station for each month\n",
    "slopes_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([ground_df.columns.difference(diff_columns), months_name]),\n",
    "    columns=pd.MultiIndex.from_product([ground_df['Station'].unique().tolist(), ['Slope', 'P-Value', 'Error', 'Trend Percentage']])\n",
    ")\n",
    "\n",
    "legend_added = set()\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    \n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        # fig = make_subplots(\n",
    "        #     4,\n",
    "        #     3,\n",
    "        #     subplot_titles=months_name,\n",
    "        #     vertical_spacing=0.08,  # Reduce vertical spacing\n",
    "        #     horizontal_spacing=0.05  # Reduce horizontal spacing\n",
    "        # )\n",
    "        \n",
    "        for i, month in enumerate(months):\n",
    "            \n",
    "            month_df = station_df[station_df.index.month == month]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'ds': month_df.index,\n",
    "                'y': month_df[column]\n",
    "            })\n",
    "            \n",
    "            model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df)\n",
    "            # Make predictions for both columns\n",
    "            future = model.make_future_dataframe(periods=0)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # compute the linear regression on the trend\n",
    "            X = np.arange(forecast.shape[0])\n",
    "            X = sm.add_constant(X)\n",
    "            y = forecast['trend'].copy()\n",
    "            \n",
    "            model = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            slope = results.params.iloc[1]\n",
    "            p_value = results.pvalues.iloc[1]\n",
    "            st_error = results.bse.iloc[1]\n",
    "            \n",
    "            # store the results in the dataframe\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Slope')] = slope\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'P-Value')] = p_value\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Error')] = st_error\n",
    "            \n",
    "            # compute the percentage of the trend\n",
    "            trend_percentage = (slope / forecast['trend'].mean()) * 100\n",
    "            slopes_df.loc[(column, months_name[i]), (station_id, 'Trend Percentage')] = trend_percentage\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df.to_excel(os.path.join(ground_plot_folder, 'trends', 'monthwise', \"slopes.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# per il momento lascia qua, potrebbe tornare utile se si decidesse di calcolare una linear regression tra ogni changepoint\n",
    "\n",
    "for station_id in ground_df['Station'].unique():\n",
    "    \n",
    "    station_df = ground_df[ground_df['Station'] == station_id].copy()\n",
    "    \n",
    "    station_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    for column in station_df.columns.difference(diff_columns):\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'ds': station_df.index,\n",
    "                'y': station_df[column]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model = Prophet(weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df)\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # add the changepoints detected by Prophet\n",
    "        changepoints = model.changepoints\n",
    "    \n",
    "        signif_changepoints = changepoints[\n",
    "            np.abs(np.nanmean(model.params['delta'], axis=0)) >= 0.1\n",
    "        ]\n",
    "        \n",
    "        changepoints_values = forecast.loc[forecast['ds'].isin(signif_changepoints), 'trend']\n",
    "        \n",
    "        trend = forecast['trend']\n",
    "        \n",
    "        # compute linear regression on trend\n",
    "        X = np.arange(trend.shape[0])\n",
    "        X = sm.add_constant(X)\n",
    "        y = trend.copy()\n",
    "        \n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "        \n",
    "        # plot the line of the linear regression\n",
    "        line = pd.Series(results.predict(X), index=forecast['ds'])\n",
    "        \n",
    "        # get the slope with the p-value and the standard error\n",
    "        slope = results.params.iloc[1]\n",
    "        p_value = results.pvalues.iloc[1]\n",
    "        std_err = results.bse.iloc[1]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=station_df.index,\n",
    "                y=station_df[column],\n",
    "                mode='lines',\n",
    "                name='Original',\n",
    "                line=dict(color='black')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=forecast['ds'],\n",
    "                y=trend,\n",
    "                mode='lines',\n",
    "                name='Trend',\n",
    "                line=dict(color=color)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line.index,\n",
    "                y=line,\n",
    "                mode='lines',\n",
    "                name='Linear Regression',\n",
    "                line=dict(color='red', dash='dash')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=signif_changepoints,\n",
    "                y=changepoints_values,\n",
    "                mode='markers',\n",
    "                name='Changepoints',\n",
    "                marker=dict(\n",
    "                    color='green',\n",
    "                    size=10\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Slope: {slope:.3f}, P-Value: {p_value:.3f}, Std. Error: {std_err:.3f}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\"\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall Class Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_df = surface_df[surface_df['Station'] == 105][['DateTime', 'Cumulated Rainfall (mm)']].copy()\n",
    "\n",
    "# define classes as [0,1], (1, 2], (2, 3], (3, inf)\n",
    "\n",
    "rainfall_df['Class'] = pd.cut(rainfall_df['Cumulated Rainfall (mm)'], bins=[0, 1, 2, 3, np.inf], labels=['0-1', '1-2', '2-3', '3+'])\n",
    "\n",
    "rainfall_df['Year'] = rainfall_df['DateTime'].dt.year\n",
    "rainfall_df['Month'] = rainfall_df['DateTime'].dt.month\n",
    "\n",
    "# analyze if the frequency of the classes changes over time\n",
    "\n",
    "# create a pivot table\n",
    "pivot_table = rainfall_df.pivot_table(index='Year', columns='Class', aggfunc='size', fill_value=0)\n",
    "\n",
    "# plot the pivot table\n",
    "fig = go.Figure()\n",
    "\n",
    "for column in pivot_table.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pivot_table.index,\n",
    "            y=pivot_table[column],\n",
    "            mode='lines+markers',\n",
    "            name=column\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.update_layout(\n",
    "    title=\"Rainfall Classes Frequency Over Time\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Frequency\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-change-MEYtuKH4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
